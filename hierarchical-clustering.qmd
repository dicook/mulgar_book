# Hierarchical clustering

The aim of unsupervised classification, or cluster analysis, is to organize observations into similar groups.  Cluster analysis is a commonly used, appealing, and conceptually intuitive, statistical
method.  Some of its uses include market segmentation, where customers are grouped into clusters with similar attributes for targeted marketing; gene expression analysis, where genes with similar expression patterns are grouped together; and the creation of taxonomies of animals, insects, or plants. A cluster analysis results in a simplification of a dataset for two reasons: first, because the dataset can be summarized by a description of each cluster, and second, because each cluster, which is now relatively homogeneous, can
be analyzed separately.  Thus, it can be used to effectively reduce the size of massive amounts of data.

Organizing objects into groups is a task that seems to come naturally to humans, even to small children, and perhaps this is why it is an apparently intuitive method in data analysis.  However, cluster analysis is more complex than it initially appears.  Many people
imagine that it will produce neatly separated clusters like those in the top left plot of @ideal-clusters, but it almost never does.  Such ideal clusters are rarely encountered in real data, so we often need to modify our objective from ``find the natural clusters in this data'' to ``organize the cases into groups that are similar in some way.''  Even though this may seem disappointing when compared with the ideal, it is still often an effective means of simplifying
and understanding a dataset.

%  Figure 1
\begin{figure}[ht]
\centerline{\includegraphics[width=4in]{chap-clust/ideal.pdf}}
\caption[Structures in data and their impact on cluster
analysis]{Different structures in data and their impact on cluster
analysis.  When there are well-separated groups {\bf (top left)}, it is
simple to group similar observations.  Even when there are not {\bf
(top right)}, grouping observations may still be useful. There may be
nuisance variables that do not contribute to the clustering {\bf
(bottom left)}, and there may oddly shaped clusters {\bf (bottom
right)}.}
\label{ideal-clusters}
\end{figure}

At the heart of the clustering process is the work of discovering
which variables are most important for defining the groups.  It is
often true that we only require a subset of the variables for finding
clusters, whereas another subset (called \Term{nuisance variables}) has no
impact.  In the bottom left plot of Fig.~\ref{ideal-clusters}, it is
clear that the variable plotted horizontally is important for
splitting this data into two clusters, whereas the variable plotted
vertically is a nuisance variable. Nuisance is an apt term for these
variables, because they can radically change the interpoint distances
and impair the clustering process.
\index{cluster analysis!interpoint distance}
\index{cluster analysis!nuisance variable}

Dynamic graphical methods help us to find and understand the
cluster structure in high dimensions.  With the tools in our toolbox,
primarily tours, along with linked scatterplots and parallel
coordinate plots, we can see clusters in high-dimensional spaces. We
can detect gaps between clusters, the shape and relative positions of
clusters, and the presence of nuisance variables.  We can even find
unusually shaped clusters, like those in the bottom right plot in
Fig.~\ref{ideal-clusters}.  In simple situations we can use graphics
alone to group observations into clusters, using a ``spin and brush''
method. In more difficult data problems, we can assess and refine
numerical solutions using graphics.\index{brushing!persistent}
\index{cluster analysis!spin and brush}

This chapter discusses the use of interactive and dynamic graphics in
the clustering of data.  Section \ref{clust-bg} introduces cluster
analysis, focusing on interpoint distance measures. Section
\ref{clust-graphics} describes an example of a purely graphical
approach to cluster analysis, the spin and brush method. In the
example shown in that section, we were able to find simplifications of
the data that had not been found using numerical clustering methods,
and to find a variety of structures in high-dimensional space. Section
\ref{clust-num} describes methods for {reducing} the interpoint distance
matrix to an intercluster distance matrix using hierarchical
algorithms and model-based clustering, and shows how graphical tools
are used to assess the results of numerical methods. Section
\ref{clust-recap} summarizes the chapter and revisits the data
analysis strategies used in the examples. A good companion to the
material presented in this chapter is \citeasnoun{VR02}, which provides
data and code for practical examples of cluster analysis using R.
Section \ref{clust-recap} summarizes the chapter and revisits the data
analysis strategies used in the examples.

\section{Background}~\label{clust-bg}

Before we can begin finding groups of cases that are similar, we need
to decide on a definition of similarity.  How is similarity defined?
Consider a dataset with three cases and four variables, described in matrix
format as

\begin{eqnarray*}
\blX = \left[ \begin{array}{c} 
     \blX_1 \\ \blX_2 \\ \blX_3 \\ 
%\blX_4 \\
%     \blX_5 \\ \blX_6 \\ \blX_7 \\ \blX_8 \\ \blX_9 
     \end{array} \right] = 
     \left[ \begin{array}{rrrr}
       7.3 & 7.6 & 7.7 & 8.0 \\
       7.4 & 7.2 & 7.3 & 7.2 \\
%       6.7 & 7.2 & 7.1 & 7.4 \\
       4.1 & 4.6 & 4.6 & 4.8 \\
%       5.9 & 6.7 & 6.7 & 6.6 \\
%       8.8 & 8.2 & 8.1 & 8.1 \\
%       6.1 & 6.5 & 6.4 & 6.6 \\
%       6.3 & 6.1 & 6.7 & 6.6 \\
%       8.3 & 8.3 & 8.6 & 8.5 \\
     \end{array} \right]
\end{eqnarray*}

\noindent which is plotted in Fig.~\ref{similarity1}. 
The Euclidean distance between two cases (rows of the matrix) is
defined as

\begin{eqnarray*}
d_{\rm Euc}(\blX_i,\blX_j) &=& ||\blX_i-\blX_j|| %\\
% &=& \sqrt{(X_{i1}-X_{j1})^2+\dots + (X_{ip}-X_{jp})^2},
~~~~~~i,j=1,\dots, n,
\end{eqnarray*}

\noindent where $||\blX_i||=\sqrt{X_{i1}^2+X_{i2}^2+\dots +X_{ip}^2}$.
For example, the Euclidean distance between cases 1 and 2 in the above
data, is

\[
\sqrt{(7.3-7.4)^2+(7.6-7.2)^2+(7.7-7.3)^2+(8.0-7.2)^2} = 1.0.
\]

\index{cluster analysis!interpoint distance}

\noindent For the three cases, the interpoint Euclidean distance matrix is

\begin{eqnarray*}
d_{\rm Euc} =
\left[ \begin{array}{ccc}
 0.0  ~&     &   \\ 
 1.0 ~&  0.0 ~  &  \\
 6.3 ~& 5.5 ~&  0.0 ~ \\
\end{array} \right]
\begin{array}{r}
\blX_1 \\ \blX_2 \\ \blX_3 \\
\end{array}
\end{eqnarray*}

% Figure 2
\begin{figure}[htp]
\centerline{{\includegraphics[width=3in]{chap-clust/similarity1.pdf}}
 {\includegraphics[width=2in]{chap-clust/similarity2.pdf}}}
\caption[Clustering the example data]{Clustering the example data.
The scatterplot matrix {\bf (left)} shows that cases 1 and 2 have
similar values.  The parallel coordinate plot {\bf (right)} allows a
comparison of other structure, which shows the similarity in the profiles
on cases 1 and 3.  }
\label{similarity1}
\end{figure}

\noindent Cases 1 and 2 are more similar to each other than they are
to case 3, because the Euclidean distance between cases 1 and 2 is
much smaller than the distance between cases 1 and 3 and between cases
2 and 3.

There are many different ways to calculate similarity.  In recent
years similarity measures based on correlation distance have become
common. Correlation distance is typically used where similarity of
structure is more important than similarity in magnitude. 

\index{parallel coordinate plot}

As an example, see the parallel coordinate plot of the sample data at
the right of Fig.~\ref{similarity1}.  Cases 1 and 3 are widely
separated, but their shapes are similar (low, medium, medium, high).
Case 2, although overlapping with Case 1, has a very different shape
(high, medium, medium, low).  The correlation between two cases is
defined as

\begin{eqnarray}
\rho(\blX_i,\blX_j) = \frac{(\blX_i-c_i)'(\blX_j-c_j)}
{\sqrt{(\blX_i-c_i)'(\blX_i-c_i)} \sqrt{(\blX_j-c_j)'(\blX_j-c_j)}}
\label{corc}
\end{eqnarray}

\noindent When $c_i, c_j$ are the sample means
$\bar{\blX}_i,\bar{\blX}_j$, then $\rho$ is the Pearson correlation
coefficient. If, indeed, they are set at 0, as is commonly done,
$\rho$ is a generalized correlation that describes the angle between
the two data vectors. The correlation is then converted to a distance
metric; one equation for doing so is as follows:

\begin{eqnarray*}
d_{\rm Cor}(\blX_i,\blX_j) = \sqrt{2(1-\rho(\blX_i,\blX_j))}
\end{eqnarray*}

\noindent %Distance measures built on correlation are effectively
%angular distances between points, because for two vectors $\blX_i$ and
%$\blX_j$, $\cos (\angle(\blX_i,\blX_j)) \propto \blX_i'\blX_j$.
The
above distance metric will treat cases that are strongly negatively
correlated as the most distant.

The interpoint distance matrix for the sample data using $d_{\rm Cor}$ and
the Pearson correlation coefficient is

\begin{eqnarray*}
d_{\rm Cor} = 
\left[ \begin{array}{rrrrrrrrr}
 0.0  ~&     &  \\
 3.6 ~ & 0.0 ~ &  \\
 0.1 ~ & 3.8 ~ &  0.0 ~\\
\end{array} \right]
\end{eqnarray*}
% dist4 in R code 

\noindent By this metric, cases 1 and 3 are the most similar, because 
the correlation distance is smaller between these two cases than the
other pairs of cases. 

\index{cluster analysis!interpoint distance}

Note that these interpoint distances differ dramatically from those
for Euclidean distance.  As a consequence, the way the cases would be
clustered is also be very different. Choosing the appropriate distance
measure is an important part of a cluster analysis.

After a distance metric has been chosen and a cluster analysis has
been performed, the analyst must evaluate the results, and this is
actually a difficult task.  A cluster analysis does not generate
$p$-values or other numerical criteria, and the process tends to
produce hypotheses rather than testing them.  Even the most determined
attempts to produce the ``best'' results using modeling and validation
techniques may result in clusters that, although seemingly significant,
are useless for practical purposes.  As a result, cluster analysis is
best thought of as an exploratory technique, and it can be
quite useful despite the lack of formal validation because of its
power in data simplification.

The context in which the data arises is the key to 
assessing the results.  If the clusters can be characterized in a
sensible manner, and they increase our knowledge of the data, then
we are on the right track.  To use an even more pragmatic criterion, if
a company can gain an economic advantage by using a particular
clustering method to carve up their customer database, then that is the
method they should use.

%  This paragraph is a bit orphaned here.  It should be part of a 
%  longer discussion about assessing results.  dfs
%We've already drawn your attention to the parallel coordinate plot in
%Fig.~\ref{similarity1}.  It's a helpful plotting method to use with
%cluster analysis, both for exploring the data and for assessing the
%results.

% Introduce different clustering techniques here? hierarchical, k-means, 
% model-based, self-organizing maps.

\section{Purely graphics}~\label{clust-graphics}

\index{brushing!persistent}
\index{tour}
\index{cluster analysis!spin and brush}

A purely graphical spin and brush approach to cluster analysis works
well when there are good separations between groups, even when there
are marked differences in variance structures between groups or when
groups have non-linear boundaries. It does not work very well when
there are clusters that overlap, or when there are no distinct
clusters but rather we simply wish to partition the data. In these
situations it may be better to begin with a numerical solution and to
use visual tools to evaluate it, perhaps making refinements
subsequently.  Several examples of the spin and brush approach are
documented in the literature, such as \citeasnoun{CBCH95} and
\citeasnoun{WWS99}.

\index{datasets!\Data{PRIM7}}

This description of the spin and brush approach on \Data{PRIM7}, a
particle physics dataset, follows that in \citeasnoun{CBCH95}. The
data contains seven variables.  We have no labels for the data, so
when we begin, all the points have the same color and glyph. Watch the
data in a tour for a few minutes and you will see that there are no
natural clusters, but there is clearly structure.

%%% Difficulty rating, no separated clusters, low-dimensional
%%% structure embedded in high-d....

\index{projection pursuit!indexes}
\index{projection pursuit!indexes!holes}
\index{projection pursuit!indexes!central mass}
\index{principal component analysis}

We will use the projection pursuit guided tour to help us find that
structure.  We will tour on the principal components, rather than the
raw variables, because that improves the performance of the projection
pursuit indexes.  Two indexes are useful for detecting
clusters: holes and central mass. The holes index is sensitive to
projections where there are few points (i.e., a hole) in the center.
The central mass index is the opposite: It is sensitive to projections
that have too many points in the center. These indexes are explained
in Chap.~\ref{toolbox}.

The holes index is usually the most useful for clustering, but not for
the particle physics data, because it does not have a ``hole'' at the
center.  The central mass index is the most appropriate
here. Alternate between optimization (a guided tour) and the unguided
grand tour to find local maxima, each of which is a projection that
is potentially useful for revealing clusters.  The process is
illustrated in Fig.~\ref{prim7-tour}.

The top left plot shows the initial default projection, the second
principal component plotted against the first. The plot next to it
shows the projected data corresponding to the first local maximum
found by the guided tour. It has three strands of points stretching
out from the central clump and several outliers. We brush the points
along each strand, in red, blue, and orange, and we paint the outliers 
with open circles.  (See the next two plots.)  We continue by choosing a
new random start for the guided tour, and then waiting until new territory
in the data is discovered. \index{brushing!persistent}

The optimization settles on a projection where there are three strands
visible, as observed in the leftmost plot in the second row. Two 
strands have been previously brushed, but a new one has appeared; this
is painted yellow. 

We also notice that there is another new strand hidden below the red
strand. It is barely distinguishable from the red strand in this
projection, but the two strands separate widely in other projections.
It is tricky to brush it, because it is not well separated in this
projection.  We use a trick: Hide the red points, brush the new strand
green, and ``unhide'' the red points again (middle plot in the second
row).

Five clusters have been easily identified, and now finding new
clusters in this data is increasingly difficult. After several more
alternations between the grand tour and the guided tour, we find
something new (shown in the rightmost plot in the second row): One
more strand has emerged, and we paint it pink.

% Figure 3
\begin{figure}[htp]
\centerline{{\includegraphics[width=1.5in]{chap-clust/prim7-pp1.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/prim7-pp2.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/prim7-pp5.pdf}}}
\smallskip
\centerline{{\includegraphics[width=1.5in]{chap-clust/prim7-pp7.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/prim7-pp8.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/prim7-pp9.pdf}}}
\centerline{{\includegraphics[width=1.5in]{chap-clust/prim7-pp10.pdf}}
  {\includegraphics[width=1.5in]{chap-clust/prim7-pp11.pdf}}
  {\includegraphics[width=1.5in]{chap-clust/prim7-pp13.pdf}}}
\caption[Stages of ``spin and brush'' on \Data{PRIM7}]{Stages of spin
and brush on \Data{PRIM7}.  The high-dimensional geometry emerges as
the clusters are painted.}
\label{prim7-tour}
\end{figure}

The results at this stage are summarized by the bottom row of plots.
There is a very visible triangular component (in gray) revealed when
all of the colored points are hidden. We check the shape of this
cluster by drawing lines between outer points to contain the inner
ones. Touring after the lines are drawn helps to check how well they
match the shape of the clusters. The colored groups pair up at each
vertex, and we draw in the shape of these too ~---~ a single line
matches the structures reasonably well.

The final step of the spin and brush clustering is to clean up this
solution, touching up the color groups by continuing to tour, and
repainting a point here and there.  When we finish, we have found
seven clusters in this data that form a very strong geometric object
in the data space: a two-dimensional (2D) triangle, with two
one-dimensional (1D) strands extending in different directions from
each vertex.  The lines confirm our understanding of this object's
shape, because the points stay close to the lines in all of the
projections observed in a tour.

% Figure 4
\begin{figure}
\centerline{
   \includegraphics[width=1.5in]{chap-clust/prim7-pp13-model.pdf}
   \includegraphics[width=3in]{chap-clust/prim7-par.pdf}
}
\caption[The \Data{PRIM7} model summarized]{The \Data{PRIM7} model
summarized.  The model summary {\bf (left)} was formed by adding line
segments manually.  In the parallel coordinate plot, the profiles
highlighted in dark gray correspond to the points in the 2D triangle
at the center of the model.  }
\label{prim7-model}
\end{figure}

The next stage of cluster analysis is to characterize the nature of
the clusters. To do that, we would calculate summary statistics for
each cluster, and plot the clusters (Fig.~\ref{prim7-model}).
When we plot the clusters of the particle physics data, we find that
the 2D triangle exists primarily in the plane defined by X3 and X5. If
you do the same, notice that the variance in measurements for the gray
group is large in variables X3 and X5, but negligible in the other
variables. The linear pieces can also be characterized by their
distributions on each of the variables.  With this example, we have
shown that it is possible to uncover very unusual clusters in data
without any domain knowledge.

Here are several tips about the spin and brush approach. 
\begin{itemize}\itemsep 0in
\item Save the
dataset frequently during the exploration of a complex dataset,
being sure to save your colors and glyphs, because it may take several
sessions to arrive at a final clustering.  
\item Manual controls are useful
for refining the optimal projection because another projection in the
neighborhood may be more revealing.  
\item The holes index is usually the
most successful projection pursuit index for finding clusters.
\item Principal component coordinates may provide a better starting point
than the raw variables.
\end{itemize}
Finally, the spin and brush method will not
work well if there are no clear separations in the data, and the
clusters are high-dimensional, unlike the low-dimensional clusters
found in this example.

\section{Numerical methods}~\label{clust-num}

\index{cluster analysis!algorithms}

\subsection{Hierarchical algorithms}

\index{cluster analysis!hierarchical}
\index{cluster analysis!intercluster distance (linkage)}

Hierarchical cluster algorithms sequentially fuse neighboring points
to form ever-larger clusters, starting from a full interpoint distance
matrix. \Term{Distance between clusters} is described by a ``linkage
method'': For example, single linkage uses the smallest interpoint
distance between the members of a pair of clusters, complete linkage
uses the maximum interpoint distance, and average linkage uses the
average of the interpoint distances. A good discussion on cluster
analysis can be found in \citeasnoun{JW02} or \citeasnoun{Ev01}.

% I changed the text to match the figure.
%(Middle row) Clusters 1, 3 and 5 carve up the base
%triangle of the data. (Bottom row) Clusters 4 and 6 divide one of the
%arms, and cluster 7 is a singleton cluster.

Figure~\ref{prim7-hier} contains several plots that illustrate the
results of the hierarchical clustering of the particle physics data;
we used Euclidean interpoint distances and the average linkage
method. This is computed by:

\begin{verbatim}
> library(rggobi)
> d.prim7 <- read.csv("prim7.csv")
> d.prim7.dist <- dist(d.prim7)
> d.prim7.dend <- hclust(d.prim7.dist, method="average")
> plot(d.prim7.dend)
\end{verbatim}

\index{cluster analysis!dendrogram}

\noindent The dendrogram at the top shows the result of the clustering process.
Several large clusters were fused late in the process, with heights
(indicated by the height of the horizontal segment connecting two
clusters) well above those of the first joins; we will want to look at
these.  Two points were fused with the rest at the very last stages,
which indicates that they are outliers and have been assigned to
singleton clusters.

% Figure 5
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=3.5in]{chap-clust/prim7-dendrogram.pdf}}}
\smallskip
\centerline{{\includegraphics[width=1.2in]{chap-clust/prim7-clust1.pdf}}
 {\includegraphics[width=1.2in]{chap-clust/prim7-clust2.pdf}}
 {\includegraphics[width=1.2in]{chap-clust/prim7-clust3.pdf}}}
\smallskip
\centerline{{\includegraphics[width=1.2in]{chap-clust/prim7-clust5.pdf}}
 {\includegraphics[width=1.2in]{chap-clust/prim7-clust6.pdf}}
 {\includegraphics[width=1.2in]{chap-clust/prim7-clust7.pdf}}}
\caption[Hierarchical clustering of the particle physics
data]{Hierarchical clustering of the particle physics data.  The
dendrogram shows the results of clustering the data using average
linkage.  Clusters 1, 2, and 3 carve up the base triangle of the data;
clusters 5 and 6 divide one of the arms; and cluster 7 is a singleton.
}
\label{prim7-hier}
\end{figure*}

\index{outliers}
\index{animation}
\index{brushing!transient}
We cut the dendrogram to produce nine clusters because we would expect
to see seven clusters and a few outliers based on our observations
from the spin and brush approach, and our choice looks reasonable
given the structure of the dendrogram.  (In practice, we would usually
explore the clusters corresponding to several different cuts of the
dendrogram.) We assign each cluster an integer identifier, and in the
following plots, you see the results of highlighting one cluster at a
time and then running the grand tour to focus on the placement of that
cluster within the data. This R code follows this sequence of actions:

\begin{verbatim}
> gd <- ggobi(d.prim7)[1]
> clust9 <- cutree(d.prim7.dend, k=9)
> glyph_color(gd)[clust9==1] <- 9 # highlight triangle
> glyph_color(gd)[clust9==1] <- 1 # reset color
> glyph_color(gd)[clust9==2] <- 9 # highlight cluster 2
\end{verbatim}

The top three plots show, respectively, clusters 1, 2, and 3: These
clusters roughly divide the main triangular section of the data into
three. The bottom row of plots show clusters labeled 5, and 6, which
lie along the linear pieces, and cluster 7, which is a singleton
cluster corresponding to an outlier in the data.

The results are reasonably easy to interpret. Recall that the basic
geometry underlying this data is that there is a 2D triangle with two
linear strands extending from each vertex.  The hierarchical average
linkage clustering of the particle physics data using nine clusters
essentially divides the data into three chunks in the neighborhood of
each vertex (clusters 1, 2, and 3), three pieces at the ends of the
six linear strands (4, 5, and 6), and three clusters containing
outliers (7, 8, and 9).  This data provides a big challenge for any cluster
algorithm ~---~ low-dimensional pieces embedded in high-dimensional space
~---~ and we are not surprised that no algorithm that we have tried will
extract the structure we found using interactive tools.

The particle physics dataset is ill-suited to hierarchical clustering,
but this extreme failure is an example of a common problem.  When
performing cluster analysis, we want to group the observations into
clusters without knowing the distribution of the data.  How many
clusters are appropriate? What do the clusters look like?  Could we
just as confidently divide the data in several different ways and get
very different but equally valid interpretations? Graphics can help us
assess the results of a cluster analysis by helping us explore the
distribution of the data and the characteristics of the clusters.

\index{cluster analysis!model-based}

\subsection{Model-based clustering}

Model-based clustering \cite{FR02} fits a multivariate normal mixture
model to the data. It uses the EM algorithm to fit the parameters for
the mean, variance--covariance of each population, and the mixing
proportion. The variance--covariance matrix is re-parametrized using an
eigen-decomposition

\[
\Sigma_k = \lambda_kD_kA_kD_k', ~~~k=1, \dots, g ~~\mbox{(number of clusters)}
\]

\noindent resulting in several model choices, ranging from
simple to complex:

\begin{center}
\begin{tabular}{l@{\hspace{.1in}}l@{\hspace{.1in}}l@{\hspace{.1in}}l@{\hspace{.1in}}l@{\hspace{.15in}}l@{\hspace{.15in}}l} \hline
\T \B Name & $\Sigma_k$ & Distribution & Volume & Shape & Orientation \\ \hline
\T EII & $\lambda I$ & Spherical & equal & equal & NA & \\
VII & $\lambda_kI$ & Spherical & variable & equal & NA & \\
EEI & $\lambda A$ & Diagonal & equal & equal & coordinate axes & \\
VEI & $\lambda_kA$ & Diagonal & variable & equal & coordinate axes & \\
VVI & $\lambda_kA_k$ & Diagonal & variable & variable & coordinate axes & \\
EEE & $\lambda DAD'$ & Ellipsoidal & equal & equal & equal &\\
EEV & $\lambda D_kAD_k'$ & Ellipsoidal & equal & equal & variable & \\ 
VEV & $\lambda_k D_kAD_k'$ & Ellipsoidal & variable & equal & variable & \\ 
\B VVV & $\lambda_kD_kA_kD_k'$ & Ellipsoidal & variable & variable & variable & \\\hline
\end{tabular}
\end{center}

\noindent Note the distribution descriptions ``spherical'' and
``ellipsoidal.'' These are descriptions of the shape of the
variance--covariance for a multivariate normal distribution. A standard
multivariate normal distribution has a variance--covariance matrix with
zeros in the off-diagonal elements, which corresponds to
spherically shaped data.  When the variances (diagonals) are different
or the variables are correlated, then the shape of data from a
multivariate normal is ellipsoidal.

\index{Bayes Information Criterion (BIC)}

\index{datasets!\Data{Australian Crabs}}

The models are typically scored using the Bayes Information Criterion
(BIC), which is based on the log likelihood, number of variables, and
number of mixture components.  They should also be assessed using
graphical methods, as we demonstrate using the \Data{Australian Crabs}
data. 
%To introduce the methods 
We start with two of the five real-valued variables (\Vbl{frontal
lobe} and \Vbl{rear width}) and one \Vbl{species} (Blue).

\begin{verbatim}
> library(mclust)
> d.crabs <- read.csv("australian-crabs.csv")
> d.blue.crabs <- subset(d.crabs,
    species=="Blue", select=c(sex,FL:BD))
\end{verbatim}

\noindent The goal is to determine whether model-based methods can 
discover clusters that will distinguish between the two sexes.

% Figure 6 -- too large for the page, but maybe it can just
%   be reduced in the R code.
\begin{figure*}[htbp]
\centerline{\includegraphics[width=4.5in]{chap-clust/mclust.pdf}}
\caption[Model-based clustering of a reduced set of \Data{Australian
Crabs}]{Model-based clustering of a reduced set of \Data{Australian
Crabs}.  A scatterplot {\bf (top left)} shows the data with values of
\Vbl{sex} labeled.  A plot of the BIC values for the full range of
models {\bf (top right)} shows that the best model organizes the cases
into two clusters using EEV parametrization.  We label the cases by
cluster for the best model {\bf (middle left)}.  The remaining three
plots include ellipses representing the variance--covariance estimates
of the three best models, EEV-2 {\bf (middle right)}, EEV-3 {\bf
(bottom left)}, and VVV-2 {\bf (bottom right)}.}
\label{model-based1}
\end{figure*}

Figure~\ref{model-based1} contains the plots we will use to examine
the results of model-based clustering on this reduced dataset. The top
leftmost plot shows the data, with male and female crabs distinguished
by color and glyph.  The two sexes correspond to long cigar-shaped
objects that overlap a bit, particularly for smaller crabs.  The
``cigars'' are not perfectly regular: The variance of the data is
smaller at small values for both sexes, so that our cigars are
somewhat wedge-shaped.  The orientation of the longest direction of
variance differs slightly between groups too: The association has a
steeper slope for female crabs than for males, because
female crabs have relatively larger \Vbl{rear width} than male
crabs. With the heterogeneity in variance--covariance, this data does
not strictly adhere to the multivariate normal mixture model
underlying model-based methods, but we hope that the departure from
regularity is not so extreme that it prevents the model from working.

The top right plot shows the BIC results for a full range of models,
EEE, EEV, and VVV variance--covariance parametrization for one to nine
clusters:

\begin{verbatim}
> blue.crabBIC <- mclustBIC(subset(d.blue.crabs,
    select=c(FL,RW)), modelNames=c("EEE","EEV","VVV"))
> blue.crabBIC
 BIC:
        EEE       EEV       VVV
1 -810.3289 -810.3289 -810.3289
2 -820.7272 -778.6450 -783.7705
3 -832.7712 -792.5937 -821.8645
4 -824.8927 -835.5631 -835.7799
5 -805.8402 -805.9425 -853.1395
6 -807.8380 -821.1586 -879.3500
7 -827.1099 -860.7258 -878.0679
8 -833.8051 -861.1460 -891.9757
9 -835.6620 -854.6120 -904.6108
> plot(blue.crabBIC)
EEE EEV VVV 
 15  12   0 
\end{verbatim}

\noindent  The best model, EEV-2, used the equal volume, equal shape,
and different orientation variance--covariance parametrization and
divided the data into two clusters. This solution seems to be perfect!
We can imagine that this result corresponds to two equally shaped
ellipses that intersect near the lowest values of the data and angle
toward higher values. We will check by drawing ellipses representing
the variance--covariance parametrization on the data plots. The
parameter estimates are used to scale and center the ellipses:

\begin{verbatim}
> mclst1 <- mclustBIC(subset(d.blue.crabs,
    select=c(FL,RW)), G=2, modelNames="EEV")
> mclst1
 BIC:
       EEV
2 -778.645
> smry1 <- mclustModel(subset(d.blue.crabs,
    select=c(FL,RW)), mclst1, G=2, modelNames="EEV")
> vc <- smry1$parameters$variance$sigma[,,1]
> xm <- smry1$parameters$mean[,1]
> y1 <- f.vc.ellipse(vc,xm,500)
> ...
\end{verbatim}

\noindent yielding the plots in the middle and bottom rows of
Fig.~\ref{model-based1}.  In the plot of the data alone, cluster id is
used for the color and glyph of points.  (Compare this plot with the one
directly above it, in which the classes are known.)  Cluster 1 mostly
corresponds to the female crabs, and cluster 2 to the males, except
that all the small crabs, both male and female, have been assigned to
cluster 1. In the rightmost plot, we have added ellipses representing
the estimated variance--covariances. The ellipses are the same shape,
as specified by the model, but the ellipse for cluster 2 is shifted
toward the large values.

The next two best models, according to the BIC values, are EEV-3 and
VVV-2.  The plots in the bottom row display representations of the
variance--covariances for these models. EEV-3 organizes the crabs into
three clusters according to the size, not the sex, of the crabs. The
VVV-2 solution is similar to EEV-2.

What solution is the best for this data?  If the EEV-3 model had done
what we intuitively expected, it would have been ideal: The sexes of
smaller crabs are indistinguishable, so they should be afforded their
own cluster, whereas larger crabs could be clustered into males and
females.  However, the cluster that includes the small crabs also
includes a fair number of middle-sized female crabs.

Finally, model-based clustering did not discover the true gender
clusters.  Still, it produced a useful and interpretable clustering of
the crabs.

Plots are indispensable for choosing an appropriate cluster
model. It is easy to visualize the models when there are only two
variables but increasingly difficult as the number of variables
grows.  Tour methods save us from producing page upon page of plots.
They allow us to look at many projections of the data, which
enables us to conceptualize the shapes and relationships between
clusters in more than two dimensions.

Figure~\ref{model-based2} displays the graphics for the corresponding
high-dimensional investigation using all five variables and four
classes (two species, two sexes) of the \Data{Australian Crabs}. The
cluster analysis is much more difficult now. Can model-based
clustering uncover these four groups?

In the top row of plots, we display the raw data, before modeling.
Each plot is a tour projection of the data, colored according to the
four true classes.  The blue and purple points are the male and female
crabs of the blue species, and the yellow and orange points are the
male and female crabs of the orange species.  This table will help you
keep track:

\begin{center}
\begin{tabular}{l@{\hspace{.1in}}l@{\hspace{.1in}}l} \hline
\T \B & Male & Female \\ \hline
\T Blue Species & blue rectangles & purple circles \\ 
\B Orange Species & yellow circles & orange rectangles \\\hline
\end{tabular}
\end{center}

The clusters corresponding to the classes are long thin wedges in five
dimensions (5D), with more separation and more variability at larger
values, as we saw in the subset just discussed. The rightmost plot
shows the ``looking down the barrel'' view of the wedges.  At small
values the points corresponding to the sexes are mixed (leftmost
plot).  The species are reasonably well separated even for small crabs
(middle plot). The variance--covariance is wedge-shaped rather than
elliptical, but again we hope that modeling based on the normal
distribution that has elliptical variance--covariance will be
adequate.

% Figure 7
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=1.5in]{chap-clust/crabs7.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/crabs9.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/crabs11.pdf}}}
\smallskip
\centerline{{\includegraphics[width=1.5in]{chap-clust/crabs8.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/crabs10.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/crabs12.pdf}}}
\caption[Comparing the \Data{Australian Crabs} data with results of
model-based clustering]{Comparing the \Data{Australian Crabs} data
with results of model-based clustering using all variables.  Compare
the tour projections of the 5D data {\bf (top row)} with the 5D ellipses
corresponding to the variance--covariance in the four-cluster model
{\bf (bottom row)}. The ellipses of the four clusters do not match the four
known groups in the data.}
\label{model-based2}
\end{figure*}

In the results from model-based clustering, there is very little
difference in BIC value for variance--covariance models EEE, EEV, VEV,
and VVV, with a number of clusters from three to eight.  The best model is
EEV-3, and EEV-4 is second best. We know that three clusters is
insufficient to capture the four classes we have in mind, so we
examine the four-cluster solution. 

\begin{verbatim}
> mclst4 <- mclustBIC(subset(d.blue.crabs,select=c(FL:BD)),
    G=1:8, modelNames=c("EEE","EEV","VVV"))
> plot(mclst4)
EEE EEV VVV 
 15  12   0 
> mclst5 <- mclustBIC(subset(d.blue.crabs,select=c(FL:BD)), 
   G=4, modelNames="EEV")
> smry5 <- mclustModel(subset(d.blue.crabs,select=c(FL:BD)), 
   mclst5, G=4, modelNames="EEV")
\end{verbatim}

The bottom row of plots in Fig.~\ref{model-based2} illustrates the
four-cluster model in three different projections, matching the
projections in the top row showing the data.

\begin{verbatim}
> vc <- smry5$parameters$variance$sigma[,,1]
> mn <- smry5$parameters$mean[,1]
> y1 <- f.vc.ellipse(vc, mn)
> ...
> mclst5.model <- cbind(matrix(NA,500*4,3),
    rbind(y1,y2,y3,y4))
> colnames(mclst5.model) <-
    c("Species","Sex","Index","FL","RW","CL","CW","BD")
> d.crabs.model <- rbind(d.crabs, mclst5.model)
> gd <- ggobi(d.crabs.model)[1]
> glyph_color(gd) <- c(rep(4,50), rep(1,50), rep(9,50), 
    rep(6,50), rep(8,2000))
\end{verbatim}

\noindent In each view, the ellipsoids representing the
variance--covariance estimates for the four clusters are shown in four
shades of gray, because none of these match any actual cluster in the
data.  Remember that these are 2D projections of 5D ellipsoids.  The
resulting clusters from the model do not match the true classes very
well.  The result roughly captures the two species, as we see in the
plots in the first column, where the species are separated both in the
data and in the ellipses.  On the other hand, the grouping
corresponding to \Vbl{sex} is completely missed: See the plots in the
middle and right-hand columns, where sexes are separated in the actual
data but the ellipses are not separated. Just as in the smaller subset
(two variables, one species) discussed earlier, there is a cluster for
the smaller crabs of both species and sexes.  The results of
model-based clustering on the full 5D data are very unsatisfactory.

In summary, plots of the data and parameter estimates for model-based
cluster analysis are very useful for understanding the solution, and
choosing an appropriate model. Tours are very helpful for examining
the results in higher dimensions, for arbitrary numbers of variables.

% Also, using principal components rather than the raw variables...

\index{cluster analysis!self-organizing maps (SOM)}

\subsection{Self-organizing maps}

A self-organizing map (SOM) \cite{Ko01} is constructed using a
constrained $k$-means algorithm. A 1D or 2D net is stretched through
the data. The knots, in the net, form the cluster means, and the
points closest to the knot are considered to belong to that cluster.
The similarity of nodes (and their corresponding clusters) is defined
as proportional to their distance from one another on the net.

We will demonstrate SOM using the music data. The data has 62 cases,
each one corresponding to a piece of music.  For each piece there are
seven variables: the artist, the type of music, and five
characteristics, based on amplitude and frequency, that were computed
using the first 40 seconds of the piece on CD.  The music used
included popular rock songs by Abba, the Beatles, and the Eels;
classical compositions by Vivaldi, Mozart and Beethoven; and several
new wave pieces by Enya. Figure~\ref{music-som} displays a typical
view of the results of clustering using SOM on the music data. Each
data point corresponds to a piece of music and is labeled by the band
or the composer. The map was generated by this R code:

\begin{verbatim}
> library(som)
> d.music <- read.csv("music-sub.csv", row.names=1)
> d.music.std <- cbind(subset(d.music.std,
   select=c(artist,type)),
   apply(subset(d.music.std,select=lvar:lfreq), 
   2, f.std.data))
> music.som <- som(subset(d.music.std,select=lvar:lfreq), 
   6, 6, neigh="bubble", rlen=1000)
\end{verbatim}

The left plot in Fig.~\ref{music-som} is called the 2D map
view. Here we have used a $6\times 6$ net pulled through the 5D
data. The net that was wrapped through the high-dimensional space is
straightened and laid out flat, and the points, like fish in a fishing
net, are laid out where they have been trapped. In the plot shown
here, the points have been jittered slightly, away from the knots of
the net, so that the labels do not overlap too much. If the fit is
good, the points that are close together in this 2D map view are close
together in the high-dimensional data space and close to the net
as it was placed in the high-dimensional space.

Much of the structure in the map is no surprise: The rock (purple) and
classical tracks (green) are on opposing corners, with rock in the
upper right and classical in the lower left. The Abba tracks are all
grouped at the top and left of the map. The Beatles and Eels tracks
are mixed. There are some unexpected associations: For example, one
Beatles song, which turns out to be ``Hey Jude,'' is mixed among the
classical compositions!

% Figure 8
\begin{figure*}[htbp]
\centerline{
  {\includegraphics[width=2.4in]{chap-clust/music-som.pdf}}
  {\includegraphics[width=2.4in]{chap-clust/music-pca.pdf}}
}
\caption[Comparison of clustering music tracks using a self-organizing
map versus principal components]{Comparison of clustering music
tracks using a self-organizing map versus principal components.
The data was clustered using a self-organizing map, as shown in a 2D
map view.  (When tracks clustered at a node, jittering was used to
spread the labels.)  Compare with the scatterplot of the first two
principal components.}
\label{music-som}
\end{figure*}

Construction of a self-organizing map is a dimension reduction method,
akin to multidimensional scaling \cite{BG05} or principal component
analysis \cite{JW02}. Using principal component analysis to find a
low-dimensional approximation of the similarity between music pieces,
yields the second plot in Fig.~\ref{music-som}. There are many
differences between the two representations. The SOM has a more even
spread of music pieces across the grid, in contrast to the stronger
clumping of points in the PCA view.  Indeed, the PCA view shows
several outliers, notably one of the Vivaldi compositions, which could
lead us to learn things about the data that we might miss by relying
exclusively on the SOM.
\index{outliers}
\index{principal component analysis}
\index{multidimensional scaling (MDS)}

These two methods, SOM and PCA, have provided two contradictory
clustering models.  How can we determine which is the more accurate
description of the data structure?  An important part of model
assessment is plotting the model in relation to the data.  Although
plotting the low-dimensional map is the common way to graphically
assess the SOM results, it is woefully limited.  If a model has flaws,
they may not show up in this view and will only appear in plots of
the model in the data space.  We will use the grand tour to create
these plots, and this will help us assess the two models.

%Although the reduced dimension view is the common way to graphically
%assess the SOM results, it is woefully limited. What might appear to
%be an appealing result when seen in the map view may in fact be a poor
%fit, but we may not realize that without looking at plots of the model
%in the data space.

We will use a grand tour to view the net wrapped in among the data,
hoping to learn how the net converged to this solution, and how it
wrapped through the data space.  Actually, it is rather tricky to fit
a SOM: Like many algorithms, it has a number of parameters and
initialization conditions that affect the outcome. 

To set up the data, we will need to add variables containing the map
coordinates to the data:

\begin{verbatim}
> d.music.som <- f.ggobi.som(subset(d.music.std,
   select=lvar:lfreq), music.som)
\end{verbatim}

\noindent Because this data has several useful categorical 
labels for each row, we will want to keep this information in the data
when it is loaded into GGobi:

\begin{verbatim}
> d.music.som <- data.frame(
  Songs=factor(c(as.character(row.names(d.music)),
    rep("0",36))),
  artist=factor(c(as.character(d.music[,1]),rep("0",36))),
  type=factor(c(as.character(d.music[,2]),rep("0",36))),
  lvar=d.music.som[,1], 
  lave=d.music.som[,2], 
  lmax=d.music.som[,3],
  lfener=d.music.som[,4], lfreq=d.music.som[,5],
  Map.1=d.music.som[,6], Map.2=d.music.som[,7])
> gd <- ggobi(d.music.som)[1]
\end{verbatim}

\noindent Add the edges that form the SOM net:

\begin{verbatim}
> d.music.som.net <- f.ggobi.som.net(music.som)
> edges(gd) <- d.music.som.net + 62
\end{verbatim}

\noindent And finally color the points according to the type of music:

\begin{verbatim}
> gcolor <- rep(8,98)
> gcolor[d.music.som$Type=="Rock"] <- 6
> gcolor[d.music.som$Type=="Classical"] <- 4
> gcolor[d.music.som$Type=="New wave"] <- 1
> glyph_color(g) <- gcolor
\end{verbatim}

The results can be seen in Figs.~\ref{clust-SOMa} and
\ref{clust-SOMb}. The plots show two different states of the fitting
process and of the SOM net cast through the data. In both fits, a
$6\times 6$ grid is used and the net is initialized in the direction
of the first two principal components. In both fits the variables were
standardized to have mean equal to zero and standard deviation equal
to 1. The first SOM fit, shown in \ref{clust-SOMa}, was obtained using
the default settings; it gave terrible results.  At the left is the
map view, in which the fit looks deceptively reasonable.  The points
are spread evenly through the grid, with rock tracks (purple) at the
upper right, classical tracks (green) at the lower left, and new wave
tracks (the three black rectangles) in between.  The tour view in the
same figure, however, shows the fit to be inadequate.  The net is a
flat rectangle in the 5D space and has not sufficiently wrapped
through the data. This is the result of stopping the algorithm too
soon, thus failing to let it converge fully.

% Figure 9
\begin{figure*}[thbp]
\centerline{{\includegraphics[width=2in]{chap-clust/music-SOM1a-bad.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-SOM1b-bad.pdf}}}
\caption[Unsuccessful SOM fit]{Unsuccessful SOM fit shown in a 2D map
view and a tour projection.  Although the fit looks good in the map
view, the view of the fit in the 5D data space shows that the net has
not sufficiently wrapped into the data: The algorithm has not
converged fully.}
\label{clust-SOMa}
\end{figure*}

% Figure 10
\begin{figure*}[bhtp]
\centerline{{\includegraphics[width=2in]{chap-clust/music-SOM2f.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-SOM2b.pdf}}}
\smallskip
\centerline{{\includegraphics[width=2in]{chap-clust/music-SOM2d.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-SOM2e.pdf}}}
\caption[Successful SOM fit]{Successful SOM fit shown in a 2D map
view and tour projections.  Here we see a more successful SOM fit,
using standardized data.  The net wraps through the nonlinear
dependencies in the data, but some outliers remain.  }
\label{clust-SOMb}
\end{figure*}

Figure \ref{clust-SOMb} shows our favorite fit to the data. The data
was standardized, we used a 6$\times$6 net, and we ran the SOM algorithm for
1,000 iterations. The map is at the top left, and it matches the map
already shown in Fig.~\ref{music-som}, except for the small
jittering of points in the earlier figure. The other three plots show
different projections from the grand tour.  The upper right plot shows
how the net curves with the nonlinear dependency in the data: The net
is warped in some directions to fit the variance pattern.  At the
bottom right we see that one side of the net collects a long separated
cluster of the Abba tracks. We can also see that the net has not been
stretched out to the full extent of the range of the data. It is
tempting to manually manipulate the net to stretch it in different
directions and update the fit.

It turns out that the PCA view of the data more accurately reflects
the structure in the data than the map view.  The music pieces really
are clumped together in the 5D space, and there are a few outliers.
\index{outliers}

\subsection{Comparing methods}

\index{cluster analysis!confusion table}
\index{datasets!\Data{Music}}

To compare the results of two methods we commonly compute a confusion
table. For example, Table \ref{confusion} is the confusion table for
five-cluster solutions for the \Data{Music} data from $k$-means and
Ward's linkage hierarchical clustering, generated by:

\begin{verbatim}
> d.music.dist <- dist(subset(d.music.std,
   select=c(lvar:lfreq)))
> d.music.hc <- hclust(d.music.dist, method="ward")
> cl5 <- cutree(d.music.hc,5)
> d.music.km <- kmeans(subset(d.music.std,
   select=c(lvar:lfreq)), 5)
> table(d.music.km$cluster, cl5)
> d.music.clustcompare <- 
    cbind(d.music.std,cl5,d.music.km$cluster)
> names(d.music.clustcompare)[8] <- "Wards"
> names(d.music.clustcompare)[9] <- "km"
> gd <- ggobi(d.music.clustcompare)[1]
\end{verbatim}

\noindent The numerical labels of clusters are arbitrary, so these can
be rearranged to better digest the table. There is a lot of agreement
between the two methods: Both methods agree on the cluster for 48
tracks out of 62, or 77\% of the time.  We want to explore the data
space to see where the agreement occurs and where the two methods
disagree.

\begin{center}
\begin{table}[h]
\caption[Tables showing the agreement between two solutions for the
\Data{Music} data]{Tables showing the agreement between two
five-cluster solutions for the \Data{Music} data, showing a lot of
agreement between $k$-means and Ward's linkage hierarchical
clustering.  The rows have been rearranged to make the table more
readable.}
\begin{tabular}{cp{0.2in}p{1.3in}c}
\begin{tabular}{c@{\hspace{.1in}}|rrrrr}
& \multicolumn{5}{c}{Ward's} \\
\B $k$-means & ~1 & ~2 & ~3 & 4 & 5 \\\hline
\T 1 & 0 & 0 & 3 & 0 & 14 \\ 
2 & 0 & 0 & 1 & 0 & 0 \\
3 & 0 & 9 & 5 & 0 & 0 \\
4 & 8 & 2 & 1 & 0 & 0 \\
5 & 0 & 0 & 3 & 16 & 0 \\
\end{tabular}
& & Rearrange rows $\Rightarrow$  &
\begin{tabular}{c@{\hspace{.1in}}|rrrrr}
& \multicolumn{5}{c}{Ward's} \\
\B $k$-means & ~1 & ~2 & ~3 & 4 & 5 \\\hline
\T 4 & 8 & 2 & 1 & 0 & 0 \\
3 & 0 & 9 & 5 & 0 & 0 \\
2 & 0 & 0 & 1 & 0 & 0 \\
5 & 0 & 0 & 3 & 16 & 0 \\
1 & 0 & 0 & 3 & 0 & 14 \\ 
\end{tabular}
\end{tabular}
\label{confusion}
\vspace{.5em}
\end{table}
\end{center}

% Figure 11
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=2in]{chap-clust/music-hc1a.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-hc1b.pdf}}}
\smallskip
\centerline{{\includegraphics[width=2in]{chap-clust/music-hc2a.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-hc2b.pdf}}}
\caption[Comparing two five-cluster models of the \Data{Music} data
using confusion tables linked to tour plots]{Comparing two
five-cluster models of the \Data{Music} data using confusion tables
linked to tour plots.  In the confusion tables, $k$-means cluster
identifiers for each plot are plotted against Ward's linkage
hierarchical clustering ids.  (The values have been jittered.)  Two
different areas of agreement have been highlighted, and the tour
projections show the tightness of each cluster where the methods
agree. }
\label{clust-compare}
\end{figure*}

\index{jittering}

In Fig.~\ref{clust-compare}, we link jittered plots of the confusion
table for the two clustering methods with 2D tour plots of the
data. The first column contains two jittered plots of the confusion
table.  In the top row of the figure, we have highlighted a group of
14 points that both methods agree form a cluster, painting them as
orange triangles. From the plot at the right, we see that this cluster
is a closely grouped set of points in the data space. From the tour
axes we see that \Vbl{lvar} has the largest axis pointing in the
direction of the cluster separation, which suggests that music pieces
in this cluster are characterized by high values on \Vbl{lvar}
(variable 3 in the data); that is, they have large variance in
frequency. By further investigating which tracks are in this cluster,
we can learn that it consists of a mix of tracks by the Beatles
(``Penny Lane,'' ``Help,'' ``Yellow Submarine,'' ...)  and the Eels
(``Saturday Morning,'' ``Love of the Loveless,'' ...).

In the bottom row of the figure, we have highlighted a second group of
tracks that were clustered together by both methods, painting them
again using orange triangles.  In the plot to the right, we see that
this cluster is closely grouped in the data space.  Despite that, this
cluster is a bit more difficult to characterize.  It is oriented
mostly in the negative direction of \Vbl{lave} (variable 4), so it would
have smaller values on this variable. But this vertical direction in
the plot also has large contributions from variables 3 (\Vbl{lvar}) and 7
(\Vbl{lfreq}).  If you label these eight points on your own, you will see
that they are all Abba songs (``Dancing Queen,'' ``Waterloo,'' ``Mamma
Mia,'' ...).
% slightly reworded this last sentence to make it clear that this
% is not shown in any figure but still state the result.  dfs

We have explored two groups of tracks where the methods agree. In a
similar fashion, we could also explore the tracks where the methods
disagree.

\index{cluster analysis!cluster characterization}
\section{Characterizing clusters}

The final step in a cluster analysis is to characterize the clusters.
Actually, we have engaged in cluster characterization throughout the
examples, because it is an intrinsic part of assessing the results of
any cluster analysis.  If we cannot detect any numerical or qualitative
differences between clusters, then our analysis was not successful,
and we start over with a different distance metric or algorithm.

However, once we are satisfied that we have found a set of clusters
that can be differentiated from one another, we want to describe them
more formally, both quantitatively and qualitatively.  We characterize
them quantitatively by computing such statistics as cluster means and
standard deviations for each variable.  We can look at these results
in tables and in plots, and we can refine the qualitative descriptions
of the clusters we made during the assessment process.

\index{parallel coordinate plot}

The parallel coordinate plot is often used during this stage.
Figure~\ref{clust-char} shows the parallel coordinate plot for the
first of the clusters of music pieces singled out for study in the
previous section. Ward's hierarchical linkage and $k$-means both
agreed that these music pieces form a cluster.  Since the matrix and
the number of clusters are both small, we plot the raw data; for
larger problems, we might plot cluster statistics as well [see,
for example, \citeasnoun{DSP05}].

% figure 12
\begin{figure*}[htbp]
\begin{center}
  {\includegraphics[width=4in]{chap-clust/music-clust1.pdf}}
\end{center}
\caption[Characterizing clusters in a parallel coordinate
plot]{Characterizing clusters in a parallel coordinate plot.  The
highlighted profiles correspond to one cluster for which
Ward's hierarchical linkage and $k$-means were in agreement.  }
\label{clust-char}
\end{figure*}

This cluster containing a mix of Beatles and Eels music has high
values on \Vbl{lvar}, medium values of \Vbl{lave}, high values of
\Vbl{lmax}, high values of \Vbl{lfener}, and varied \Vbl{lfreq}
values. That is, these pieces of music have a large variance in
frequency, high frequency, and high energy relative to the other
music pieces.

%\newpage
\section{Recap}~\label{clust-recap}

% Generating clusters
% Interpreting clusters
% Model assessment
%    confusion tables
%    SOM -- grid
%    model-based  -- compare ellipse to shape

Graphics are invaluable for cluster analysis, whether they are used to
find clusters or to interpret and evaluate the results of a cluster
analysis arrived at by other means.

The spin and brush approach can be used to get an initial look at the
data and to find clusters, and occasionally, it is sufficient.  When
the clustering is the result of an algorithm, a very useful first step
is to paint the points by cluster membership and to look at the data to
see whether the clustering seems sensible.  How many clusters are
there, and how big are they?  What shape are they, and do they overlap
one another?  Which variables have contributed most to the clustering?
Can the clusters be qualitatively described?  All the plots we have
described can be useful: scatterplots, parallel coordinate plots, and
area plots, as well as static plots like dendrograms.

When the clusters have been generated by a model, we should also use
graphics to help us assess the model.  If the model makes
distributional assumptions, we can generate ellipses and compare them
with the clusters to see whether the shapes are consistent.  For
self-organizing maps the tour can assist in uncovering problems with
the fit, such as when the map wraps in on itself through the data
making it appear that some cases are far apart when they are truly
close together. A confusion table can come alive with linked brushing,
so that mismatches and agreements between methods can be
explored. \index{brushing!linked}

% Euclidean distance, vs standardizing each row...
% Principal component coordinates vs raw variables.

\section*{Exercises}
\addcontentsline{toc}{section}{Exercises}

\begin{enumerate}

\item Using the spin and brush method, uncover three clusters in the
\Data{Flea Beetles} data and confirm that these correspond to the
three species. (Hint: Transform the data to principal components and
enter these variables into the projection pursuit guided tour running
the holes index. Also the species should not be identified by color or
symbol, until the clusters have been uncovered.)

\item Run hierarchical clustering with average linkage on the
\Data{Flea Beetles} data (excluding \Vbl{species}).

\begin{enumerate}
\item Cut the tree at three clusters and append a cluster id to the dataset. 
How well do the clusters correspond to the species? (Plot
\Vbl{cluster id} vs \Vbl{species}, and use jittering if necessary.)
Using brushing in a plot of \Vbl{cluster id} linked to a tour plot of
the six variables, examine the beetles that are misclassified.

\item Now cut the tree at four clusters, and repeat the last part.

\item Which is the better solution, three or four clusters? Why?
\end{enumerate}

%\newpage
\item For the \Data{Italian Olive Oils},
\begin{enumerate}
\item Consider the oils from the four areas of Southern Italy. What
would you expect to be the result of model-based clustering on the
eight fatty acid variables?
\item Run model-based clustering on the Southern oils, with the goal
of extracting clusters corresponding to the four areas. What is the
best model? Create ellipsoids corresponding to the model and examine
these in a tour. Do they match your expectations?
\item Create ellipsoids corresponding to alternative models and use
these to decide on a best solution.
\end{enumerate}

\item This question uses the \Data{Rat Gene Expression} data.
\begin{enumerate}
\item Explore the patterns in expression level for the functional
classes. Can you characterize the expression patterns for each class?
\item How well do the cluster analysis results match the functional
classes? Where do they differ? 
\item Could you use the cluster analysis results to refine the
classification of genes into functional classes? How would you do
this?
\end{enumerate}

\item In the \Data{Music} data, make further comparisons of the five-cluster
solutions of $k$-means and Ward's hierarchical clustering.
\begin{enumerate}
\item On what tracks do the methods disagree?
\item Which track does $k$-means consider to be a singleton cluster,
while Ward's hierarchical clustering groups it with 12 other tracks?
\item Identify and  characterize the tracks in the four clusters where
both methods agree.
\end{enumerate}

\item In the \Data{Music} data, fit a $5\times 5$ grid SOM, and
observe the results for 100, 200, 500, and 1,000 updates. How does the
net change with the increasing number of updates?

\item There is a mystery dataset in the collection, called 
\Data{clusters-unknown.csv}. How many clusters are in this dataset?

\end{enumerate}

