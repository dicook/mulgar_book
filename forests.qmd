# Trees and forests {#sec-trees-forests}

## Trees {#sec-trees}

The tree algorithm @BFOS84 is a simple and versatile algorithmic method for supervised classification. The basic tree algorithm generates a classification rule by sequentially splitting the data into two buckets. Splits are made between sorted data values of individual variables, with the goal of obtaining pure classes on each side of the split. The inputs for a simple tree classifier commonly include (1) an impurity measure, an indication of the relative diversity among the cases in the terminal nodes; (2) a parameter that sets the minimum number of cases in a node, or the minimum number of observations in a terminal node of the tree; and (3) a complexity measure that controls the growth of a tree, balancing the use of a simple generalizable tree against a more accurate tree
tailored to the sample.  When applying tree methods, exploring the effects of the input parameters on the tree is instructive; for example, it helps us to assess the stability of the tree model.

Although algorithmic models do not depend on distributional assumptions, that does not mean that every algorithm is suitable for all data.  For example, the tree model works best when all variables are independent within each class, because it does not take such dependencies into account.  Visualization can help us to determine whether a particular model should be applied.  In classification problems, it is useful to explore the cluster structure, comparing the clusters with the classes and looking for evidence of correlation within each class. 
The plots in @fig-lda-assumptions1 and @fig-penguins-lda-ellipses shows a strong correlation between the variables within each species, which indicates that the tree model may not give good results for the penguins data. We'll show how this is the case with two variables initially, and then extend to the four variables.

::: {#fig-p-bl-bd-tree layout-ncol=2}

```{r}
#| message: false
#| label: fig-p-bl-bd-tree1
#| fig-cap: Default tree fit
#| fig-width: 3
#| fig-height: 3
library(mulgar)
library(rpart)
library(rpart.plot)
library(colorspace)
library(classifly)
library(ggplot2)

load("data/penguins_sub.rda")
p_bl_bd_tree <- rpart(species~bl+bd, data=penguins_sub)
rpart.plot(p_bl_bd_tree, box.palette="Grays")
```

```{r}
#| message: false
#| label: fig-p-bl-bd-tree2
#| fig-cap: Boundaries of tree fit
#| fig-width: 3
#| fig-height: 3
#| code-summary: "Code to draw tree and model fit"
p_bl_bd_tree_boundaries <- explore(p_bl_bd_tree, penguins_sub)
ggplot(p_bl_bd_tree_boundaries) +
  geom_point(aes(x=bl, y=bd, colour=species, shape=.TYPE)) + 
  scale_color_discrete_divergingx(palette="Zissou 1") +
  scale_shape_manual(values=c(46, 16)) +
  theme_minimal() +
  theme(aspect.ratio = 1, legend.position = "none")
```

The correlation between variables causes problems for using a tree model on the penguins data.
:::

The plots in @fig-p-bl-bd-tree show the inadequacies of the tree fit. The background color indicates the class predictions, and thus boundaries produced by the tree fit. They can be seen to be boxy, and missing the elliptical nature of the penguin clusters. This produces errors in the classification of observations which are indefensible. One could always force the tree to fit the data more closely by adjusting the parameters, but the main problem persists: that one is trying to fit elliptical data using boxes.

::: info
For a non-parametric model we do not need to check assumptions. However, it is still important to understand the model and the data to make sure they work well together. For example, a tree model may not be a good choice in case of correlated variables, since all interactions need to be described via alternating splits on the variables.
:::

The boundaries for the tree model on all four variables of the penguins data can be viewed similarly using the tour. The default fitted tree is delightfully simple, with just six splits of the data. 

```{r}
#| eval: false
#| code-summary: "Code to make animated gifs of slice tour of boundaries"
p_tree <- rpart(species~., data=penguins_sub[,1:5])
rpart.plot(p_tree, box.palette="Grays")

p_tree_boundaries <- explore(p_tree, penguins_sub)
animate_slice(p_tree_boundaries[p_tree_boundaries$.TYPE == "simulated",1:4], col=p_tree_boundaries[p_tree_boundaries$.TYPE == "simulated",6], v_rel=0.02, axes="bottomleft")
load("data/penguins_tour_path.rda")
render_gif(p_tree_boundaries[p_tree_boundaries$.TYPE == "simulated",1:4],
           planned_tour(pt1),
           display_slice(v_rel=0.02, 
             col=p_tree_boundaries[p_tree_boundaries$.TYPE == "simulated",6], 
             axes="bottomleft"),                     gif_file="gifs/penguins_tree_boundaries.gif",
           frames=500,
           loop=FALSE
           )
```

::: {#fig-penguins-lda-tree layout-ncol=2}

::: {.content-hidden when-format="pdf"}

![Boundaries produced by the LDA model.](gifs/penguins_lda_boundaries.gif){#fig-lda-boundary fig-alt="FIX ME" width=300}
:::

::: {.content-hidden when-format="pdf"}

![Boundaries produced by the tree model.](gifs/penguins_tree_boundaries.gif){#fig-tree-boundary fig-alt="FIX ME" width=300}
:::

Comparison of the boundaries produced by the LDA model and the tree models.
:::

\index{classification methods!random forest}

## Random forests 

A random forest [@Br01,@Cu04] is a classifier that is built from multiple trees generated by randomly sampling the cases and the variables.  The random sampling (with replacement) of cases has the fortunate effect of creating a training ("in-bag") and a test ("out-of-bag") sample for each tree computed.  The class of each case in the out-of-bag sample for each tree is predicted, and the predictions for all trees are combined into a vote for the class identity.  

A random forest is a computationally intensive method, a "black box" classifier, but it produces several diagnostics that make the outcome less mysterious.  Some diagnostics that help us to assess the model are the votes, the measure of variable importance, and the proximity matrix.

### Examining the votes matrix

Here we show how to use the `randomForest` [@randomForest2002] votes matrix for the penguins data to investigate confusion between classes, and observations which are problematic to classify. With only three classes the votes matrix is only a 2D object, and thus easy to examine. With four or more classes the votes matrix needs to be examined in a tour. 



```{r}
#| message: false
#| code-fold: false
library(randomForest)
library(dplyr)
penguins_rf <- randomForest(species~.,
                             data=penguins_sub[,1:5],
                             importance=TRUE)
penguins_rf
```


To examine the votes matrix, we extract the `votes` element from the random forest model object. This will have three columns corresponding to the three species, but because each row is a set of proportions it is only a 2D object, as seen in @fig-p-votes-tour. To reduce the dimension from 3D to the 2D we use a Helmert matrix [@helmert]. Helmert matrices are orthogonal matrices, where the first row is all 1's, and then subsequent rows sequentially replace one element with a 0. The rows are usually normalised to have length 1. They are used to create contrasts to test combinations of factor levels for post-testing after Analysis of Variance (ANOVA). For compositional data, like the votes matrix, when the first row is removed a Helmert matrix can be used to reduce the dimension appropriately. For three classes, this will generate the common 2D ternary diagram, but for higher dimensions it will reduce to a $(g-1)$-dimensional simplex. For the penguins data, the Helmert matrix for 3D is 

```{r}
#| code-summary: "Code to compute Helmert matrix"
geozoo::f_helmert(3)
``` 

We drop the first row, transpose it, and use matrix multiplication with the votes matrix to get the ternary diagram.

```{r}
#| message: false
#| code-fold: false
# Project 4D into 3D
library(geozoo)
proj <- t(geozoo::f_helmert(3)[-1,])
p_rf_v_p <- as.matrix(penguins_rf$votes) %*% proj
colnames(p_rf_v_p) <- c("x1", "x2")
p_rf_v_p <- p_rf_v_p %>%
  as.data.frame() %>%
  mutate(species = penguins_sub$species)
```

We can use the `geozoo` package to generate the surrounding simplex, which is a triangle for 2D.

```{r}
#| code-fold: false
# Add simplex
simp <- simplex(p=2)
sp <- data.frame(cbind(simp$points), simp$points[c(2,3,1),])
colnames(sp) <- c("x1", "x2", "x3", "x4")
sp$species = sort(unique(penguins_sub$species))
library(ggthemes)
p_ternary <- ggplot() +
  geom_segment(data=sp, aes(x=x1, y=x2, xend=x3, yend=x4)) +
  geom_text(data=sp, aes(x=x1, y=x2, label=species),
            nudge_x=c(-0.06, 0.07, 0),
            nudge_y=c(0.05, 0.05, -0.05)) +
  geom_point(data=p_rf_v_p, aes(x=x1, y=x2, colour=species), size=2, alpha=0.5) +
  scale_color_discrete_divergingx(palette="Zissou 1") +
  theme_map() +
  theme(aspect.ratio=1, legend.position="none")
```

```{r}
#| eval: false
#| code-summary: "Code to generate animated gifs"
# Look at the votes matrix, in its 3D space
animate_xy(penguins_rf$votes, col=penguins_sub$species)

# Save an animated gif
render_gif(penguins_rf$votes,
           grand_tour(),
           display_xy(v_rel=0.02, 
             col=penguins_sub$species, 
             axes="bottomleft"), 
           gif_file="gifs/penguins_rf_votes.gif",
           frames=500,
           loop=FALSE
)
```

::: {#fig-penguins-votes layout-ncol=2}

::: {.content-hidden when-format="pdf"}

![Votes matrix in a tour.](gifs/penguins_rf_votes.gif){#fig-p-votes-tour fig-alt="FIX ME" width=300}
:::

```{r}
#| echo: false
#| label: fig-p-votes-ggplot
#| fig-cap: Votes matrix in its 2D space, a ternary diagram.
#| fig-width: 4
#| fig-height: 4
p_ternary
```

Examining the votes matrix from a random forest fit to the penguins.
:::

The votes matrix reports the proportion of trees each observation is classified as each class. From the tour of the votes matrix, it can be seen to be 2D in 3D space. This is due to the constraint that the three proportions for each observation sum to 1. Using a Helmert matrix, this data can be projected into the 2D space, or more generally the $(g-1)$-dimensional space where it resides. In 2D this is called a ternary diagram, and in higher dimensions the bounding shapes might be considered to be a simplex. The vertices of this shape correspond to $(1,0,0), (0,1,0), (0,0,1)$ (and analogously for higher dimensions), which represent perfect confidence, that an observation is classified into that group all the time.

What we can see here is a concentration of points in the corners of the triangle indicates that most of the penguins are confidently classified into their correct class. Then there is more separation between the Gentoo and the others, than between Chinstrap and Adelie. That means that as a group Gentoo are more distinguishable. Only one of the Gentoo penguins has substantial confusion, mostly confused as a Chinstrap, but occasionally confused as an Adelie -- if it was only ever confused as a Chinstrap it would fall on the edge between Gentoo and Chinstrap. There are quite a few Chinstrap and Adelie penguins confused as each other, with a couple of each more confidently predicted to be the other class. This can be seen because there are points of the wrong colour close to those vertices. 

The votes matrix is useful for investigating the fit, but one should remember that there are some structural elements of this data that don't lend themselves to tree models. Although a forest has the capacity to generate non-linear boundaries by combining predictions from multiple trees, it is still based on the boxy boundaries of trees. This makes it less suitable for the penguins data with elliptical classes. You could use the techniques from the previous section to explore the boundaries produced by the forest, and you will find that the are more boxy than the LDA models.

To examine a vote matrix for a problem with more classes, we will examine the 10 class fake_trees data example. The full data has 100 variables, and we have seen from @sec-clust-graphics that reducing to 10 principal components allows the linear branching structure in the data to be seen. Given that the branches correspond to the classes, it will be interesting to see how well the random forest model performs.

```{r}
#| code-fold: false
library(mulgar)
library(dplyr)
library(liminal)
ft_pca <- prcomp(fake_trees[,1:100], 
                 scale=TRUE, retx=TRUE)
ft_pc <- as.data.frame(ft_pca$x[,1:10])
ft_pc$branches <- fake_trees$branches
library(randomForest)
ft_rf <- randomForest(branches~., data=ft_pc, 
                            importance=TRUE)
ft_rf
```

```{r}
#| code-fold: false
ft_rf_votes <- ft_rf$votes %>%
  as_tibble() %>%
  mutate(branches = fake_trees$branches)

proj <- t(geozoo::f_helmert(10)[-1,])
f_rf_v_p <- as.matrix(ft_rf_votes[,1:10]) %*% proj
colnames(f_rf_v_p) <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9")
f_rf_v_p <- f_rf_v_p %>%
  as.data.frame() %>%
  mutate(branches = fake_trees$branches)

simp <- geozoo::simplex(p=9)
sp <- data.frame(simp$points)
colnames(sp) <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9")
sp$branches = ""
f_rf_v_p_s <- bind_rows(sp, f_rf_v_p) %>%
  mutate(branches = factor(branches))
labels <- c("0" , "1", "2", "3", "4", "5", "6", "7", "8", "9",
                rep("", 3000))
```

```{r}
#| eval: false
#| code-summary: "Code to make animated gifs"
animate_xy(f_rf_v_p_s[,1:9], col = f_rf_v_p_s$branches, 
           axes = "off", half_range = 0.8,
           edges = as.matrix(simp$edges),
           obs_labels = labels, palette = "Viridis")

render_gif(f_rf_v_p_s[,1:9],
           grand_tour(),
           display_xy(col = f_rf_v_p_s$branches, 
           axes = "off", half_range = 0.8,
           edges = as.matrix(simp$edges),
           obs_labels = labels, palette="Viridis"),
           gif_file="gifs/ft_votes.gif",
           frames=500) 
```


::: {#fig-ft-votes layout-ncol=2}

::: {.content-hidden when-format="pdf"}

![The 9D votes matrix for the 10 class fake_trees data in a tour.](gifs/ft_votes.gif){#fig-ft-votes-tour fig-alt="FIX ME" width=300}
:::

![Several static views from the tour revealing how clusters connect.](images/ft-votes.png){#fig-ft-votes-prj fig-alt="FIX ME" width=300}

The votes matrix for the fake_trees data has a very striking geometric shape. The branching nature of the clusters is very clear. Most classes are distinct except for a connection with class 0.
:::

The votes matrix is 9D, but the structure of it is easy to read, and very interesting. The observations are coloured by class. There is one vertex (0) which has connections to all other vertexes. That is, there are points stretching without big breaks from this vertex to every other. It means that some observations in every other class can be confused with class 0, and class 0 observations can be confused with every other class. All of the other vertexes have a string of points almost entirely along one edge, the edge leading to vertex 0. This shows the lack of confusion with any other class, except 0. Cluster 0 could be considered the trunk of the tree, from which the other clusters grow. 

This pattern is what can be inferred from the confusion matrix, but we can't determine whether the clusters are mostly separated except for a few observations, or some other clustering shape. The visual pattern in the votes matrix is so striking, and gives additional information about the clustering distribution, and shapes of clusters. It reinforces the clusters are linear extending into different dimensions in the 100D space, but really only into about 8D (as we'll see from the variable importance explanation below). We also see that 9 of the clusters are all connected to one cluster.

::: info
By visualizing the votes matrix we can understand which observations are harder to classify, which of the classes are more easily confused with each other and if there are groups with similar patterns in the data.
:::

\index{R package!\RPackage{randomForest}}

### Using variable importance {#sec-forest-var-imp}

The variable importance score across all classes, and for each class is useful for choosing variables to enter into a tour, to explore class differences. This is particularly so when there are many variables, as in the fake_trees data. We would also expect that this data will have a difference between importance for some classes.

```{r}
#| code-fold: false
#| label: tbl-ft-importance
#| tbl-cap: Variable importance from the random forest fit to the fake_trees data.
library(gt)
ft_rf$importance %>% 
  as_tibble(rownames="Variable") %>% 
  rename(Accuracy=MeanDecreaseAccuracy,
         Gini=MeanDecreaseGini) %>%
  #arrange(desc(Gini)) %>%
  gt() %>%
  fmt_number(columns = c(`0`,`1`,`2`,`3`,`4`,`5`,`6`,`7`,`8`,`9`, Accuracy),
             decimals = 2) %>%
  fmt_number(columns = Gini,
             decimals = 0)
```

From the variable importance, we can see that PC9 and PC10 do not substantially contribute. That means the 100D data can be reduced to 8 PCs while maintaining the information about the clustering. PC1 is most important overall, but each cluster has a different set of variables that are important. For example, the variables important for distinguishing cluster 1 are PC1, PC2, PC4 and PC6, and for cluster 7 they are PC2, PC6, PC7. We can use this information to choose variables to provide to the tour. It can be helpful to reduce the class variable to focus on a particular class, by creating a new class variable, as follows. 

```{r}
#| code-fold: false
ft_pc <- ft_pc %>%
  mutate(cl1 = factor(case_when(
                 branches == "0" ~ "0",
                 branches == "1" ~ "1",
                 .default = "other"
  )))
```

```{r}
#| eval: false
#| code-summary: "Code to make animated gifs"
animate_xy(ft_pc[,c("PC1", "PC2", "PC4", "PC6")], col=ft_pc$cl1, palette="Viridis")
render_gif(ft_pc[,c("PC1", "PC2", "PC4", "PC6")],
           grand_tour(),
           display_xy(col=ft_pc$cl1, palette="Viridis"),
           gif_file="gifs/ft_cl1.gif",
           frames=500)
```

```{r}
#| code-summary: "Code to make plot"
ft_pc_cl1 <- ggplot(ft_pc, aes(x=PC4, y=PC2, col=cl1)) +
  geom_point(alpha=0.7, size=1) +
  scale_color_discrete_sequential(palette="Viridis", rev=FALSE) +
  theme_minimal() +
  theme(aspect.ratio = 1)
```

From @fig-ft-cl we can see how cluster 1 is distinct from all of the other observations, albeit with a close connection to the trunk of the tree (cluster 0). The distinction is visible in PC1, PC2, PC4, PC6, but can be seen clearly with just two of these.

::: {#fig-ft-cl layout-ncol=2}

::: {.content-hidden when-format="pdf"}

![Tour of most important variables for class 1.](gifs/ft_cl1.gif){#fig-ft-cl1 fig-alt="FIX ME" width=300}
:::

```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 4
#| out-width: 100%
#| label: fig-ft-cl1-pc
#| fig-cap: PC2 and PC4 together reveal cluster 1.
ft_pc_cl1 
```

Focusing on class 1 in the fake_trees data. The most important variables were PC1, PC2, PC4, PC6. A combination of PC2 and PC4 reveals the difference between cluster 1 and all the other clusters.
:::

For a problem with this many classes it can be useful to focus on several groups together. We've chosen cluster 8, because it appears to have less of a connection with cluster 0. See the light green cluster in bottom right static plot of @fig-ft-votes-prj is connected more to a different vertex, which is cluster 6 when carefully viewed. This is also suggested by the confusion matrix, where there is one observation from cluster 8 confused with cluster 6, although somewhat contradictory information, most are confused with cluster 0. We have also added cluster 1 to the investigation because it is closely connected to 6 and 8.

```{r}
#| code-fold: false
ft_pc <- ft_pc %>%
  mutate(cl8 = factor(case_when(
                 branches == "0" ~ "0",
                 branches == "6" ~ "6",
                 branches == "1" ~ "1",
                 branches == "8" ~ "8",
                 .default = "other"
  )))
```

```{r}
#| eval: false
#| code-summary: "Code to make animated gif"
animate_xy(ft_pc[,c("PC1", "PC2", "PC4", "PC5", "PC6")], col=ft_pc$cl8, palette="Viridis")
render_gif(ft_pc[,c("PC1", "PC2", "PC4", "PC5", "PC6")],
           grand_tour(),
           display_xy(col=ft_pc$cl8, palette="Viridis"),
           gif_file="gifs/ft_cl8.gif",
           frames=500)
```

```{r}
#| code-summary: "Code to make plot"
ft_pc_cl8 <- ggplot(ft_pc, aes(x=PC1, y=PC5, col=cl8)) +
  geom_point(alpha=0.7, size=1) +
  scale_color_discrete_sequential(palette="Viridis", rev=FALSE) +
  theme_minimal() +
  theme(aspect.ratio = 1)
```

From @fig-ft-cl2 we can see that clusters 1, 6, and 8 share one end of the trunk (cluster 0). Cluster 8 is almost more closely connected with cluster 6, though, than cluster 0. PC1 and PC5 mostly show the distinction between cluster 8 and the rest of the points, but it is clearer if more variables are used.

::: {#fig-ft-cl2 layout-ncol=2}

::: {.content-hidden when-format="pdf"}

![Tour of most important variables for class 1.](gifs/ft_cl8.gif){#fig-ft-cl8 fig-alt="FIX ME" width=300}
:::

```{r}
#| echo: false
#| fig-width: 4
#| fig-height: 4
#| out-width: 100%
#| label: fig-ft-cl8-pc
#| fig-cap: PC1 and PC5 together mostly reveal cluster 8.
ft_pc_cl8 
```

Focusing on class 8 in the fake_trees data, relative to nearby clusters 1 and 6. The most important variables for cluster 8 are  PC1, PC2, PC5, but to explore in association with clusters 1 and 6, we include PC4 and PC6. A combination of PC1 and PC5 reveals the difference between cluster 8, 6, 1 and 0.
:::

::: info
Variable importance can be used in feature selection. Looking at class-wise variable importance we can select a subspace that can separate a selected class and with a tour we can understand how this separation in multiple variables looks like. This works best when focusing on a small subset of classes.
:::

## Exercises {-}

1. Using a grand tour compare the boundaries from the random forest model on the penguins data to that of (a) a default tree model, (b) an LDA model. Is it less boxy than the tree model, but still more boxy than that of the LDA model?
2. Tinker with the parameters of the tree model to force it to fit a tree more closely to the data. Compare the boundaries from this with the default tree, and with the forest model. Is it less boxy than the default tree, but more boxy than the forest model?
3. Fit a random forest model to the `bushfires` data using the `cause` variable as the class. It is a highly imbalanced classification problem. What is the out-of-bag error rate for the forest? Are there some classes that have lower error rate than others? Examine the 4D votes matrix with a tour, and describe the confusion between classes. This is interesting because it is difficult to accurately classify the fire ignition cause, and only some groups are often confused with each other. You should be able to see this from the 3D votes matrix. 
4. Explore the 5D votes matrix for a random forest on the sketches data. Why does it look star-shaped?
5. Choose a cluster (or group of clusters) from the fake_trees data (2, 3, 4, 5, 7, 9) to explore in detail like done in @sec-forest-var-imp. Be sure to choose which PCs are the most useful using a tour, and follow-up by making a scatterplot showing the best distinction between your chosen cluster and the other observations. 

```{r}
#| eval: false
#| echo: false
library(mulgar)
library(tourr)
data(bushfires)

bushfires_sub <- bushfires[,c(5, 8:45, 48:55, 57:60)] %>%
  mutate(cause = factor(cause))

# Checking the dependencies between predictors
bushfires_pca <- prcomp(bushfires_sub[,-51],
                        scale=TRUE, retx=TRUE)
ggscree(bushfires_pca)

bushfires_pcs <- bushfires_pca$x[,1:7] %>%
  as_tibble() %>%
  mutate(cause = factor(bushfires$cause))

library(tourr)
animate_xy(bushfires_pcs[,1:7],
           guided_tour(lda_pp(bushfires_pcs$cause)),
           col=bushfires_pcs$cause)

bushfires_pca$rotation[,2]
ggplot(bushfires, aes(x=FOR_CODE)) + geom_density()
ggplot(bushfires, aes(x=COVER)) + geom_density()
ggplot(bushfires, aes(x=HEIGHT)) + geom_density()
ggplot(bushfires, aes(x=FOREST)) + geom_density()
ggplot(bushfires, aes(x=arf28)) + geom_density()

library(randomForest)
bushfires_rf <- randomForest(cause~.,
                             data=bushfires_sub,
                             importance=TRUE)
bushfires_rf

# Create votes matrix data
bushfires_rf_votes <- bushfires_rf$votes %>%
  as_tibble() %>%
  mutate(cause = bushfires_sub$cause)

# Project 4D into 3D
library(geozoo)
proj <- t(geozoo::f_helmert(4)[-1,])
b_rf_v_p <- as.matrix(bushfires_rf_votes[,1:4]) %*% proj
colnames(b_rf_v_p) <- c("x1", "x2", "x3")
b_rf_v_p <- b_rf_v_p %>%
  as.data.frame() %>%
  mutate(cause = bushfires_sub$cause)
  
# Add simplex
simp <- simplex(p=3)
sp <- data.frame(simp$points)
colnames(sp) <- c("x1", "x2", "x3")
sp$cause = ""
b_rf_v_p_s <- bind_rows(sp, b_rf_v_p) %>%
  mutate(cause = factor(cause))
labels <- c("accident" , "arson", 
                "burning_off", "lightning", 
                rep("", nrow(b_rf_v_p)))

# Check votes matrix
animate_xy(bushfires_rf_votes[,1:4],
           col=bushfires_rf_votes$cause)

# Examine votes matrix with bounding simplex
animate_xy(b_rf_v_p_s[,1:3], col = b_rf_v_p_s$cause, 
           axes = "off", half_range = 1.3,
           edges = as.matrix(simp$edges),
           obs_labels = labels)
render_gif(b_rf_v_p_s[,1:3],
           grand_tour(),
           display_xy(col = b_rf_v_p_s$cause, 
           axes = "off", half_range = 1.3,
           edges = as.matrix(simp$edges),
           obs_labels = labels),
           gif_file="gifs/bushfires_votes.gif",
           frames=500)  

library(gt)
bushfires_rf$importance %>% 
  as_tibble(rownames="Variable") %>% 
  rename(Accuracy=MeanDecreaseAccuracy,
         Gini=MeanDecreaseGini) %>%
  arrange(desc(Gini)) %>%
  gt() %>%
  fmt_number(columns = c(accident, arson, burning_off, lightning, Accuracy),
             decimals = 4) %>%
  fmt_number(columns = Gini,
             decimals = 2)
```

```{r}
#| eval: false
#| echo: false
# Answer to Q3
There are four classes: accident, arson, burning_off, lightning. It is highly imbalanced, with most observations belonging to the lightning class, fires ignited by lightning. 

We can see that most of the observations lie on the face of lightning, arson and accident. The handful of the burning_off observations lie off this plane, in the direction of burning-off, so are less confused with the other three classes. This could be expected because burning off is highly regulated, and tends to occur before the bushfire season is at risk of starting. The arson cases are hard to classify, frequently confused with lightning or accident, and occasionally burning off. Lightning and accident have many more observations that are confidently classified correctly. 
```

```{r}
#| eval: false
#| echo: false
library(mulgar)
library(dplyr)
data("sketches_train")
sketches_pca <- prcomp(sketches_train[,1:784])
ggscree(sketches_pca, guide=FALSE)
sketches_pc <- as.data.frame(sketches_pca$x[,1:21])
sketches_pc$word <- sketches_train$word

library(tourr)
animate_xy(sketches_pc[,1:6],
           tour=guided_tour(lda_pp(sketches_pc$word)), 
           col=sketches_pc$word)
library(randomForest)
sketches_rf <- randomForest(word~., data=sketches_pc, 
                            mtry=5, ntree=2500, 
                            importance=TRUE)
sketches_rf$importance
# This would be a good one to explain how to explore multiclass
# Difference PCs are more important for some classes
# Create new binary classes to explore.

sketches_rf_votes <- sketches_rf$votes %>%
  as_tibble() %>%
  mutate(word = sketches_train$word)

proj <- t(geozoo::f_helmert(6)[-1,])
s_rf_v_p <- as.matrix(sketches_rf_votes[,1:6]) %*% proj
colnames(s_rf_v_p) <- c("x1", "x2", "x3", "x4", "x5")
s_rf_v_p <- s_rf_v_p %>%
  as.data.frame() %>%
  mutate(word = sketches_train$word)

simp <- geozoo::simplex(p=5)
sp <- data.frame(simp$points)
colnames(sp) <- c("x1", "x2", "x3", "x4", "x5")
sp$word = ""
s_rf_v_p_s <- bind_rows(sp, s_rf_v_p) %>%
  mutate(word = factor(word))
labels <- c("banana" , "boomerang", 
                "cactus", "crab", "flip flops", "kangaroo",
                rep("", 5998))
animate_xy(s_rf_v_p_s[,1:5], col = s_rf_v_p_s$word, 
           axes = "off", half_range = 0.8,
           edges = as.matrix(simp$edges),
           obs_labels = labels)

render_gif(s_rf_v_p_s[,1:5],
           grand_tour(),
           display_xy(col = s_rf_v_p_s$cause, 
           axes = "off", half_range = 0.8,
           edges = as.matrix(simp$edges),
           obs_labels = labels),
           gif_file="gifs/sketches_votes.gif",
           frames=500)  
```
