# Trees and forests

<!-- Topics to include:

- trees
- forests
- boosted trees
- PP forest
-->

## Trees

The tree algorithm @BFOS84 is a simple and versatile algorithmic method for supervised classification. The basic tree algorithm generates a classification rule by sequentially splitting the data into two buckets. Splits are made between sorted data values of individual variables, with the goal of obtaining pure classes on each side of the split. The inputs for a simple tree classifier commonly include (1) an impurity measure, an indication of the relative diversity among the cases in the terminal nodes; (2) a parameter that sets the minimum number of cases in a node, or the minimum number of observations in a terminal node of the tree; and (3) a complexity measure that controls the growth of a tree, balancing the use of a simple generalizable tree against a more accurate tree
tailored to the sample.  When applying tree methods, exploring the effects of the input parameters on the tree is instructive; for example, it helps us to assess the stability of the tree model.

Although algorithmic models do not depend on distributional assumptions, that does not mean that every algorithm is suitable for all data.  For example, the tree model works best when all variables are independent within each class, because it does not take such dependencies into account.  Visualization can help us to determine whether a particular model should be applied.  In classification problems, it is useful to explore the cluster structure, comparing the clusters with the classes and looking for evidence of correlation within each class. 
The plots in @fig-lda-assumptions1 and @fig-penguins-lda-ellipses shows a strong correlation between the variables within each species, which indicates that the tree model may not give good results for the penguins data. We'll show how this is the case with two variables initially, and then extend to the four variables.

::: {#fig-p-bl-bd-tree layout-ncol=2}

```{r}
#| message: false
#| label: fig-p-bl-bd-tree1
#| fig-cap: Default tree fit
#| fig-width: 3
#| fig-height: 3
library(mulgar)
library(rpart)
library(rpart.plot)
library(colorspace)
library(classifly)
library(ggplot2)

load("data/penguins_sub.rda")
p_bl_bd_tree <- rpart(species~bl+bd, data=penguins_sub)
rpart.plot(p_bl_bd_tree, box.palette="Grays")
```

```{r}
#| message: false
#| label: fig-p-bl-bd-tree2
#| fig-cap: Boundaries of tree fit
#| fig-width: 3
#| fig-height: 3
p_bl_bd_tree_boundaries <- explore(p_bl_bd_tree, penguins_sub)
ggplot(p_bl_bd_tree_boundaries) +
  geom_point(aes(x=bl, y=bd, colour=species, shape=.TYPE)) + 
  scale_color_discrete_qualitative("Dark 3") +
  scale_shape_manual(values=c(46, 16)) +
  theme_minimal() +
  theme(aspect.ratio = 1, legend.position = "none")
```

The correlation between variables causes problems for using a tree model on the penguins data.
:::

The plots in @fig-p-bl-bd-tree show the inadequacies of the tree fit. The background color indicates the class predictions, and thus boundaries produced by the tree fit. They can be seen to be boxy, and missing the elliptical nature of the penguin clusters. This produces errors in the classification of observations which are indefensible. One could always force the tree to fit the data more closely by adjusting the parameters, but the main problem persists: that one is trying to fit elliptical data using boxes.

The boundaries for the tree model on all four variables of the penguins data can be viewed similarly using the tour. The default fitted tree is delightfully simple, with just six splits of the data. 

```{r}
#| eval: false
p_tree <- rpart(species~., data=penguins_sub)
rpart.plot(p_tree, box.palette="Grays")

p_tree_boundaries <- explore(p_tree, penguins_sub)
animate_slice(p_tree_boundaries[p_tree_boundaries$.TYPE == "simulated",1:4], col=p_tree_boundaries[p_tree_boundaries$.TYPE == "simulated",6], v_rel=0.02, axes="bottomleft")
load("data/penguins_tour_path.rda")
render_gif(p_tree_boundaries[p_tree_boundaries$.TYPE == "simulated",1:4],
           planned_tour(pt1),
           display_slice(v_rel=0.02, 
             col=p_tree_boundaries[p_tree_boundaries$.TYPE == "simulated",6], 
             axes="bottomleft"),                     gif_file="gifs/penguins_tree_boundaries.gif",
           frames=500,
           loop=FALSE
           )
```

::: {#fig-penguins-lda-tree layout-ncol=2}

![Boundaries produced by the LDA model.](gifs/penguins_lda_boundaries.gif){#fig-lda-boundary fig-alt="FIX ME" width=300}

![Boundaries produced by the tree model.](gifs/penguins_tree_boundaries.gif){#fig-tree-boundary fig-alt="FIX ME" width=300}

Comparison of the boundaries produced by the LDA model and the tree models.
:::


\index{classification methods!random forest}

## Random forests 

A random forest [@Br01,@Cu04] is a classifier that is built from multiple trees generated by randomly sampling the cases and the variables.  The random sampling (with replacement) of cases has the fortunate effect of creating a training ("in-bag") and a test ("out-of-bag") sample for each tree computed.  The class of each case in the out-of-bag sample for each tree is predicted, and the predictions for all trees are combined into a vote for the class identity.  

A random forest is a computationally intensive method, a "black box" classifier, but it produces several diagnostics that make the outcome less mysterious.  Some diagnostics that help us to assess the model are the votes, the measure of variable importance, and the proximity matrix.

### Examining the votes matrix

Here we show how to use the `randomForest` [@Li06] votes matrix for the penguins data to investigate confusion between classes, and observations which are problematic to classify. With only three classes the votes matrix is only a 2D object, and thus easy to examine. With four or more classes the votes matrix needs to be examined in a tour. 

The random forest fit is very good, with only a few misclassifications. 

```{r}
#| message: false
#| code-fold: false
library(randomForest)
library(dplyr)
penguins_rf <- randomForest(species~.,
                             data=penguins_sub,
                             importance=TRUE)
penguins_rf
```


To examine the votes matrix, we extract the `votes` element from the random forest model object. This will have three columns corresponding to the three species, but because each row is a set of proportions it is only a 2D object, as seen in @fig-p-votes-tour. To reduce the dimension from 3D to the 2D we use a Helmert matrix [@helmert]. Helmert matrices are orthogonal matrices, where the first row is all 1's, and then subsequent rows sequentially replace one element with a 0. The rows are usually normalised to have length 1. They are used to create contrasts to test combinations of factor levels for post-testing after Analysis of Variance (ANOVA). For compositional data, like the votes matrix, when the first row is removed a Helmert matrix can be used to reduce the dimension appropriately. For three classes, this will generate the common 2D ternary diagram, but for higher dimensions it will reduce to a $(g-1)$-dimensional simplex. For the penguins data, the Helmert matrix for 3D is 

```{r}
geozoo::f_helmert(3)
``` 

We drop the first row, transpose it, and use matrix multiplication with the votes matrix to get the ternary diagram.

```{r}
#| message: false
#| code-fold: false
# Project 4D into 3D
library(geozoo)
proj <- t(geozoo::f_helmert(3)[-1,])
p_rf_v_p <- as.matrix(penguins_rf$votes) %*% proj
colnames(p_rf_v_p) <- c("x1", "x2")
p_rf_v_p <- p_rf_v_p %>%
  as.data.frame() %>%
  mutate(species = penguins_sub$species)
```

We can use the `geozoo` package to generate the surrounding simplex, which is a triangle for 2D.

```{r}
#| code-fold: false
# Add simplex
simp <- simplex(p=2)
sp <- data.frame(cbind(simp$points), simp$points[c(2,3,1),])
colnames(sp) <- c("x1", "x2", "x3", "x4")
sp$species = sort(unique(penguins_sub$species))
library(ggthemes)
p_ternary <- ggplot() +
  geom_segment(data=sp, aes(x=x1, y=x2, xend=x3, yend=x4)) +
  geom_text(data=sp, aes(x=x1, y=x2, label=species),
            nudge_x=c(-0.06, 0.07, 0),
            nudge_y=c(0.05, 0.05, -0.05)) +
  geom_point(data=p_rf_v_p, aes(x=x1, y=x2, colour=species), size=2, alpha=0.5) +
  scale_color_discrete_qualitative("Dark 3") +
  theme_map() +
  theme(aspect.ratio=1, legend.position="none")
```

```{r}
#| eval: false
# Look at the votes matrix, in its 3D space
animate_xy(penguins_rf$votes, col=penguins_sub$species)

# Save an animated gif
render_gif(penguins_rf$votes,
           grand_tour(),
           display_xy(v_rel=0.02, 
             col=penguins_sub$species, 
             axes="bottomleft"), 
           gif_file="gifs/penguins_rf_votes.gif",
           frames=500,
           loop=FALSE
)
```

::: {#fig-penguins-votes layout-ncol=2}

![Votes matrix in a tour.](gifs/penguins_rf_votes.gif){#fig-p-votes-tour fig-alt="FIX ME" width=300}

```{r}
#| echo: false
#| label: fig-p-votes-ggplot
#| fig-cap: Votes matrix in its 2D space, a ternary diagram.
#| fig-width: 4
#| fig-height: 4
p_ternary
```

Examining the votes matrix from a random forest fit to the penguins.
:::


The votes matrix, reports the proportion of trees each observation is classified as each class. From the tour of the votes matrix, it can be seen to be 2D in 3D space. This is due to the constraint that the three proportions for each observation sum to 1. Using a Helmert matrix, this data can be projected into the 2D space, or more generally the $(g-1)$-dimensional space where it resides. In 2D this is called a ternary diagram, and in higher dimensions the bounding shapes might be considered to be a simplex. The vertices of this shape correspond to $(1,0,0), (0,1,0), (0,0,1)$ (and analogously for higher dimensions), which represent perfect confidence, that an observation is classified into that group all the time.

What we can see here is a concentration of points in the corners of the triangle indicates that most of the penguins are confidently classified into their correct class. Then there is more separation between the Gentoo and the others, than between Chinstrap and Adelie. That means that as a group Gentoo are more distinguishable. Only one of the Gentoo penguins has substantial confusion, mostly confused as a Chinstrap, but occasionally confused as an Adelie -- if it was only ever confused as a Chinstrap it would fall on the edge between Gentoo and Chinstrap. There are quite a few Chinstrap and Adelie penguins confused as each other, with a couple of each more confidently predicted to be the other class. This can be seen because there are points of the wrong colour close to those vertices. 

The votes matrix is useful for investigating the fit, but one should remember that there are some structural elements of this data that don't lend themselves to tree models. Although a forest has the capacity to generate non-linear boundaries by combining predictions from multiple trees, it is still based on the boxy boundaries of trees. This makes it less suitable for the penguins data with elliptical classes. You could use the techniques from the previous section to explore the boundaries produced by the forest, and you will find that the are more boxy than the LDA models.

To examine a vote matrix for a problem with more classes, we'll look at the bushfires data. There are four classes: accident, arson, burning_off, lightning. It is highly imbalanced, with most observations belonging to the lightning class, fires ignited by lightning. We can see that most of the observations lie on the face of lightning, arson and accident. The handful of the burning_off observations lie off this plane, in the direction of burning-off, so are less confused with the other three classes. This could be expected because burning off is highly regulated, and tends to occur before the bushfire season is at risk of starting. The arson cases are hard to classify, frequently confused with lightning or accident, and occasionally burning off. Lightning and accident have many more observations that are confidently classified correctly. 

```{r}
#| eval: false
library(mulgar)
library(tourr)
data(bushfires)

bushfires_sub <- bushfires[,c(5, 8:45, 48:55, 57:60)] %>%
  mutate(cause = factor(cause))

bushfires_pca <- prcomp(bushfires_sub[,-51],
                        scale=TRUE, retx=TRUE)
ggscree(bushfires_pca)

bushfires_pcs <- bushfires_pca$x[,1:7] %>%
  as_tibble() %>%
  mutate(cause = factor(bushfires$cause))

library(tourr)
animate_xy(bushfires_pcs[,1:7],
           guided_tour(lda_pp(bushfires_pcs$cause)),
           col=bushfires_pcs$cause)

bushfires_pca$rotation[,2]
ggplot(bushfires, aes(x=FOR_CODE)) + geom_density()
ggplot(bushfires, aes(x=COVER)) + geom_density()
ggplot(bushfires, aes(x=HEIGHT)) + geom_density()
ggplot(bushfires, aes(x=FOREST)) + geom_density()
ggplot(bushfires, aes(x=arf28)) + geom_density()

library(randomForest)
bushfires_rf <- randomForest(cause~.,
                             data=bushfires_sub,
                             importance=TRUE)
bushfires_rf_votes <- bushfires_rf$votes %>%
  as_tibble() %>%
  mutate(cause = bushfires_sub$cause)

animate_xy(bushfires_rf_votes[,1:4],
           col=bushfires_rf_votes$cause)

# Project 4D into 3D
library(geozoo)
proj <- t(geozoo::f_helmert(4)[-1,])
b_rf_v_p <- as.matrix(bushfires_rf_votes[,1:4]) %*% proj
colnames(b_rf_v_p) <- c("x1", "x2", "x3")
b_rf_v_p <- b_rf_v_p %>%
  as.data.frame() %>%
  mutate(cause = bushfires_sub$cause)
  
# Add simplex
simp <- simplex(p=3)
sp <- data.frame(simp$points)
colnames(sp) <- c("x1", "x2", "x3")
sp$cause = ""
b_rf_v_p_s <- bind_rows(sp, b_rf_v_p) %>%
  mutate(cause = factor(cause))
labels <- c("accident" , "arson", 
                "burning_off", "lightning", 
                rep("", 1021))
animate_xy(b_rf_v_p_s[,1:3], col = b_rf_v_p_s$cause, 
           axes = "off", half_range = 1.3,
           edges = as.matrix(simp$edges),
           obs_labels = labels)
render_gif(b_rf_v_p_s[,1:3],
           grand_tour(),
           display_xy(col = b_rf_v_p_s$cause, 
           axes = "off", half_range = 1.3,
           edges = as.matrix(simp$edges),
           obs_labels = labels),
           gif_file="gifs/bushfires_votes.gif",
           frames=500)  
```

![The 3D votes matrix for the four class bushfires data in a tour.](gifs/bushfires_votes.gif){#fig-b-votes fig-alt="FIX ME" width=300}

\index{R package!\RPackage{randomForest}}

### Using variable importance

bushfires data

## Examining misclassifications

Need a linked brushing example showing tour and scatterplot.

## Exercises {-}

1. Compare the boundaries from the random forest model on the penguins data to that of the (a) LDA model, (b) a default tree model.
2. Tinker with the parameters of the tree model to force it to fit a tree more closely to the data. Compare the boundaries from this with the default tree, and with the forest model. Is it less boxy than the default tree, but more boxy than the forest model?
3. Fit a random forest model to the forst 10 PCs of `fake_trees` data using the `branches` variable as the class. What is the out-of-bag error rate for the forest? Are there some classes that have lower error rate than others? Examine the 9D votes matrix with a tour. 
4. Explore the 5D votes matrix for a random forest on the sketches data. Why does it look star-shaped?

```{r}
#| eval: false
#| echo: false
library(mulgar)
library(dplyr)
library(liminal)
ft_pca <- prcomp(fake_trees[,1:100], 
                 scale=TRUE, retx=TRUE)
ft_pc <- as.data.frame(ft_pca$x[,1:10])
ft_pc$branches <- fake_trees$branches
library(randomForest)
ft_rf <- randomForest(branches~., data=ft_pc, 
                            importance=TRUE)
ft_rf
ft_rf$importance

ft_rf_votes <- ft_rf$votes %>%
  as_tibble() %>%
  mutate(branches = fake_trees$branches)

proj <- t(geozoo::f_helmert(10)[-1,])
f_rf_v_p <- as.matrix(ft_rf_votes[,1:10]) %*% proj
colnames(f_rf_v_p) <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9")
f_rf_v_p <- f_rf_v_p %>%
  as.data.frame() %>%
  mutate(branches = fake_trees$branches)

simp <- geozoo::simplex(p=9)
sp <- data.frame(simp$points)
colnames(sp) <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9")
sp$branches = ""
f_rf_v_p_s <- bind_rows(sp, f_rf_v_p) %>%
  mutate(branches = factor(branches))
labels <- c("0" , "1", "2", "3", "4", "5", "6", "7", "8", "9",
                rep("", 3000))
animate_xy(f_rf_v_p_s[,1:9], col = f_rf_v_p_s$branches, 
           axes = "off", half_range = 0.8,
           edges = as.matrix(simp$edges),
           obs_labels = labels)

render_gif(f_rf_v_p_s[,1:9],
           grand_tour(),
           display_xy(col = f_rf_v_p_s$branches, 
           axes = "off", half_range = 0.8,
           edges = as.matrix(simp$edges),
           obs_labels = labels),
           gif_file="gifs/ft_votes.gif",
           frames=500) 
```

```{r}
#| eval: false
#| echo: false
library(mulgar)
library(dplyr)
data("sketches_train")
sketches_pca <- prcomp(sketches_train[,1:784])
ggscree(sketches_pca, guide=FALSE)
sketches_pc <- as.data.frame(sketches_pca$x[,1:21])
sketches_pc$word <- sketches_train$word

library(tourr)
animate_xy(sketches_pc[,1:6],
           tour=guided_tour(lda_pp(sketches_pc$word)), 
           col=sketches_pc$word)
library(randomForest)
sketches_rf <- randomForest(word~., data=sketches_pc, 
                            mtry=5, ntree=2500, 
                            importance=TRUE)
sketches_rf$importance
# This would be a good one to explain how to explore multiclass
# Difference PCs are more important for some classes
# Create new binary classes to explore.

sketches_rf_votes <- sketches_rf$votes %>%
  as_tibble() %>%
  mutate(word = sketches_train$word)

proj <- t(geozoo::f_helmert(6)[-1,])
s_rf_v_p <- as.matrix(sketches_rf_votes[,1:6]) %*% proj
colnames(s_rf_v_p) <- c("x1", "x2", "x3", "x4", "x5")
s_rf_v_p <- s_rf_v_p %>%
  as.data.frame() %>%
  mutate(word = sketches_train$word)

simp <- geozoo::simplex(p=5)
sp <- data.frame(simp$points)
colnames(sp) <- c("x1", "x2", "x3", "x4", "x5")
sp$word = ""
s_rf_v_p_s <- bind_rows(sp, s_rf_v_p) %>%
  mutate(word = factor(word))
labels <- c("banana" , "boomerang", 
                "cactus", "crab", "flip flops", "kangaroo",
                rep("", 5998))
animate_xy(s_rf_v_p_s[,1:5], col = s_rf_v_p_s$word, 
           axes = "off", half_range = 0.8,
           edges = as.matrix(simp$edges),
           obs_labels = labels)

render_gif(s_rf_v_p_s[,1:5],
           grand_tour(),
           display_xy(col = s_rf_v_p_s$cause, 
           axes = "off", half_range = 0.8,
           edges = as.matrix(simp$edges),
           obs_labels = labels),
           gif_file="gifs/sketches_votes.gif",
           frames=500)  
```

<!--
1. For the \Data{Italian Olive Oils}:
    a. Split the samples from North Italy into $2/3$ training and
$1/3$ test samples for each area.
    b. Build a tree model to classify the oils by `area` for the
three areas of North Italy. Which are the most important
variables? Make plots of these variables. What is the accuracy of the
model for the training and test sets?
    c. Build a random forest to classify oils into the three areas of
North Italy. Compare the order of importance of variables with what
you found from a single tree. Make a parallel coordinate plot in the
order of variable importance.
-->