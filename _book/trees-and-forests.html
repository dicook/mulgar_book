<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Trees and forests | Interactive and dynamic graphics for multivariate data using R</title>
  <meta name="description" content="This is a book about exploring multivariate data." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Trees and forests | Interactive and dynamic graphics for multivariate data using R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a book about exploring multivariate data." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Trees and forests | Interactive and dynamic graphics for multivariate data using R" />
  
  <meta name="twitter:description" content="This is a book about exploring multivariate data." />
  

<meta name="author" content="Di Cook, Ursula Laa, Stuart Lee, Earo Wang" />


<meta name="date" content="2021-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-discriminant-analysis-and-manova.html"/>
<link rel="next" href="neural-networks-and-deep-learning.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#audience"><i class="fa fa-check"></i><b>0.1</b> Audience</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#how-to-use-the-book"><i class="fa fa-check"></i><b>0.2</b> How to use the book?</a><ul>
<li class="chapter" data-level="0.2.1" data-path="index.html"><a href="index.html#what-do-we-assume-about-you"><i class="fa fa-check"></i><b>0.2.1</b> What do we assume about you?</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#setting-up-your-workflow"><i class="fa fa-check"></i><b>0.3</b> Setting up your workflow</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#whats-different-about-space-beyond-2d"><i class="fa fa-check"></i><b>1.1</b> Whatâ€™s different about space beyond 2D</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#interactive-and-dynamic-graphics-literature"><i class="fa fa-check"></i><b>1.2</b> Interactive and dynamic graphics literature</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#an-opening-case-study"><i class="fa fa-check"></i><b>1.3</b> An opening case study</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#the-big-picture-of-the-book"><i class="fa fa-check"></i><b>1.4</b> The big picture of the book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="toolbox.html"><a href="toolbox.html"><i class="fa fa-check"></i><b>2</b> Toolbox</a></li>
<li class="chapter" data-level="3" data-path="dimension-reduction.html"><a href="dimension-reduction.html"><i class="fa fa-check"></i><b>3</b> Dimension reduction</a></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Data</a><ul>
<li class="chapter" data-level="4.0.1" data-path="data.html"><a href="data.html#other-possible-sources-for-data"><i class="fa fa-check"></i><b>4.0.1</b> Other possible sources for data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-methods.html"><a href="regression-methods.html"><i class="fa fa-check"></i><b>5</b> Regression methods</a><ul>
<li class="chapter" data-level="5.1" data-path="regression-methods.html"><a href="regression-methods.html#support-vector-machine"><i class="fa fa-check"></i><b>5.1</b> Support vector machine</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-discriminant-analysis-and-manova.html"><a href="linear-discriminant-analysis-and-manova.html"><i class="fa fa-check"></i><b>6</b> Linear discriminant analysis and MANOVA</a></li>
<li class="chapter" data-level="7" data-path="trees-and-forests.html"><a href="trees-and-forests.html"><i class="fa fa-check"></i><b>7</b> Trees and forests</a><ul>
<li class="chapter" data-level="7.1" data-path="trees-and-forests.html"><a href="trees-and-forests.html#trees"><i class="fa fa-check"></i><b>7.1</b> Trees</a></li>
<li class="chapter" data-level="7.2" data-path="trees-and-forests.html"><a href="trees-and-forests.html#random-forests"><i class="fa fa-check"></i><b>7.2</b> Random forests</a></li>
<li class="chapter" data-level="" data-path="trees-and-forests.html"><a href="trees-and-forests.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="neural-networks-and-deep-learning.html"><a href="neural-networks-and-deep-learning.html"><i class="fa fa-check"></i><b>8</b> Neural networks and deep learning</a></li>
<li class="chapter" data-level="9" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>9</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="10" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>10</b> k-means clustering</a></li>
<li class="chapter" data-level="11" data-path="model-based-clustering.html"><a href="model-based-clustering.html"><i class="fa fa-check"></i><b>11</b> model-based clustering</a></li>
<li class="chapter" data-level="12" data-path="multivariate-time-series.html"><a href="multivariate-time-series.html"><i class="fa fa-check"></i><b>12</b> Multivariate time series</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic graphics for multivariate data using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="trees-and-forests" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Trees and forests</h1>
<p>Topics to include:</p>
<ul>
<li>trees</li>
<li>forests</li>
<li>boosted trees</li>
<li>PP forest</li>
</ul>
<div id="trees" class="section level2">
<h2><span class="header-section-number">7.1</span> Trees</h2>
<p>The tree algorithm  is a widely used algorithmic method.
The tree algorithm generates a classification rule by sequentially
splitting the data into two buckets. Splits are made between sorted
data values of individual variables, with the goal of obtaining pure
classes on each side of the split. The inputs for a simple tree
classifier commonly include (1) an impurity measure, an indication of
the relative diversity among the cases in the terminal nodes; (2) a
parameter that sets the minimum number of cases in a node, or the
minimum number of observations in a terminal node of the tree; and (3)
a complexity measure that controls the growth of a tree, balancing the
use of a simple generalizable tree against a more accurate tree
tailored to the sample. When applying tree methods, exploring the
effects of the input parameters on the tree is instructive; for
example, it helps us to assess the stability of the tree model.</p>
<p>Although algorithmic models do not depend on distributional assumptions,
that does not mean that every algorithm is suitable for all data. For
example, the tree model works best when all variables are independent
within each class, because it does not take such dependencies into
account. As always, visualization can help us to determine whether a
particular model should be applied. In classification problems, it is
useful to explore the cluster structure, comparing the clusters with
the classes and looking for evidence of correlation within each class.
The upper left-hand plot in Fig.~ shows a strong
correlation between  and  within each cluster,
which indicates that the tree model may not give good results for the
. The plots in Fig.~
provide added evidence. They use background color to display the
class predictions for LDA and a tree. The LDA boundaries, which are
formed from a linear combination of  and , look
more appropriate than the rectangular boundaries of the tree
classifier.</p>
% Figure 2

<p> and  include thorough discussions of
algorithms for supervised classification presented from a modeling
perspective with a theoretical emphasis.  is an early
volume describing and illustrating both classical statistical methods
and algorithms for supervised classification. All three books contain
some excellent examples of the use of graphics to examine
two-dimensional (2D) boundaries generated by different
classifiers. The discussions in these and other writings on data
mining algorithms take a less exploratory approach than that of this
chapter, and they lack treatments of the use of graphics to examine
the high-dimensional spaces in which the classifiers operate.</p>

<p>
</p>
<p>A classifierâ€™s performance is usually assessed using its error or,
conversely, its accuracy. Error is calculated by comparing the
predicted class with the known true class, using a misclassification
table. For example, below are the respective misclassification tables
for LDA and the tree classifier applied to the :</p>

<p>The total error is the number of misclassified samples
divided by the total number of cases: <span class="math inline">\(4/74=0.054\)</span> for LDA and
<span class="math inline">\(5/74=0.068\)</span> for the tree classifier.</p>
<p>It is informative to study the misclassified cases and to see which
pockets of the data space contain more error. The misclassified cases
for LDA and tree classifiers are highlighted (large orange <span class="math inline">\(\times\)</span>es
and large green circles) in Fig.~. Some errors
made by the tree classifier, such as the uppermost large green circle,
seem especially egregious. As noted earlier, they result from the
limitations of the algorithm when variables are correlated.</p>
<p></p>
<p>To be useful, the error estimate should predict the performance of the
classifier on new samples not yet seen. However, if the error is
calculated using the same data that was used by the classifier, it is
likely to be too low. Many methods are used to avoid
double-dipping from the data, including several types of
. A simple example of cross-validation is to
split the data into a training sample (used by the classifier) and a
test sample (used for calculating error).</p>
<p>
</p>
<p>Ensemble methods build cross-validation into the error
calculations. Ensembles are constructed by using multiple classifiers
and by pooling the predictions using a voting scheme. A random forest
, for example, builds in cross-validation by
constructing multiple trees, each of which is generated by randomly
sampling the input variables and the cases. Because each tree is
built using a sample of the cases, there is in effect a training
sample and a test sample for each tree. (See
Sect.~ for more detail.)</p>
<p></p>
</div>
<div id="random-forests" class="section level2">
<h2><span class="header-section-number">7.2</span> Random forests</h2>
<p>A random forest  is a classifier that is built from
multiple trees generated by randomly sampling the cases and the
variables. The random sampling (with replacement) of cases has the
fortunate effect of creating a training (<code>in-bag'') and a test (</code>out-of-bagâ€™â€™) sample for each tree computed. The class of each
case in the out-of-bag sample for each tree is predicted, and the
predictions for all trees are combined into a vote for the class
identity.</p>
<p>A random forest is a computationally intensive method, a ``black boxâ€™â€™
classifier, but it produces various diagnostics that make the outcome
less mysterious. Some diagnostics that help us to assess the
model are the votes, the measures of variable importance, the error
estimate, and as usual, the misclassification tables.</p>
<p></p>
<p>We test the method on the  by building a random
forest classifier of 500 trees, using the R package 
:</p>

% Insert page break to avoid breaking the R output.

% Figure 11

<p>Each tree used a random sample of four of the eight
variables, as well as a random sample of about a third of the 572
cases. The votes are displayed in the left-hand plot of
Fig.~, next to a projection from a
 2D tour. Since there are three classes, the votes
form a triangle, with one vertex for each region, with oils from the
South at the far right, Sardinian oils at the top, and Northern oils
at the lower left. Samples that are consistently classified correctly
are close to the vertices; cases that are commonly misclassified are
further from a vertex. Although forests perfectly classify this data,
the number of points falling between the Northern and the Sardinian
vertices suggests some potential for error in classifying future
samples.</p>
<p>For more understanding of the votes, we turn to another diagnostic:
variable importance. Forests return two measures of variable
importance, both of which give similar results. Based on the Gini
measure, the most important variables, in order, are ,
, , , ,
, , and .</p>
<p>Some of this ordering is as expected, given the initial graphical
inspection of the data (Sect.~). The importance of
 was our first discovery, as shown in the top row of
Fig.~. And yes,  is next in importance:
The first two plots in Fig.~ make that clear. The
surprise is that the forest should consider  to be less
important than . This is not what we found, as shown in
the right-hand plot in that figure.</p>
<p>Did we overlook something important in our earlier investigation? We
return to the use of the manual manipulation of the tour
 to see whether  does in fact perform
better than  at finding a gap between the two regions.
But it does not. By overlooking the importance of ,
the random forest never finds an adequate gap between the oils of the
Northern and the Sardinian regions, and that probably explains why there
is more confusion about some Northern samples than there should be.</p>
<p>We rebuild the forest using a new variable constructed from a linear
combination of  and  (),
just as we did when applying the single tree classifier. Since
correlated variables reduce each otherâ€™s importance, we need to remove
 and  when we add . Once we
have done this, the confusion between Northern and Sardinian oils
disappears (Fig.~, lower plot): The points are now
tightly clumped at each vertex, which indicates more certainty in
their class predictions. The new variable becomes the second most
important variable according to the importance diagnostic.</p>
<p>% Need R code here and output - variable importance</p>
<p>Classifying the oils by the three large s is too easy a
problem for forests; they are designed to tackle more challenging
classification tasks. We will use them to examine the oils from the
areas in the Southern region (North and South Apulia, Calabria, and
Sicily). Remember the initial graphical inspection of the data, which
showed that oils from the four areas were not completely
separable. The samples from Sicily overlapped those of the three other
areas. We will use a forest classifier to see how well it can
differentiate the Southern oils by :</p>
<div style="page-break-after: always;"></div>
% Insert newpage to pull the first two lines to the next page

<p>After experimenting with several input parameters, we show
the results for a forest of 1,500 trees, sampling two variables at each
tree node, and yielding an error rate of 0.068. The misclassification
table is:</p>

<p>The error of the forest is surprisingly low, but the
error is definitely not uniform across classes. Predictions for Sicily
are wrong about a third of the time. Figure~ shows
some more interesting aspects of the results. For this figure, the
following table describes the correspondence between area and symbol:</p>

<p></p>
<p>Look first at the top row of the figure. The
misclassification table is represented by a jittered scatterplot, at
the left. A plot from a 2D tour  of the four
voting variables is in the center. Because there are four groups, the
votes lie on a 3D tetrahedron (a simplex). The votes from three of
the areas are pretty well separated, one at each ``corner,â€™â€™ but those
from Sicily overlap all of them. Remember that when points are
clumped at the vertex, class members are consistently predicted
correctly. Since this does not occur for Sicilian oils, we see that
there is more uncertainty in the predictions for this area.</p>
<p>The plot at right confirms this observation. It is a projection from a
2D tour  of the four most important variables,
showing a pattern we have seen before. We can achieve pretty good
separation of the oils from North Apulia, Calabria, and South Apulia,
but the oils from Sicily overlap all three clusters. Clearly these
are tough samples to classify correctly.</p>
% Figure 12

<p>We remove the Sicilian oils from the plots so we can focus on the
other three areas (bottom row of plots). The points representing North
Apulian oils form a very tight cluster at a vertex, with three
exceptions. Two of these points are misclassified as Calabrian, and we
have highlighted them as large filled circles by painting the
misclassification plot.</p>
<p>The pattern of the votes (middle plot) suggests that there is high
certainty in the predictions for North Apulian oils, with the
exception of these two samples. When we watch the votes in the tour
 for a while, we see that the votes of these two
samples travel as if they were in a cluster all their own, which is
distinct from the remaining North Apulian oils.</p>
<p>However, when we look at the data, we find the votes for these two
samples a bit puzzling. We watch the four most important variables in
the tour for a while (as in the right plot), and these two points
do not behave as if they were in a distinct cluster; they travel with
the rest of the samples from North Apulia. They do seem to be
outliers with respect their class, but they are not so far from their
group <sub>â€”</sub> it is a bit surprising that the forest has trouble
classifying these cases.</p>
<p>Rather than exploring the other misclassifications, we leave that for
the reader.</p>
<p>In summary, a random forest is a useful method for tackling tough
classification problems. Its diagnostics provide a rich basis for
graphical exploration, which helps us to digest and evaluate the solution.</p>
</div>
<div id="exercises" class="section level2 unnumbered">
<h2>Exercises</h2>
<ol style="list-style-type: decimal">
<li>For the :
<ol style="list-style-type: lower-alpha">
<li>Split the samples from North Italy into <span class="math inline">\(2/3\)</span> training and
<span class="math inline">\(1/3\)</span> test samples for each area.</li>
<li>Build a tree model to classify the oils by  for the
three areas of North Italy. Which are the most important
variables? Make plots of these variables. What is the accuracy of the
model for the training and test sets?</li>
<li>Build a random forest to classify oils into the three areas of
North Italy. Compare the order of importance of variables with what
you found from a single tree. Make a parallel coordinate plot in the
order of variable importance.</li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-discriminant-analysis-and-manova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="neural-networks-and-deep-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["mulgar.pdf", "mulgar.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
