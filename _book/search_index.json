[
["index.html", "Interactive and dynamic graphics for multivariate data using R Preface", " Interactive and dynamic graphics for multivariate data using R Di Cook, Ursula Laa, Stuart Lee, Earo Wang 2021-02-08 Preface Why write this book What is expected of the reader What packages to install install.packages(&quot;mulgar&quot;) # or the development version # devtools::install_github(&quot;dicook/mulgar&quot;) "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction Some background into multivariate data vis What’s different about space beyond 2D Interactive and dynamic graphics literature Outline of the different chapters and how they all fit together - some diagrams "],
["toolbox.html", "Chapter 2 Toolbox", " Chapter 2 Toolbox Description and explanation of primary methods used throughout the book. Mostly focusing on tour methods. We need a really nice friendly introduction to tour methods "],
["dimension-reduction.html", "Chapter 3 Dimension reduction", " Chapter 3 Dimension reduction This chapter will focus on methods for reducing dimension, and how the tour, with linked plots, can be used to assist: PCA, MDS, tSNE, factor analysis "],
["data.html", "Chapter 4 Data", " Chapter 4 Data This chapter describes the datasets used throughout the book, one possibility is that this could be moved to an appendix and in the online version of the book any mention of the dataset in the main text could be cross-linked to its description. One thing that I really liked in the first edition is the emphasis on analysis question of interest and the little notes about the data. I believe the original layout was to put the data in order of appearance of the book, which naturally clusters them by technique. There were fifteen datasets listed in chapter 7 of the first edition. Several of these were related to networks which we are not including this time. I have tried to give a mix of things on a variety of topics. It would be nice to see if there’s updated versions of the “tips” and Di’s music data. I think it would be useful to have more unstructured data sets like tweets or books. Name Description Source Proposed Chapter Other Notes pdfsense Physics data that is featured heavily in Ursula’s work https://metapdf.hepforge.org/PDFSense/ clustering, classification I consider this a successor to prim7 pedestrian Melbourne pedestrian counts rwalkr / https://data.melbourne.vic.gov.au/Transport/Pedestrian-Counting-System-Sensor-Locations/h57g-5234 multivariate time series, regression (?) This is still one of my faves and has a lot going on. It could be turned into a multivariate time series with the addition of weather information… mouseretina Single cell RNA-seq data from Macosko et al. Available in scRNAseq package on Bioconductor dimension, clustering Used in the liminal paper, one concern is that there a lot of nuances to processing the data that might need to be explained. An updated version of the arabidopsis expression data from 1e. pisa OECD programme for international student assessment data Di has data regression would need to check usage agreements sketches Google’s Quickdraw data https://quickdraw.withgoogle.com/ neural networks, classification mosquitos An audio “civic” science dataset containing wingbeat frequencies for classifying mosquito species https://elifesciences.org/articles/27854; https://datadryad.org/stash/dataset/doi:10.5061/dryad.98d7s neural networks, classification (forests / LDA) aklhouseprices Auckland house prices collected by Earo Earo has data regression, toolbox would need to check usage agreements books Text data ala Gutenberg examples we use in 1010 / could also be another text dataset dimension, clustering wages An updated version of the wages dataset Di is working on this I think regression, toolbox, multivariate time series bushfires The data Di used in her conversation article https://theconversation.com/open-data-shows-lightning-not-arson-was-the-likely-cause-of-most-victorian-bushfires-last-summer-151912 toolbox, forests "],
["regression-methods.html", "Chapter 5 Regression methods 5.1 Support vector machine", " Chapter 5 Regression methods Topics to include: - Many regression models - Logistic regression 5.1 Support vector machine % http://www.support-vector-machines.org/SVM_osh.html % wikipedia A support vector machine (SVM) is a binary classification method. An SVM looks for gaps between clusters in the data, based on the extreme observations in each class. In this sense it mirrors the graphical approach described at the start of this chapter, in which we searched for gaps between groups. We describe this method more fully than we did the other algorithms for two reasons: first, because of its apparent similarity to the graphical approach, and second, because it is difficult to find a simple explanation of the method in the literature. The algorithm takes an \\(n \\times p\\) data matrix, where each column is scaled to [$-$1,1] and each row is labeled as one of two classes (\\(y_i=+1\\) or \\(-1\\)), and finds a hyperplane that separates the two groups, if they are separable. Each row of the data matrix is a vector in \\(p\\)-dimensional space, denoted as % Should this be represented as a row instead of a column? dfs \\[ \\blX=\\left[ \\begin{array}{c} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p \\end{array} \\right] \\] and the separating hyperplane can be written as \\[ \\blW&#39;\\blX + b = 0 \\] where \\(\\blW = [ w_1~~ w_2 ~~ \\dots ~~ w_p]&#39;\\) is the normal vector to the separating hyperplane and \\(b\\) is a constant. The best separating hyperplane is found by maximizing the margin of separation between the two classes as defined by two parallel hyperplanes: \\[ \\blW&#39;\\blX + b = 1, ~~~~~ \\blW&#39;\\blX + b = -1. \\] These hyperplanes should maximize the distance from the separating hyperplane and have no points between them, capitalizing on any gap between the two classes. The distance from the origin to the separating hyperplane is \\(|b|/||\\blW||\\), so the distance between the two parallel margin hyperplanes is \\(2/||\\blW||=2/\\sqrt{w_1^2+\\dots +w_p^2}\\). Maximizing this is the same as minimizing \\(||\\blW||/2\\). To ensure that the two classes are separated, and that no points lie between the margin hyperplanes we need: \\[ \\blW&#39;\\blX_i + b \\geq 1, ~~~\\mbox{ or } ~~~\\blW&#39;\\blX_i + b \\leq -1 ~~~\\forall i=1, ..., n \\] which corresponds to \\[\\begin{eqnarray} y_i(\\blW&#39;\\blX_i+b)\\geq 1 ~~~\\forall i=1, ..., n \\label{svm-crit} \\end{eqnarray}\\] Thus the problem corresponds to Interestingly, only the points closest to the margin hyperplanes are needed to define the separating hyperplane. We might think of these points as lying on or close to the convex hull of each cluster in the area where the clusters are nearest to each other. These points are called support vectors, and the coefficients of the separating hyperplane are computed from a linear combination of the support vectors \\(\\blW = \\sum_{i=1}^{s} y_i\\alpha_i\\blX_i\\), where \\(s\\) is the number of support vectors. We could also use \\(\\blW = \\sum_{i=1}^n y_i\\alpha_i\\blX_i\\), where \\(\\alpha_i=0\\) if \\(\\blX_i\\) is not a support vector. For a good fit the number of support vectors \\(s\\) should be small relative to \\(n\\). Fitting algorithms can achieve gains in efficiency by using only samples of the cases to find suitable support vector candidates; this approach is used in the SVMLight software. In practice, the assumption that the classes are completely separable is unrealistic. Classification problems rarely present a gap between the classes, such that there are no misclassifications. relaxed the separability condition to allow some misclassified training points by adding a tolerance value \\(\\epsilon_i\\) to Equation , which results in the modified criterion \\(y_i(\\blW&#39;\\blX_i+b)&gt;1-\\epsilon_i, \\epsilon_i\\geq 0\\). Points that meet this criterion but not the stricter one are called slack vectors. Nonlinear classifiers can be obtained by using nonlinear transformations of \\(\\blX_i\\), \\(\\phi(\\blX_i)\\) , which is implicitly computed during the optimization using a kernel function \\(K\\). Common choices of kernels are linear \\(K(\\blx_i,\\blx_j)=\\blx_i&#39;\\blx_j\\), polynomial \\(K(\\blx_i,\\blx_j)=(\\gamma\\blx_i&#39;\\blx_j+r)^d\\), radial basis \\(K(\\blx_i,\\blx_j)=\\exp(-\\gamma||\\blx_i-\\blx_j||^2)\\), or sigmoid functions \\(K(\\blx_i,\\blx_j)=\\mbox{tanh}(\\gamma\\blx_i&#39;\\blx_j+r)\\), where \\(\\gamma&gt;0, r,\\) and \\(d\\) are kernel parameters. % She didn’t say to delete the terminating colon here, but by % analogy with these rest, I will. dfs The ensuing minimization problem is formulated as \\[ \\mbox{\\em minimizing } \\frac{1}{2}||\\blW|| + C\\sum_{i=1}^n \\epsilon_i ~~ \\mbox{\\em subject to } y_i(\\blW&#39;\\phi(\\blX)+b)&gt;1-\\epsilon_i \\] where \\(\\epsilon_i\\geq 0\\), \\(C&gt;0\\) is a penalty parameter guarding against over-fitting the training data and \\(\\epsilon\\) controls the tolerance for misclassification. The normal to the separating hyperplane \\(\\blW\\) can be written as \\(\\sum_{i=1}^{n} y_i\\alpha_i{\\phi(\\blX_i)}\\), where points other than the support and slack vectors will have \\(\\alpha_i=0\\). Thus the optimization problem becomes \\[\\begin{eqnarray*} \\mbox{\\em minimizing } \\frac{1}{2} \\sum_{i=1}^n\\sum_{j=1}^n y_iy_j\\alpha_i\\alpha_jK(\\blX_i,\\blX_j)+C\\sum_{i=1}^n \\epsilon_i \\\\ ~~~~~~~~~~~\\mbox{\\em subject to } y_i(\\blW&#39;\\phi(\\blX)+b)&gt;1-\\epsilon_i \\end{eqnarray*}\\] We use the {svm} function in the package of R, which uses , to classify the oils of the four areas in the Southern region. SVM is a binary classifier, but this algorithm overcomes that limitation by comparing classes in pairs, fitting six separate classifiers, and then using a voting scheme to make predictions. To fit the SVM we also need to specify a kernel, or rely on the internal tuning tools of the algorithm to choose this for us. Automatic tuning in the algorithm chooses a radial basis, but we found that a linear kernel performed better, so that is what we used. (This accords with our earlier visual inspection of the data in Sect.~.) Here is the R code used to fit the model: These are our misclassification tables: The training error is \\(9/246=0.037\\), and the test error is \\(6/77=0.078\\). (The training error is the same as that of the neural network classifier, but the test error is lower.) Most error is associated with Sicily, which we have seen repeatedly to be an especially difficult class to separate. In the training data there are no other errors, and in the test data there are just two samples from Calabria mistakenly classified. Figure~ illustrates our examination of the misclassified cases, one in each row of the figure. (Points corresponding to Sicily were removed from all four plots.) Each of the two cases is brushed (using a filled red circle) in the plot of misclassification table and viewed in a linked 2D tour. Both of these cases are on the edge of their clusters so the confusion of classes is reasonable. % Figure 14 % Figure 15 The linear SVM classifier uses 20 support vectors and 29 slack vectors to define the separating planes between the four areas. It is interesting to examine which points are selected as support vectors, and where they are located in the data space. For each pair of classes, we expect to find some projection in which the support vectors line up on either side of the margin of separation, whereas the slack vectors lie closer to the boundary, perhaps mixed in with the points of other classes. The plots in Fig.~ represent our use of the 2D tour, augmented by manual manipulation,~to look for these projections. (The Sicilian points are again removed.) The support vectors are represented by open circles and the slack vectors by open rectangles, and we have been able to find a number of projections in which the support vectors are on the opposing outer edge of the point clouds for each cluster. The linear SVM does a very nice job with this difficult classification. The accuracy is almost perfect on three classes, and the misclassifications are quite reasonable mistakes, being points that are on the extreme edges of their clusters. However, this method joins the list of those defeated by the difficult problem of distinguishing the Sicilian oils from the rest. For some classification problems, it is possible to get a good picture of the boundary between two classes. With LDA and SVM classifiers the boundary is described by the equation of a hyperplane. For others the boundary can be determined by evaluating the classifier on points sampled in the data space, using either a regular grid or some more efficient sampling scheme. % Figure 16 We use the R package to generate points illustrating boundaries, add those points to the original data, and display them in GGobi. Figure~ shows projections of boundaries between pairs of classes in the . In each example, we used the 2D tour with manual control~to focus the view on a projection that revealed the boundary between two groups. % Needs to be checked The top two plots show tour projections of the North (purple) and Sardinia (green) oils where the two classes are separated and the boundary appears in gray. The LDA boundary (shown at left) slices too close to the Northern oils. This might be due to the violation of the LDA assumption that the two groups have equal variance; since that is not true here, it places the boundary too close to the group with the larger variance. The SVM boundary (at right) is a bit closer to the Sardinian oils than the LDA boundary is, yet it is still a tad too close to the oils from the North. The bottom row of plots examines the more difficult classification of the areas of the South, focusing on separating the South Apulian oils (in pink), which is the largest sample, from the oils of the other areas (all in orange). Perfect separation between the classes does not occur. Both plots are tour projections showing SVM boundaries, the left plot generated by a linear kernel and the right one by a radial kernel. Recall that the radial kernel was selected automatically by the SVM software we used, whereas we actually chose to use a linear kernel. These pictures illustrate that the linear basis yields a more reasonable boundary between the two groups. The shape of the clusters of the two groups is approximately the same, and there is only a small overlap of the two. The linear boundary fits this structure neatly. The radial kernel wraps around the South Apulian oils. "],
["linear-discriminant-analysis-and-manova.html", "Chapter 6 Linear discriminant analysis and MANOVA", " Chapter 6 Linear discriminant analysis and MANOVA Discriminant analysis dates to the early 1900s. Fisher’s linear discriminant determines a linear combination of the variables that separates two classes by comparing the differences between class means with the variance of values within each class. It makes no assumptions about the distribution of the data. Linear discriminant analysis (LDA), as proposed by , formalizes Fisher’s approach by imposing the assumption that the data values for each class arise from a \\(p\\)-dimensional multivariate normal distribution, which shares a common variance–covariance matrix with data from other classes. Under this assumption, Fisher’s linear discriminant gives the optimal separation between the two groups. For two equally weighted groups, where \\(Y\\) is coded as \\(\\{0, 1\\}\\), the LDA rule is: where ${}_k $ are the class mean vectors of an \\(n\\times p\\) data matrix \\(\\blX_k ~~(k=1,2)\\), \\[ \\blS_{\\rm pooled} = \\frac{(n_1-1) \\blS_1}{(n_1-1)+(n_2-1)} + \\frac{(n_2-1) \\blS_2}{(n_1-1)+(n_2-1)} \\] is the pooled variance–covariance matrix, and \\[ \\blS_k = \\frac{1}{n-1}\\sum_{i=1}^{n} (\\blX_{ki}-\\bar{\\blX}_k)(\\blX_{ki}-\\bar{\\blX}_k)&#39;, ~~k=1,2 \\] is the class variance–covariance matrix. The linear discriminant part of this rule is \\((\\bar{\\blX}_1-\\bar{\\blX}_2)&#39;\\blS^{-1}_{\\rm pooled}\\), which defines the linear combination of variables that best separates the two groups. To define a classification rule, we compute the value of the new observation \\(\\blX_0\\) on this line and compare it with the value of the average of the two class means \\((\\bar{\\blX}_1+\\bar{\\blX}_2)/2\\) on the same line. %Computing the value of the new observation \\(\\blX_0\\) on this %line and comparing it with the value of the average of the two class %means \\((\\bar{\\blX}_1+\\bar{\\blX}_2)/2\\) on this line gives the %classification rule. For multiple \\((g)\\) classes, the rule and the discriminant space are constructed using the between-group sum-of-squares matrix, \\[ \\blB = \\sum_{k=1}^g n_k(\\bar{\\blX}_k-\\bar{\\blX})(\\bar{\\blX}_k-\\bar{\\blX})&#39; \\] which measures the differences between the class means, compared with the overall data mean \\(\\bar{\\blX}\\) and the within-group sum-of-squares matrix, \\[ \\blW = \\sum_{k=1}^g\\sum_{i=1}^{n_k} (\\blX_{ki}-\\bar{\\blX}_k)(\\blX_{ki}-\\bar{\\blX}_k)&#39; \\] which measures the variation of values around each class mean. The linear discriminant space is generated by computing the eigenvectors (canonical coordinates) of \\(\\blW^{-1}\\blB\\), and this is the space where the group means are most separated with respect to the pooled variance–covariance. The resulting classification rule is to allocate a new observation to the class with the highest value of \\[\\begin{eqnarray} \\bar{\\blX}_k&#39;\\blS^{-1}_{\\rm pooled}\\blX_0 - \\frac{1}{2}\\bar{\\blX}_k&#39;\\blS^{-1}_{\\rm pooled}\\bar{\\blX}_k ~~~k=1,...,g \\label{lda-rule} \\end{eqnarray}\\] which results in allocating the new observation into the class with the closest mean. This LDA approach is widely applicable, but it is useful to check the underlying assumptions on which it depends: (1) that the cluster structure corresponding to each class forms an ellipse, showing that the class is consistent with a sample from a multivariate normal distribution, and (2) that the variance of values around each mean is nearly the same. Figure~ illustrates two datasets, of which only one is consistent with these assumptions. Other parametric models, such as quadratic discriminant analysis or logistic regression, also depend on assumptions about the data which should be validated. % Figure 1 Our description is derived from (???) and (???). A good general treatment of parametric methods for supervised classification can be found in (???) or another similar multivariate analysis textbook. Missing from multivariate textbooks is a good explanation of the use of interactive graphics both to check the assumptions underlying the methods and to explore the results. This chapter fills this gap. Algorithmic methods have overtaken parametric methods in the practice of supervised classification. A parametric method such as linear discriminant analysis yields a set of interpretable output parameters, so it leaves a clear trail helping us to understand what was done to produce the results. An algorithmic method, on the other hand, is more or less a black box, with various input parameters that are adjusted to tune the algorithm. The algorithm’s input and output parameters do not always correspond in any obvious way to the interpretation of the results. All the same, these methods can be very powerful and their use is not limited by requirements about variable distributions as is the case with parametric methods. "],
["trees-and-forests.html", "Chapter 7 Trees and forests 7.1 Trees 7.2 Random forests Exercises", " Chapter 7 Trees and forests Topics to include: trees forests boosted trees PP forest 7.1 Trees The tree algorithm is a widely used algorithmic method. The tree algorithm generates a classification rule by sequentially splitting the data into two buckets. Splits are made between sorted data values of individual variables, with the goal of obtaining pure classes on each side of the split. The inputs for a simple tree classifier commonly include (1) an impurity measure, an indication of the relative diversity among the cases in the terminal nodes; (2) a parameter that sets the minimum number of cases in a node, or the minimum number of observations in a terminal node of the tree; and (3) a complexity measure that controls the growth of a tree, balancing the use of a simple generalizable tree against a more accurate tree tailored to the sample. When applying tree methods, exploring the effects of the input parameters on the tree is instructive; for example, it helps us to assess the stability of the tree model. Although algorithmic models do not depend on distributional assumptions, that does not mean that every algorithm is suitable for all data. For example, the tree model works best when all variables are independent within each class, because it does not take such dependencies into account. As always, visualization can help us to determine whether a particular model should be applied. In classification problems, it is useful to explore the cluster structure, comparing the clusters with the classes and looking for evidence of correlation within each class. The upper left-hand plot in Fig.~ shows a strong correlation between and within each cluster, which indicates that the tree model may not give good results for the . The plots in Fig.~ provide added evidence. They use background color to display the class predictions for LDA and a tree. The LDA boundaries, which are formed from a linear combination of and , look more appropriate than the rectangular boundaries of the tree classifier. % Figure 2 and include thorough discussions of algorithms for supervised classification presented from a modeling perspective with a theoretical emphasis. is an early volume describing and illustrating both classical statistical methods and algorithms for supervised classification. All three books contain some excellent examples of the use of graphics to examine two-dimensional (2D) boundaries generated by different classifiers. The discussions in these and other writings on data mining algorithms take a less exploratory approach than that of this chapter, and they lack treatments of the use of graphics to examine the high-dimensional spaces in which the classifiers operate. A classifier’s performance is usually assessed using its error or, conversely, its accuracy. Error is calculated by comparing the predicted class with the known true class, using a misclassification table. For example, below are the respective misclassification tables for LDA and the tree classifier applied to the : The total error is the number of misclassified samples divided by the total number of cases: \\(4/74=0.054\\) for LDA and \\(5/74=0.068\\) for the tree classifier. It is informative to study the misclassified cases and to see which pockets of the data space contain more error. The misclassified cases for LDA and tree classifiers are highlighted (large orange \\(\\times\\)es and large green circles) in Fig.~. Some errors made by the tree classifier, such as the uppermost large green circle, seem especially egregious. As noted earlier, they result from the limitations of the algorithm when variables are correlated. To be useful, the error estimate should predict the performance of the classifier on new samples not yet seen. However, if the error is calculated using the same data that was used by the classifier, it is likely to be too low. Many methods are used to avoid double-dipping from the data, including several types of . A simple example of cross-validation is to split the data into a training sample (used by the classifier) and a test sample (used for calculating error). Ensemble methods build cross-validation into the error calculations. Ensembles are constructed by using multiple classifiers and by pooling the predictions using a voting scheme. A random forest , for example, builds in cross-validation by constructing multiple trees, each of which is generated by randomly sampling the input variables and the cases. Because each tree is built using a sample of the cases, there is in effect a training sample and a test sample for each tree. (See Sect.~ for more detail.) 7.2 Random forests A random forest is a classifier that is built from multiple trees generated by randomly sampling the cases and the variables. The random sampling (with replacement) of cases has the fortunate effect of creating a training (in-bag'') and a test (out-of-bag’’) sample for each tree computed. The class of each case in the out-of-bag sample for each tree is predicted, and the predictions for all trees are combined into a vote for the class identity. A random forest is a computationally intensive method, a ``black box’’ classifier, but it produces various diagnostics that make the outcome less mysterious. Some diagnostics that help us to assess the model are the votes, the measures of variable importance, the error estimate, and as usual, the misclassification tables. We test the method on the by building a random forest classifier of 500 trees, using the R package : % Insert page break to avoid breaking the R output. % Figure 11 Each tree used a random sample of four of the eight variables, as well as a random sample of about a third of the 572 cases. The votes are displayed in the left-hand plot of Fig.~, next to a projection from a 2D tour. Since there are three classes, the votes form a triangle, with one vertex for each region, with oils from the South at the far right, Sardinian oils at the top, and Northern oils at the lower left. Samples that are consistently classified correctly are close to the vertices; cases that are commonly misclassified are further from a vertex. Although forests perfectly classify this data, the number of points falling between the Northern and the Sardinian vertices suggests some potential for error in classifying future samples. For more understanding of the votes, we turn to another diagnostic: variable importance. Forests return two measures of variable importance, both of which give similar results. Based on the Gini measure, the most important variables, in order, are , , , , , , , and . Some of this ordering is as expected, given the initial graphical inspection of the data (Sect.~). The importance of was our first discovery, as shown in the top row of Fig.~. And yes, is next in importance: The first two plots in Fig.~ make that clear. The surprise is that the forest should consider to be less important than . This is not what we found, as shown in the right-hand plot in that figure. Did we overlook something important in our earlier investigation? We return to the use of the manual manipulation of the tour to see whether does in fact perform better than at finding a gap between the two regions. But it does not. By overlooking the importance of , the random forest never finds an adequate gap between the oils of the Northern and the Sardinian regions, and that probably explains why there is more confusion about some Northern samples than there should be. We rebuild the forest using a new variable constructed from a linear combination of and (), just as we did when applying the single tree classifier. Since correlated variables reduce each other’s importance, we need to remove and when we add . Once we have done this, the confusion between Northern and Sardinian oils disappears (Fig.~, lower plot): The points are now tightly clumped at each vertex, which indicates more certainty in their class predictions. The new variable becomes the second most important variable according to the importance diagnostic. % Need R code here and output - variable importance Classifying the oils by the three large s is too easy a problem for forests; they are designed to tackle more challenging classification tasks. We will use them to examine the oils from the areas in the Southern region (North and South Apulia, Calabria, and Sicily). Remember the initial graphical inspection of the data, which showed that oils from the four areas were not completely separable. The samples from Sicily overlapped those of the three other areas. We will use a forest classifier to see how well it can differentiate the Southern oils by : % Insert newpage to pull the first two lines to the next page After experimenting with several input parameters, we show the results for a forest of 1,500 trees, sampling two variables at each tree node, and yielding an error rate of 0.068. The misclassification table is: The error of the forest is surprisingly low, but the error is definitely not uniform across classes. Predictions for Sicily are wrong about a third of the time. Figure~ shows some more interesting aspects of the results. For this figure, the following table describes the correspondence between area and symbol: Look first at the top row of the figure. The misclassification table is represented by a jittered scatterplot, at the left. A plot from a 2D tour of the four voting variables is in the center. Because there are four groups, the votes lie on a 3D tetrahedron (a simplex). The votes from three of the areas are pretty well separated, one at each ``corner,’’ but those from Sicily overlap all of them. Remember that when points are clumped at the vertex, class members are consistently predicted correctly. Since this does not occur for Sicilian oils, we see that there is more uncertainty in the predictions for this area. The plot at right confirms this observation. It is a projection from a 2D tour of the four most important variables, showing a pattern we have seen before. We can achieve pretty good separation of the oils from North Apulia, Calabria, and South Apulia, but the oils from Sicily overlap all three clusters. Clearly these are tough samples to classify correctly. % Figure 12 We remove the Sicilian oils from the plots so we can focus on the other three areas (bottom row of plots). The points representing North Apulian oils form a very tight cluster at a vertex, with three exceptions. Two of these points are misclassified as Calabrian, and we have highlighted them as large filled circles by painting the misclassification plot. The pattern of the votes (middle plot) suggests that there is high certainty in the predictions for North Apulian oils, with the exception of these two samples. When we watch the votes in the tour for a while, we see that the votes of these two samples travel as if they were in a cluster all their own, which is distinct from the remaining North Apulian oils. However, when we look at the data, we find the votes for these two samples a bit puzzling. We watch the four most important variables in the tour for a while (as in the right plot), and these two points do not behave as if they were in a distinct cluster; they travel with the rest of the samples from North Apulia. They do seem to be outliers with respect their class, but they are not so far from their group — it is a bit surprising that the forest has trouble classifying these cases. Rather than exploring the other misclassifications, we leave that for the reader. In summary, a random forest is a useful method for tackling tough classification problems. Its diagnostics provide a rich basis for graphical exploration, which helps us to digest and evaluate the solution. Exercises For the : Split the samples from North Italy into \\(2/3\\) training and \\(1/3\\) test samples for each area. Build a tree model to classify the oils by for the three areas of North Italy. Which are the most important variables? Make plots of these variables. What is the accuracy of the model for the training and test sets? Build a random forest to classify oils into the three areas of North Italy. Compare the order of importance of variables with what you found from a single tree. Make a parallel coordinate plot in the order of variable importance. "],
["neural-networks-and-deep-learning.html", "Chapter 8 Neural networks and deep learning", " Chapter 8 Neural networks and deep learning Neural networks for classification can be thought of as additive models where explanatory variables are transformed, usually through a logistic function, added to other explanatory variables, transformed again, and added again to yield class predictions. Aside from the data mining literature, mentioned earlier, a good comprehensive and accessible description for statisticians can be found in . The model can be formulated as: \\[ \\hat{y} = f(x) = \\phi(\\alpha+\\sum_{h=1}^{s} w_{h}\\phi(\\alpha_h+\\sum_{i=1}^{p} w_{ih}x_i)) \\] where \\(x\\) is the vector of explanatory variable values, \\(y\\) is the target value, \\(p\\) is the number of variables, \\(s\\) is the number of nodes in the single hidden layer, and \\(\\phi\\) is a fixed function, usually a linear or logistic function. This model has a single hidden layer and univariate output values. The model is fit by minimizing the sum of squared differences between observed values and fitted values, and the minimization does not always converge. A neural network is a black box that accepts inputs, computes, and spits out predictions. With graphics, some insight into the black box can be gained. We use the feed-forward neural network provided in the { nnet} package of R to illustrate. We continue to work with , and we look at the performance of the neural network in classifying the oils in the four areas of the South, a difficult challenge. Because the software does not include a method for computing the predictive error, we break the data into training and test samples so we can better estimate the predictive error. (We could tweak the neural network to perfectly fit all the data, but then we could not estimate how well it would perform with new data.) % this may need a few more words After trying several values for \\(s\\), the number of nodes in the hidden layer, we chose \\(s=4\\); we also chose a linear \\(\\phi\\), \\(decay=0.005\\), and \\(range=0.06\\). We fit the model using many different random starting values, rejecting the results until it eventually converged to a solution with a reasonably low error: % Insert page break to avoid breaking the R output. Below are the misclassification tables for the training and test samples. The training error is \\(9/246=0.037\\), and the test error is \\(12/77=0.156\\). The overall errors, as in the random forest model, are not uniform across classes. This is particularly obvious in the test error table: The error in classifying North Apulian oils is close to a third, and it is even worse for Sicilian oils, which have an almost even chance of being misclassified. Our exploration of the misclassifications is shown in Fig.~. (The troublesome Sicilian oils have been excluded from all plots in this figure.) Consider first the plots in the top row. The left-hand plot shows the misclassification table. Two samples of oils from North Apulia (orange \\(+\\)) have been incorrectly classified as South Apulian (pink \\(\\times\\)), and these two points have been brushed as filled orange circles. Note where these points fall in the next two plots, which are linked 2D tour projections. One of the two misclassified points is on the edge of the cluster of North Apulian points, close to the Calabrian cluster. It is understandable that there might be some confusion about this case. The other sample is on the outer edge of the North Apulian cluster, but it is far from the Calabrian cluster — this should not have been confused. % Figure 13 In the bottom row of plots, we follow the same procedure to examine the single North Apulian sample misclassified as South Apulian. It is painted as a filled orange circle in the misclassification plot and viewed in a tour. This point is on the outer edge of the North Apulian cluster, but it is closer to the Calabrian cluster than the South Apulian cluster. It would be understandable for it to be misclassified as Calabrian, so it is puzzling that it is misclassified as South Apulian. In summary, a neural network is a black box method for tackling tough classification problems. It will generate different solutions each time the net is fit, some much better than others. When numerical measures suggest that a reasonable model has been found, graphics can be used to inspect the model in more detail. "],
["hierarchical-clustering.html", "Chapter 9 Hierarchical clustering", " Chapter 9 Hierarchical clustering "],
["k-means-clustering.html", "Chapter 10 k-means clustering", " Chapter 10 k-means clustering "],
["model-based-clustering.html", "Chapter 11 model-based clustering", " Chapter 11 model-based clustering "],
["multivariate-time-series.html", "Chapter 12 Multivariate time series", " Chapter 12 Multivariate time series Potential topics: computing features, and exploring feature space linked to individual time series plots projection pursuit of multiple time series, 1D indexes against time lag plots, multivariate shifting series against each other "],
["references.html", "References", " References "]
]
