<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Regression methods | Interactive and dynamic graphics for multivariate data using R</title>
  <meta name="description" content="This is a book about exploring multivariate data." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Regression methods | Interactive and dynamic graphics for multivariate data using R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a book about exploring multivariate data." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Regression methods | Interactive and dynamic graphics for multivariate data using R" />
  
  <meta name="twitter:description" content="This is a book about exploring multivariate data." />
  

<meta name="author" content="Di Cook, Ursula Laa, Stuart Lee, Earo Wang" />


<meta name="date" content="2021-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data.html"/>
<link rel="next" href="linear-discriminant-analysis-and-manova.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#audience"><i class="fa fa-check"></i><b>0.1</b> Audience</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#how-to-use-the-book"><i class="fa fa-check"></i><b>0.2</b> How to use the book?</a><ul>
<li class="chapter" data-level="0.2.1" data-path="index.html"><a href="index.html#what-do-we-assume-about-you"><i class="fa fa-check"></i><b>0.2.1</b> What do we assume about you?</a></li>
</ul></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#setting-up-your-workflow"><i class="fa fa-check"></i><b>0.3</b> Setting up your workflow</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#whats-different-about-space-beyond-2d"><i class="fa fa-check"></i><b>1.1</b> Whatâ€™s different about space beyond 2D</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#interactive-and-dynamic-graphics-literature"><i class="fa fa-check"></i><b>1.2</b> Interactive and dynamic graphics literature</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#an-opening-case-study"><i class="fa fa-check"></i><b>1.3</b> An opening case study</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#the-big-picture-of-the-book"><i class="fa fa-check"></i><b>1.4</b> The big picture of the book</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="toolbox.html"><a href="toolbox.html"><i class="fa fa-check"></i><b>2</b> Toolbox</a></li>
<li class="chapter" data-level="3" data-path="dimension-reduction.html"><a href="dimension-reduction.html"><i class="fa fa-check"></i><b>3</b> Dimension reduction</a></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Data</a><ul>
<li class="chapter" data-level="4.0.1" data-path="data.html"><a href="data.html#other-possible-sources-for-data"><i class="fa fa-check"></i><b>4.0.1</b> Other possible sources for data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-methods.html"><a href="regression-methods.html"><i class="fa fa-check"></i><b>5</b> Regression methods</a><ul>
<li class="chapter" data-level="5.1" data-path="regression-methods.html"><a href="regression-methods.html#support-vector-machine"><i class="fa fa-check"></i><b>5.1</b> Support vector machine</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-discriminant-analysis-and-manova.html"><a href="linear-discriminant-analysis-and-manova.html"><i class="fa fa-check"></i><b>6</b> Linear discriminant analysis and MANOVA</a></li>
<li class="chapter" data-level="7" data-path="trees-and-forests.html"><a href="trees-and-forests.html"><i class="fa fa-check"></i><b>7</b> Trees and forests</a><ul>
<li class="chapter" data-level="7.1" data-path="trees-and-forests.html"><a href="trees-and-forests.html#trees"><i class="fa fa-check"></i><b>7.1</b> Trees</a></li>
<li class="chapter" data-level="7.2" data-path="trees-and-forests.html"><a href="trees-and-forests.html#random-forests"><i class="fa fa-check"></i><b>7.2</b> Random forests</a></li>
<li class="chapter" data-level="" data-path="trees-and-forests.html"><a href="trees-and-forests.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="neural-networks-and-deep-learning.html"><a href="neural-networks-and-deep-learning.html"><i class="fa fa-check"></i><b>8</b> Neural networks and deep learning</a></li>
<li class="chapter" data-level="9" data-path="hierarchical-clustering.html"><a href="hierarchical-clustering.html"><i class="fa fa-check"></i><b>9</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="10" data-path="k-means-clustering.html"><a href="k-means-clustering.html"><i class="fa fa-check"></i><b>10</b> k-means clustering</a></li>
<li class="chapter" data-level="11" data-path="model-based-clustering.html"><a href="model-based-clustering.html"><i class="fa fa-check"></i><b>11</b> model-based clustering</a></li>
<li class="chapter" data-level="12" data-path="multivariate-time-series.html"><a href="multivariate-time-series.html"><i class="fa fa-check"></i><b>12</b> Multivariate time series</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interactive and dynamic graphics for multivariate data using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-methods" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Regression methods</h1>
<p>Topics to include:
- Many regression models
- Logistic regression</p>
<p></p>
<div id="support-vector-machine" class="section level2">
<h2><span class="header-section-number">5.1</span> Support vector machine</h2>
<p>% <a href="http://www.support-vector-machines.org/SVM_osh.html" class="uri">http://www.support-vector-machines.org/SVM_osh.html</a>
% wikipedia</p>
<p>A support vector machine (SVM)  is a binary classification
method. An SVM looks for gaps between clusters in the data, based on the
extreme observations in each class. In this sense it mirrors the
graphical approach described at the start of this chapter, in which we
searched for gaps between groups. We describe this method more fully
than we did the other algorithms for two reasons: first, because of
its apparent similarity to the graphical approach, and second, because
it is difficult to find a simple explanation of the method in the
literature.</p>
<p>The algorithm takes an <span class="math inline">\(n \times p\)</span> data matrix, where each column is
scaled to [$-$1,1] and each row is labeled as one of two classes
(<span class="math inline">\(y_i=+1\)</span> or <span class="math inline">\(-1\)</span>), and finds a hyperplane that separates the two
groups, if they are separable. Each row of the data matrix is a vector
in <span class="math inline">\(p\)</span>-dimensional space, denoted as</p>
<p>% Should this be represented as a row instead of a column? dfs</p>
<p><span class="math display">\[
\blX=\left[ \begin{array}{c}
  x_1 \\ x_2 \\ \vdots \\ x_p \end{array} \right]
\]</span></p>
<p>and the separating hyperplane can be written as</p>
<p><span class="math display">\[
\blW&#39;\blX + b = 0
\]</span></p>
<p>where <span class="math inline">\(\blW = [ w_1~~ w_2 ~~ \dots ~~ w_p]&#39;\)</span> is the normal
vector to the separating hyperplane and <span class="math inline">\(b\)</span> is a constant. The best
separating hyperplane is found by maximizing the margin of separation
between the two classes as defined by two parallel hyperplanes:</p>
<p><span class="math display">\[
\blW&#39;\blX + b = 1, ~~~~~ \blW&#39;\blX + b = -1.
\]</span></p>
<p>These hyperplanes should maximize the distance from the
separating hyperplane and have no points between them, capitalizing on
any gap between the two classes. The distance from the origin to the
separating hyperplane is <span class="math inline">\(|b|/||\blW||\)</span>, so the distance between the
two parallel margin hyperplanes is <span class="math inline">\(2/||\blW||=2/\sqrt{w_1^2+\dots +w_p^2}\)</span>. Maximizing this is the same as minimizing <span class="math inline">\(||\blW||/2\)</span>. To
ensure that the two classes are separated, and that no points lie
between the margin hyperplanes we need:</p>
<p><span class="math display">\[
\blW&#39;\blX_i + b \geq 1, ~~~\mbox{  or  } ~~~\blW&#39;\blX_i + b \leq -1 ~~~\forall i=1, ..., n
\]</span></p>
<p>which corresponds to</p>
<p><span class="math display">\[\begin{eqnarray}
y_i(\blW&#39;\blX_i+b)\geq 1 ~~~\forall i=1, ..., n
\label{svm-crit}
\end{eqnarray}\]</span></p>
Thus the problem corresponds to

<p></p>
<p>Interestingly, only the points closest to the margin
hyperplanes are needed to define the separating hyperplane. We might
think of these points as lying on or close to the convex hull of each
cluster in the area where the clusters are nearest to each other.
These points are called support vectors, and the coefficients of the
separating hyperplane are computed from a linear combination of the
support vectors <span class="math inline">\(\blW = \sum_{i=1}^{s} y_i\alpha_i\blX_i\)</span>, where <span class="math inline">\(s\)</span>
is the number of support vectors. We could also use <span class="math inline">\(\blW = \sum_{i=1}^n y_i\alpha_i\blX_i\)</span>, where <span class="math inline">\(\alpha_i=0\)</span> if <span class="math inline">\(\blX_i\)</span> is not
a support vector. For a good fit the number of support vectors <span class="math inline">\(s\)</span>
should be small relative to <span class="math inline">\(n\)</span>. Fitting algorithms can achieve gains
in efficiency by using only samples of the cases to find suitable
support vector candidates; this approach is used in the SVMLight
 software.</p>
<p>In practice, the assumption that the classes are completely separable
is unrealistic. Classification problems rarely present a gap between
the classes, such that there are no misclassifications.
 relaxed the separability condition to allow some
misclassified training points by adding a tolerance value <span class="math inline">\(\epsilon_i\)</span>
to Equation , which results in the modified criterion
<span class="math inline">\(y_i(\blW&#39;\blX_i+b)&gt;1-\epsilon_i, \epsilon_i\geq 0\)</span>. Points that meet
this criterion but not the stricter one are called slack vectors.</p>
<p>Nonlinear classifiers can be obtained by using nonlinear
transformations of <span class="math inline">\(\blX_i\)</span>, <span class="math inline">\(\phi(\blX_i)\)</span> , which is
implicitly computed during the optimization using a kernel function
<span class="math inline">\(K\)</span>. Common choices of kernels are linear
<span class="math inline">\(K(\blx_i,\blx_j)=\blx_i&#39;\blx_j\)</span>, polynomial
<span class="math inline">\(K(\blx_i,\blx_j)=(\gamma\blx_i&#39;\blx_j+r)^d\)</span>, radial basis
<span class="math inline">\(K(\blx_i,\blx_j)=\exp(-\gamma||\blx_i-\blx_j||^2)\)</span>, or sigmoid
functions <span class="math inline">\(K(\blx_i,\blx_j)=\mbox{tanh}(\gamma\blx_i&#39;\blx_j+r)\)</span>, where
<span class="math inline">\(\gamma&gt;0, r,\)</span> and <span class="math inline">\(d\)</span> are kernel parameters.</p>
<p>% She didnâ€™t say to delete the terminating colon here, but by
% analogy with these rest, I will. dfs</p>
<p>The ensuing minimization problem is formulated as</p>
<p><span class="math display">\[
\mbox{\em minimizing } \frac{1}{2}||\blW|| + C\sum_{i=1}^n \epsilon_i ~~ \mbox{\em subject to } 
y_i(\blW&#39;\phi(\blX)+b)&gt;1-\epsilon_i
\]</span></p>
<p>where <span class="math inline">\(\epsilon_i\geq 0\)</span>, <span class="math inline">\(C&gt;0\)</span> is a penalty parameter guarding
against over-fitting the training data and <span class="math inline">\(\epsilon\)</span> controls the
tolerance for misclassification. The normal to the separating
hyperplane <span class="math inline">\(\blW\)</span> can be written as <span class="math inline">\(\sum_{i=1}^{n} y_i\alpha_i{\phi(\blX_i)}\)</span>, where points other than the support and
slack vectors will have <span class="math inline">\(\alpha_i=0\)</span>. Thus the optimization problem
becomes</p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{\em minimizing } \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n y_iy_j\alpha_i\alpha_jK(\blX_i,\blX_j)+C\sum_{i=1}^n \epsilon_i \\ ~~~~~~~~~~~\mbox{\em subject to } 
y_i(\blW&#39;\phi(\blX)+b)&gt;1-\epsilon_i
\end{eqnarray*}\]</span></p>
<p>
</p>
<p>We use the {svm} function in the  package
 of R, which uses  , to classify
the oils of the four areas in the Southern region. SVM is a binary
classifier, but this algorithm overcomes that limitation by comparing
classes in pairs, fitting six separate classifiers, and then using a
voting scheme to make predictions. To fit the SVM we also need to
specify a kernel, or rely on the internal tuning tools of the
algorithm to choose this for us. Automatic tuning in the algorithm
chooses a radial basis, but we found that a linear kernel performed
better, so that is what we used. (This accords with our earlier visual
inspection of the data in Sect.~.) Here is the R
code used to fit the model:</p>

<p>These are our misclassification tables:</p>






<p>The training error is <span class="math inline">\(9/246=0.037\)</span>, and the test error is
<span class="math inline">\(6/77=0.078\)</span>. (The training error is the same as that of the neural
network classifier, but the test error is lower.) Most error
is associated with Sicily, which we have seen repeatedly to be an
especially difficult class to separate. In the training data there
are no other errors, and in the test data there are just two samples
from Calabria mistakenly classified. Figure~
illustrates our examination of the misclassified cases, one in each
row of the figure. (Points corresponding to Sicily were removed from
all four plots.) Each of the two cases is brushed (using a filled red
circle) in the plot of misclassification table and viewed in a linked
2D tour.  Both of these
cases are on the edge of their clusters so the confusion of classes is
reasonable.</p>
% Figure 14

% Figure 15

<p>The linear SVM classifier uses 20 support vectors and 29 slack vectors
to define the separating planes between the four areas. It is
interesting to examine which points are selected as support vectors,
and where they are located in the data space. For each pair of
classes, we expect to find some projection in which the support vectors
line up on either side of the margin of separation, whereas the slack
vectors lie closer to the boundary, perhaps mixed in with the points
of other classes.</p>
<p>  The plots in
Fig.~ represent our use of the 2D tour, augmented by
manual manipulation,~to look for these projections. (The Sicilian
points are again removed.) The support vectors are represented by open
circles and the slack vectors by open rectangles, and we have been
able to find a number of projections in which the support vectors are
on the opposing outer edge of the point clouds for each cluster.</p>
<p>The linear SVM does a very nice job with this difficult
classification. The accuracy is almost perfect on three classes,
and the misclassifications are quite reasonable mistakes, being points
that are on the extreme edges of their clusters. However, this method
joins the list of those defeated by the difficult problem of
distinguishing the Sicilian oils from the rest.</p>

<p></p>
<p>For some classification problems, it is possible to get a good picture
of the boundary between two classes. With LDA and SVM classifiers the
boundary is described by the equation of a hyperplane. For others the
boundary can be determined by evaluating the classifier on points
sampled in the data space, using either a regular grid or some more
efficient sampling scheme.</p>
% Figure 16

<p> 
 We use the R package 
 to generate points illustrating boundaries, add those
points to the original data, and display them in GGobi.
Figure~ shows projections of boundaries between
pairs of classes in the . In each example, we used
the 2D tour with manual control~to focus the view on a projection that
revealed the boundary between two groups.</p>
% Needs to be checked

<p>The top two plots show tour projections of the North (purple) and
Sardinia (green) oils where the two classes are separated and the
boundary appears in gray. The LDA boundary (shown at left) slices too
close to the Northern oils. This might be due to the violation of the
LDA assumption that the two groups have equal variance; since that is
not true here, it places the boundary too close to the group with the
larger variance. The SVM boundary (at right) is a bit closer to the
Sardinian oils than the LDA boundary is, yet it is still a tad too
close to the oils from the North.</p>
<p>The bottom row of plots examines the more difficult classification of
the areas of the South, focusing on separating the South Apulian oils
(in pink), which is the largest sample, from the oils of the other
areas (all in orange). Perfect separation between the classes does not
occur. Both plots are tour projections showing SVM boundaries, the
left plot generated by a linear kernel and the right one by a radial
kernel. Recall that the radial kernel was selected automatically by
the SVM software we used, whereas we actually chose to use a linear
kernel. These pictures illustrate that the linear basis yields a more
reasonable boundary between the two groups. The shape of the clusters
of the two groups is approximately the same, and there is only a small
overlap of the two. The linear boundary fits this structure
neatly. The radial kernel wraps around the South Apulian oils.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-discriminant-analysis-and-manova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["mulgar.pdf", "mulgar.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
