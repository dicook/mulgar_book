\chapter{Miscellaneous Topics}

% Includes one \newpage command. dfs

% Structure of each section: Assuming they mirror the complete
% chapters, they should include the same parts, but shorter.
% (This may not be the correct assumption, but it gives me a 
% place to start writing the graph section, and it helps us
% remember that we want to keep an eye on the structure.)
%
% Introduction, explaining (1) where such data occurs,
%  (2) the goals in analyzing it, and (3) some of the
%  challenges that arise in the analysis.
%
% Background, with a more formal description of the
% data and the model.  (I don't know what to say about
% models for graphs yet ...)
%
% Graphical exploration
%
% Numerical methods -- especially the use of graphical
% methods to test model assumptions and evaluate model 
% accuracy.
%
% Recap -- or maybe not, since the sections are shorter
% than chapters.
%
% Exercises

% In addition, we'll need to add new sections to the
% Data chapter.  For graphs, maybe perm4 and Florentine
% families and business dealings.

% And, of course, new references and index entries.

Analysts often encounter data that cannot be fully analyzed using
the methods presented in the preceding chapters. In this chapter we
introduce, however briefly, some of these different kinds of data and
the additional methods needed to analyze them, beginning with a
section on inference.  A more complete treatment of each topic will be
found on the book web site.

\section{Inference}

\index{inference} Revealing, informative plots often provoke the
question ``Is what we see really there?''  When we see a pattern in a
plot, it is sometimes completely clear that what we see is ``really
there,'' and that it corresponds to structure in the data, but
sometimes we are not so sure.

What is meant by \Term{really there}?  Sometimes a pattern is
detected, either numerically or visually, even when there is nothing
happening. In the hypothesis testing context, we might say that the
pattern is consistent with a null scenario.  For example, in a simple
linear regression scenario with two variables X, Y, we are interested
in the \Term{dependence} between the two variables.  We call the
presence of this dependence the \Term{alternative
hypothesis}\index{hypothesis testing!alternative hypothesis}, and then
the natural \Term{null hypothesis} \index{hypothesis testing!null
hypothesis} is that the two variables are \Term{independent}.  A
common numerical ``pattern'' that suggests linear dependence is the
correlation.  The plots in Fig.~\ref{inf1} all show pairs of
variables with correlation close to 0.7, but most of these plots are
not consistent with the null hypothesis.  The top left plot is the
only one that shows the relationship we probably imagine when we hear
that two variables have a correlation of 0.7, a perfect example of
linear dependence between X and Y.

The remaining plots show very different departures from independence.
The top right plot shows an example where the correlation is
misleading: The apparent dependence is due solely to one sample point,
and the two variables are in fact not dependent. The plot at bottom
left shows two variables that are dependent, but the dependence is
among sub-groups in the sample and it is negative rather than
positive. The plot at bottom right shows two variables with some
positive linear dependence, but the obvious non-linear dependence is
more interesting.

\begin{figure*}[htp]
\centerline{{\includegraphics[width=2in]{chap-inference/infer1a.pdf}}
  {\includegraphics[width=2in]{chap-inference/infer1b.pdf}}}
  \centerline{{\includegraphics[width=2in]{chap-inference/infer1c.pdf}}
  {\includegraphics[width=2in]{chap-inference/infer1d.pdf}}}
\caption[Comparison of various pairwise relationships with the same
correlation]{Studying dependence between X and Y. All four pairs of
variables have correlation approximately equal to 0.7, but they all
have very different patterns. Only the top left plot shows two
variables matching a dependence modeled by correlation. }
\label{inf1}
\end{figure*}

With graphics we cannot only detect a linear trend, but virtually any
other trend (nonlinear, decreasing, discontinuous, outliers) as
well. That is, we can easily detect many different types of dependence with
visual methods.

The first step in using visual methods to determine whether a pattern
is ``really there'' is to identify an appropriate pair of hypotheses,
the null and an alternative.  The second step is to determine a
process that simulates the null hypothesis to generate comparison
plots. Some common null hypothesis scenarios are as follows:

\begin{enumerate}
\item Distributional assumptions are tested by simulating
samples from the distribution using parameters estimated from the
sample. For example, if we suspect, or hope, that the data is
consistent with a normal population, we simulate samples from a normal
distribution using the sample mean and variance--covariance matrix as
the parameters. This approach was used in Sect.~\ref{CMS}, because
linear discriminant analysis assumes that the data arises from a
multivariate normal mixture.

\item Independence assumptions are tested using permutation methods,
where the appropriate columns of data values are shuffled to break the
association in the ordered tuples.  When there are two variables,
shuffle one of the columns, say the X-values, and make plots of the
permuted data to compare with the real data. This approach arises from
the methods used in permutation tests \cite{Good05}.

\item In labeled data problems, the assumption that the labels matter
is tested by shuffling the labels. For example, in a designed
experiment with two groups, control and treatment, randomly re-assign
the control and treatment labels. Similarly, for a supervised
classification problem, shuffle the class ids.
%Exact tests, null hypotheses with Neyman structure, simulate the conditional 
%distribution given the sufficient statistic.
\end{enumerate}

\begin{figure*}[htp]
\centerline{{\includegraphics[width=1.6in]{chap-misc/flea-infer8.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-infer2.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-infer3.pdf}}}
\centerline{{\includegraphics[width=1.6in]{chap-misc/flea-infer4.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-infer5.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-infer6.pdf}}}
\centerline{{\includegraphics[width=1.6in]{chap-misc/flea-infer7.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-infer1.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-infer9.pdf}}}
\caption[Testing the assumption that species labels correspond to
groups in the data]{Testing the assumption that species labels
correspond to three groups in the data, using a projection pursuit
guided tour with the LDA index. The plots show optional projections of
the same data, except that in eight of the nine plots, the species
labels have been permuted.  The plot of the real data is the odd one
out, and very easy to spot!}
\label{inf2}
\end{figure*}

Under these scenarios the \Term{test statistic} \index{hypothesis
testing!test statistic} is the plot of the real data. If the plot of
the data is noticeably different from all of the plots of data
generated under the null scenario, then we have evidence for the
presence of real structure in the data.

For an example of making and testing inferences in a supervised
classification problem, we use the \Data{Flea Beetles}
\index{datasets!\Data{Flea Beetles}} data.  Our goal is to find
clusters corresponding to the three different species:

\begin{verbatim}
> library(rggobi)
> d.flea <- read.csv("flea.csv")
> attach(d.flea)
> gd <- ggobi(d.flea)[1]
\end{verbatim}

\noindent Once the points have been colored according to species, a 
projection pursuit guided tour \index{tour!projection pursuit guided}
using the LDA index \index{projection pursuit!indexes!LDA} is used
to find clusters corresponding to the species:

\begin{verbatim}
> ispecies = as.integer(species)
> gtype <- rep(6,74)
> gtype[ispecies==1] <- 4; gtype[ispecies==3] <- 3
> glyph_type(gd) <- gtype; glyph_size(g) <- 3

> gcolor = rep(1,74)
> gcolor[ispecies==1] <- 5; gcolor[ispecies==3] <- 6
> glyph_color(gd) <- gcolor
\end{verbatim}

\noindent 
Repeat the process numerous times with the species labels scrambled:

\begin{verbatim}
> pspecies <- sample(ispecies) 

> gtype <- rep(6,74)
> gtype[pspecies==1] <- 4; gtype[pspecies==3] <- 3 
> glyph_type(gd) <- gtype

> gcolor <- rep(1,74) 
> gcolor[pspecies==1] <- 5; gcolor[pspecies==3] <- 6
> glyph_color(gd) <- gcolor
\end{verbatim}

\noindent 
In Fig.~\ref{inf2}, the plot of the best projection of the real data
is embedded among plots of the best projections of the data with
permuted labels. Which one is different? The real data plot is easily
distinguished from the rest, offering evidence for the existence that
the data space includes clusters corresponding to the labeled
classes.

\begin{figure*}[htp]
\centerline{{\includegraphics[width=1.6in]{chap-misc/flea-sim-infer8.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-sim-infer2.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-sim-infer3.pdf}}}
\centerline{{\includegraphics[width=1.6in]{chap-misc/flea-sim-infer1.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-sim-infer4.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-sim-infer5.pdf}}}
\centerline{{\includegraphics[width=1.6in]{chap-misc/flea-sim-infer6.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-sim-infer7.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/flea-sim-infer9.pdf}}}
\caption[Testing the assumption that species labels correspond to
groups in simulated data]{Testing the assumption that species labels
correspond to three groups in simulated data, using a projection
pursuit guided tour with the LDA index. The data is similar to
\Data{Flea Beetles} in that there are 74 cases, divided into three classes, 
but there are now 50 variables instead of 6. The plots show optional
projections of the same data, except that in eight of the nine plots,
the species labels have been permuted.  The plot of the real data
(left plot of the middle row) is not distinguishable from the others!}
\label{inf3}
\end{figure*}

In contract, Fig.~\ref{inf3} shows an example where the data is
consistent with the null hypothesis. The data used here is based on
the \Data{Flea} example, except that here we have created 50 new
variables, each samples of size 74 from a univariate standard normal
distribution.  To create the ``real'' data, the species labels are the
same as the \Data{Flea Beetles} data, and to create the comparison
datasets, these labels are permuted. There should be no difference
between the real data and the permuted label data, because there are
no real clusters in the data. The plots verify this. Each is a
projection produced by a projection pursuit guided tour using the LDA
index. The real data (left plot of middle row) shows clustering of the
three species, but so do the other plots, produced by permuting the
species labels. The real data is consistent with the null hypothesis
that there is no real clustering of the data. The apparent clustering
of the data occurs because we have a small number of data points in
high dimensions. Here is the code that produced the simulated data:

\begin{verbatim}
> x <- matrix(rnorm(74*50),ncol=50)
> colnames(x) <- paste("V",1:100,sep="")
\end{verbatim}

\noindent The remaining code to color the points (first by species,
and again after permuting the species labels) is the same as that
given on the preceding pages for the \Data{Flea Beetles} data.

In summary, it is possible to build inference into graphical
exploration of data. The full chapter available on the web provides
more background and several more examples.

\section{Longitudinal data}

\index{longitudinal data}
\index{panel data|see{longitudinal data}}
\index{cross-sectional studies}
\index{time series}

In longitudinal data (sometimes called panel data), individuals are
repeatedly measured through time, enabling the direct study of change
\cite{DHLZ02}.  A longitudinal study focuses on the way the
relationship between the response variables and the covariates
changes.  Unique to longitudinal studies is the ability to study
individual responses.  In repeated cross-sectional studies, on the
other hand, the study of individual responses is not supported,
because a new sample is taken at each measurement time; this supports
studies of societal trends but not individual experiences.  A
longitudinal dataset supports such investigations because it is
composed of a collection of time series, one for each individual.  The
challenge posed by the data arises because these time series may not
be uniform in structure.  Each individual may have special
characteristics, and measurements on different topics or variables may
be taken each time an individual is measured.  The time
characteristics of the data can also vary between individuals, in the
number and value of the time points, and in the intervals between
them.

These irregularities yield data that is difficult to work with.  If it
were equi-spaced over time, and if each individual had the same number
of time points and covariates, it would be easier to develop formal
models to summarize trends and covariance.  Still, if we want to learn
what this data has to offer, we have to find a way to work with it.

To organize the data, we denote the {\em response} variables to be
$\blY_{ijt_i}$, and the time-dependent explanatory variables, or {\em
covariates} to be $\blX_{ikt_i}$, where $i=1,\dots , n$ indexes the
number of individuals in the study, $j=1, \dots ,q$ indexes the number
of response variables, $k=1, \dots ,p$ indexes the number of
covariates, and $t_i=1, \dots , n_i$ indexes the number of times
individual $i$ was measured.  Note that $n$ is the number of subjects
or individuals in the study, $n_i$ is the number of time points
measured for individual $i$, $q$ is the number of response variables,
and $p$ is the number of explanatory variables, measured for each
individual and time. The explanatory variables may include indicator
variables marking special events affecting an individual. There may
also be time-independent explanatory variables or covariates, which we
will denote as $\blZ_{il}$, $i=1,\dots ,n$, $l=1, \dots ,
r$. 

\index{datasets!\Data{Wages}} 

For the \Data{Wages} data, there are $n=888$ individuals, $q=1$
response variable (\Vbl{lnw}), and $p=1$ time-dependent covariate
(\Vbl{uerate}).  The measurements are time-indexed by the length of
time in the workforce (\Vbl{exper}), measured in years, when the wages
were recorded.  Note that the data does not include any variable
representing time per se, but that \Vbl{exper} stands in for time.  We
reorganize the data into two tables.  The first table contains the
time variable and the time-dependent measurements:
$\blY_{ij\Vbl{exper}_i}$, $\blX_{ik\Vbl{exper}_i}$ (\Vbl{exper},
\Vbl{lnw}, \Vbl{uerate}).  The second table, much shorter, contains
the $r=4$ time-independent covariates (\Vbl{ged}, \Vbl{black},
\Vbl{hispanic}, and \Vbl{hgc}).  In addition, both tables include the
subject \Vbl{id}'s, and that is how they are linked.  (Portions of the
tables are shown in Chap.~7.)

\begin{figure*}[htp]
\centerline{{\includegraphics[width=1.6in]{chap-misc/wages-sample1.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/wages-sample3.pdf}}
  {\includegraphics[width=1.6in]{chap-misc/wages-sample4.pdf}}}
\caption[Exploring the longitudinal \Data{Wages} data]{Looking
over the longitudinal \Data{Wages} data.  Longitudinal plots of
sampled subjects, highlighted in orange against all the data, showing
variability from subject to subject.}
\label{wages-sample}
\end{figure*}

With so many subjects, an initial plot of \Vbl{lnw} against
\Vbl{exper} is nothing but a tangle of over-plotted lines.  Imagine
looking at the plots in Fig.~\ref{wages-sample} without any
highlighting!

\begin{verbatim}
> library(rggobi)
> gg <- ggobi() # Open wages3.xml, and draw the edges
> d.wages1 <- gg[1]
> d.wages3 <- gg[3]
\end{verbatim}

\index{animation}
\noindent To get acquainted with any one subject, we can leave the
plot in place and highlight the profile for that subject in a
contrasting color.  A quick overview of the variability from subject
to subject can be obtained by animating that process for a sample of
subjects, highlighting their profiles one by one:

\begin{verbatim}
> smp <- sample(1:888,50)
> for (i in smp) {
   gtype <- rep(1,6402); gcolor <- rep(1,6402)
   gtype[d.wages1[[1]]==d.wages3[[1]][i]] <- 6
   gcolor[d.wages1[[1]]==d.wages3[[1]][i]] <- 9
   glyph_type(d.wages1) <- gtype
   glyph_color(d.wages1) <- gcolor
   glyph_size(d.wages1) <- 4
 }
\end{verbatim}

% dfs: The edges aren't highlighted -- is that deliberate?
% Di: The edges don't highlight for me! I wish they would!

\noindent The profiles of three subjects are highlighted in
Fig.~\ref{wages-sample}.  The subject highlighted in the left plot has
a slow but steady increase in wages as his experience grows, suddenly
terminated by a big drop.  For the second subject, we have only a few
early measurements, during which time his wages increased rapidly. For
the third subject, we have once again only a few measurements
capturing the beginning of his work life, but his wages fluctuated
wildly.  Using the animation to study more subjects, we are surprised
to see how much the wage profiles differ from subject to subject!

This variability suggests that we might try to summarize the data by
characterizing the different types of profiles. We will calculate
several numerical indicators for each profile and use these to study
the subjects. Initially, we count the number of data points for each
subject so that we can exclude subjects with very few measurements. 

\begin{verbatim}
> wages.count <- summary(d.wages1[[1]], maxsum=888)
\end{verbatim}

\noindent 
We then calculate descriptors for the profiles: standard
deviation of \Vbl{lnw} (log wages), standard deviation of the
differences between consecutive measurements, and linear trend,
calculated both classically and robustly.

\newpage
\begin{verbatim}
> wages.linear <- rep(0,888)
> wages.rlin <- rep(0,888)
> wages.sd <- rep(0,888)
> wages.sddif <- rep(0,888)
> for (i in 1:888) {
+   x <- d.wages1[d.wages1[[1]]==d.wages3[[1]][i]][3:2]
+   if (dim(x)[1]>1) {
+     wages.sd[i] <- sd(x)
+     difs <- NULL
+     for (j in 2:length(x))
+       difs <- c(difs, 
          d.wages1[[2]][x[j]] - d.wages1[[2]][x[j]-1])
+     wages.sddif[i] <- sd(difs)
+   }
+   if (dim(x)[1]>2)
+     wages.linear[i] <- coef(lm(lnw~exper,data=x))[2]
+   if (dim(x)[1]>3) {
+     wgts <- 1/(residuals(lm(lnw~exper,data=x))^10+1) 
+     wages.rlin[i] <- coef(lm(lnw~exper,data=x,
+       weights=wgts))[2]
+   }
+   cat(i," ")
+ }
\end{verbatim}

\noindent 
Add this additional data to GGobi to explore profiles with different
characteristics:

\begin{verbatim}
> gg["descriptors"] <- data.frame(id=d.wages3$id, 
    count=wages.count, sd=wages.sd, 
    sddif=wages.sddif, linear=wages.linear,
    rlin=wages.rlin)
\end{verbatim}

\noindent 
\index{brushing!linked} \index{linking!categorical}
Figure~\ref{wages-long} illustrates exploring profiles by linked
brushing in plots of these descriptors. (Subjects with only one or two
measurements have been excluded from these plots.) In the top two rows
of plots we examine the volatility in wage experiences by plotting the
standard deviations of all the subjects' measurements (\Vbl{sd})
against the standard deviations of differences between consecutive
measurements (\Vbl{sddif}).  Large values of both indicate subjects
who have a more varied wage experience (top row). Large values of
\Vbl{sd} but small values of \Vbl{sddif} indicate subjects whose wage
experience has varied over time but in a consistent manner, such as a
steady increase or decrease (middle row). In the bottom row of plots
we examine the slope from a robust linear fit (\Vbl{rlin}) relative to
\Vbl{sddif} to find subjects whose wages have steadily increased.

% figure 5
\begin{figure*}[htp]
\vspace{.75in}
\centerline{{\includegraphics[width=2.0in]{chap-misc/wages-long1a.pdf}}
  {\includegraphics[width=2.0in]{chap-misc/wages-long1b.pdf}}}
\centerline{{\includegraphics[width=2.0in]{chap-misc/wages-long2a.pdf}}
  {\includegraphics[width=2.0in]{chap-misc/wages-long2b.pdf}}}
\centerline{{\includegraphics[width=2.0in]{chap-misc/wages-long3a.pdf}}
  {\includegraphics[width=2.0in]{chap-misc/wages-long3b.pdf}}}
\caption[Using linked brushing to explore subject profiles]{Using
linked brushing to explore the wage profiles of individual subjects.}
\label{wages-long}
\end{figure*}

In summary, it is possible to penetrate difficult data like ragged
time longitudinal data using interactive graphics. From this data,
we have already learned that the men in this study have widely varied
wage experiences.  We would probably clean the data further before
doing a more detailed analysis, removing subjects with too few
measurements or too short a time in the workforce. The full chapter
develops this analysis further and offers more approaches to
longitudinal data.

%\section{Spatio-temporal Data}

%When data arises in a spatial, or geographic, and temporal context it
%is called spatio-temporal data. Often there are multiple measurements
%for each spatial location and time point. This type of multivariate
%spatio-temporal data is important for studying climate change and many
%other contemporary problems. With this data in addition to the
%multivariate relationships, we would like to explore the spatial and
%temporal trends, and find sites that are different from their
%neighbors or clusters in the spatial domain. 

%We will use the full \Data{TAO} dataset to illustrate some approaches.
%The \Data{TAO} has 

%?Notation?
%References: \cite{HBCUW91}, Unwin, etc, look at Juergen's papers


%\section{Compositional data}

%\section{Regression}

\section{Network data}

% Data: FlorentineFam.xml, and adjtrans4.xml (better name
%   than perm4.xml)

% Introduction  .... maybe this is Introduction and Background.

\index{graph} \index{graph!node} \index{graph!edge} Fundamental to
network data is a graph. A graph consists of a set of points (also
called nodes or vertices), some of which are connected by edges, and
this common mathematical object appears in many areas of computer
science, engineering, and data analysis.  In telecommunications, for
example, a node represents a network element, an IP address, or a
telephone number, and an edge represents the network connection or
traffic between a pair of nodes.  In \Term{social network analysis}
\cite{Wasserman94}, \index{social network} a node most often
represents a person, and an edge the connections between two people.
Social network analysis emerged as an academic discipline in about the
1930s, with networks studied for a wide variety of purposes: to
analyze organizational behavior; to understand the process by which a
disease is spread; to identify suspected criminals; and to study social
taboos in unfamiliar cultures.  This discipline has recently become
quite well known because of the popularity of
Facebook\textsuperscript{\textregistered} and
MySpace\textsuperscript{\textregistered}, web sites where people
register (adding a new node in the graph) and then create links to
their friends (adding edges to the graph).

In analyzing network data, we are interested in the attributes of the
graph itself as well as those of the nodes and edges of which it
is composed.

For example, a graph may be \Term{fully connected}, in which every
node is connected to every other node, or it may be \Term{sparse},
with few edges.  A graph may show clusters, distinct \Term{sub-graphs}
that are densely connected within themselves but very sparsely
connected to one another.  Such patterns correspond to sub-communities
that interact only through a few influential individuals.

That presents an example of an interesting node attribute, because we
may want to identify these influential individuals.  One way to
measure influence is the \Term{degree} of a node, which is the number
of other nodes to which it is directly connected.  Yet another is the
\Term{betweenness centrality}, a measure of a node's influence based
on the number of minimum paths that pass through it. Many of these
attributes can be identified algorithmically, and there is a large
body of work in discrete mathematics and computer science on which
these algorithms are based.
% Di: Can you put a reference or two here?

% Knuth's name is the best known to me, but the second book here
% seems to be used as a textbook a lot.
% Algorithm Design, Foundations, Analysis and Internet Examples, by M. T. Goodrich, R. Tamassia, John Wiley & Sons Inc, 2002, ISBN: 0-471-38365-1
% T. Cormen, C. Leiserson, and R. Rivest. Introduction to Algorithms. MIT Press, Cambridge MA, 1990.
% D. Knuth. The Stanford GraphBase: a platform for combinatorial computing. ACM Press, New York, 1994.

Edges, of course, have interesting attributes as well, such as
weights.  In \Term{directed graphs}, edge direction is significant,
and then the edges are most often represented by arrows.  An example
is a social network in which a person provides a list of people they
consider friends, establishing a set of edges from the subject's node
to the nodes of their friends.

\index{graph!layout} \Term{Graph layout} algorithms read the lists of
nodes and edges, and generate a position for each node with the goal
of revealing structure in the graph [see, for example,
\citeasnoun{DiBETT98}].  A good layout algorithm will also use data
ink efficiently and avoid distracting artifacts.  The algorithm will
manage properties such as edge length, edge crossings, number of
different slopes, node separation, and aspect ratio.  There is usually
no single best layout for any graph, and it is useful to look at the
same graph in several different layouts, since each one may reveal
different global or local structure in the graph. There are many
commercial and open source software packages designed for graph
layout.  Galleries of sample graphs on {\tt graphviz.org}, {\tt
aisee.com}, and {\tt tomsawyer.com} demonstrate the variety of graphs
and graph layout algorithms.

% Possible figure: the same graph in two different layouts

For a statistician studying graph data, the layout and description of
the graph may be only part of the story, because the nodes and edges
may each have a number of attributes, both real-valued and
categorical.  We may know the gender, age, or other demographic
variables for each person in a social network; if we are conducting a
study of disease transmission, we even may have longitudinal data
representing repeated medical test results.  Edge data may also be as
simple as a single number, such as a real-valued measure of frequency
of contact, or as complex as a multivariate time series capturing
contacts of many types.

The richer the node and edge data becomes, the more valuable it is to
be able to display the graph in the context of interactive data
visualization software that allows us to link the graph to graphical
displays of the associated data.

% Background

% Graphical Exploration

%\subsubsection{Purely graphics}

% Florentine families
\index{datasets!\Data{Florentine Families}}

The following example is based on the \Data{Florentine Families} data.
The data comes from a study of the families of Renaissance Florence,
where connections between about 50,000 people from 1,300 to 1,500 were
constructed from primary sources \cite{Padgett93}.  A tiny subset of
this data that contains information about 16 families is used here
~---~ it is a canonical dataset in social network analysis.

\begin{verbatim}
> library(graph, SNAData)     # Bioconductor packages
> data(florentineAttrs, business, marital)
> families = florentineAttrs  # nodes
> ties = families[,"NumberTies"]
> gg = ggobi(families)
\end{verbatim}

The families will be the nodes in the graph.  For these families, we
have three real-valued variables: their wealth in 1,427, their total
number of business and family ties within a larger set of families,
and their number of seats on the governing body of Florence (called
Priorates).  In addition, there are two sets of edges.  In one set, a
link between two families means that they have financial dealings with
one another: 

\begin{verbatim}
> e = t(edgeMatrix(business)) # first set of edges
> # Prepare to add edge names
> src = nodes(business)[e[,1]]
> dest = nodes(business)[e[,2]]
> edgenames = paste(src, dest, sep="->")
> # Add edge weights: the average of the two node variables
> weights = matrix((ties[e[,1]] + ties[e[,2]]) / 2, ncol=1)
> dimnames(weights) = list(edgenames, c("meanTies"))
>
> gg$business = data.frame(weights)
> edges(gg$business) = cbind(src, dest)
\end{verbatim}

\begin{figure}[htbp]
\centerline{
  \includegraphics[width=2.0in]{chap-misc/flo-scat.pdf} }
\centerline{
  \includegraphics[width=2.0in]{chap-misc/flo-bus.pdf}
  \includegraphics[width=2.0in]{chap-misc/flo-mar.pdf}
}
\caption[Business and marital relationships among Renaissance
Florentine families]{Business and marital relationships among 16
Renaissance Florentine families.}
\label{florentine1}
\end{figure}

\noindent
We also added an edge attribute.  A weight for each edge was
calculated by averaging the \Vbl{NumberTies} of the source and
destination nodes.  An ideal edge weight would convey the number of
business ties between the two families, but the dataset does not
include that information, so instead we are using a weight that is
large when the average number of {\em all} business ties for the two
connected families is high.  We can use this to draw our attention to
links between well-connected families.

The other edge set contains marital relationships, and we use similar
code to add this second set of edges.  We then have two distinct
graphs with a common set of nodes but different edges.  How different
are they?

In order to look at the graphs, we have to lay them out.  We use the
{\tt neato} layout algorithm from the {\tt graphviz}\cite{GanNor00}
library; it is available from GGobi if the library is installed.

The bottom row of plots in Fig.~\ref{florentine1} shows the two
graphs, and they are quite different.  Most families are connected by
marriage; only one family is not connected, as shown by the single
unconnected point in the graph of marital ties.  In contrast, five
families do not have financial dealings with other families in the
data, as shown by the five unconnected points in the graph of
business ties.

We might also identify the dominant families, using other information
about each family.  In a scatterplot of \Vbl{NumberTies} versus
\Vbl{Wealth}, at the top of \ref{florentine1}, we highlight the
Medicis with an orange square and the Strozzis with a green circle.
Both families have a lot of ties with other families and great wealth.
From the graphs, we see that they seem to have played very different
roles in the society.  Both families were highly interconnected by
marriage to other families ~---~ although not directly to each other
~---~ but only the Medicis had many financial relationships.

\begin{figure}[htbp]
\centerline{
  \includegraphics[width=2.0in]{chap-misc/flo-mar-edge.pdf}
  \includegraphics[width=2.0in]{chap-misc/flo-mar-edges.pdf}
}
\caption[Highlighting the links involving the greatest number of
ties]{Highlighting the links involving the greatest number of ties.
The average shifted histogram (ASH) {\bf (right)} plots the edge
weights, and the points in the ASH are linked by color to the edges in
the graph.}
\label{florentine2}
\end{figure}

% Di: Is this AveNTies for the marital relationships graph?
%  Yes.
Next we study the connections between families. Plots of the edge
weight variable, \Vbl{AveNTies}, can be linked to the
graphs. Figure~\ref{florentine2} shows an ASH of \Vbl{AveNTies} linked
to the graph of marital relationships.  We have highlighted in green
the links with the highest values of \Vbl{AveNTies}, which reflect
both marital and business ties.  We see that most links are connected
to the Medici node, which leads us to believe that the large edge
weights reflect the many financial activities of the Medici family.
The edges with the next highest values of \Vbl{AveNTies} are all
connected to the Strozzi node; as we know, this family's
relationships are principally through marriage.  \index{average
shifted histogram (ASH)}

Even though the marital graph does not show clustering in any obvious
way, this pair of graphs reinforces the impression of a division
within the dominant families of Florence of that era.

%\subsubsection{Graph layout in higher dimensions}
% or Graph layout methods

% adjtrans4.xml
% Plagiarizing shamelessly from the 1992 paper on xgvis, 
%  Littman, Swayne, Dean and Buja

Graphs can also exist in higher dimensions. We use an adjacent
transposition graph as an example.  This family of graphs has special
meaning to discrete mathematicians, and
% I looked for a reference for this and could not find one.
% It was passed on to us by Peter Winkler
% (*** who is this, can you give a full name, or reference?), who was
% using it to study extensions of partial
% orderings. 
\citeasnoun{GThom93} has also used it to depict relationships between
surveyed preference judgments, such as the results of an election.
%For exploratory and explanatory purposes, it is useful
%to have a drawing that shows nodes as close together if and only if
%they are close together in the actual graph.

\begin{figure}[htbp]
\centerline{\includegraphics[width=2.0in]{chap-misc/adjtrans3.pdf}}
\caption[The $n=3$ adjacency transposition graph]{The $n=3$ adjacency 
transposition graph.}
\label{adjtrans3}
\end{figure}

The $n = 3$ adjacent transposition graph is generated as follows.  We
start with all permutations of the sequence 1, 2, 3. There are $3!$,
or six, such sequences, and we make each a vertex in the graph. We
connect two vertices by an edge if one permutation can be turned into
the other by simply transposing two adjacent elements.  This graph is
easy to lay out and draw by hand, because it is nothing more than a
hexagon, as shown in Fig.~\ref{adjtrans3}.  Notice that the $n = 3$
graph shows no edge crossings in the plane, the two-dimensional (2D)
layout space.

Now consider the $n = 4$ adjacent transposition graph.  This graph has
$n! = 24$ vertices.  The rule for connecting vertices works such that
the node corresponding to 2134 is connected to the node for 1234
because one can be turned into the other by swapping the middle two
digits. However, 2134 is not connected to 1423 because there is no way
to move from one to the other with a single adjacent flip.  How many
flips would it take?  What we would like is a representation of the graph
that makes that easier to see, and this is not something most of us
can draw by hand.

\begin{figure}[htbp]
\centerline{
  \includegraphics[width=2.0in]{chap-misc/at4-radial.pdf}
  \includegraphics[width=2.0in]{chap-misc/at4-dot.pdf}
}
\centerline{
  \includegraphics[width=2.0in]{chap-misc/at4-neato2.pdf}
  \includegraphics[width=2.0in]{chap-misc/at4-neato3.pdf}
}
\caption[The $n=4$ adjacency transposition graph in various
layouts]{The $n=4$ adjacency transposition graph in various layouts:
{\tt radial}, {\tt dot}, {\tt neato} in the plane, and {\tt neato} in
three dimensions (3D).  After the 3D layout, we see that the resulting
object is composed of squares and hexagons.}
\label{adjtrans4}
\end{figure}

We will experiment with the different graph layout algorithms.
Figure~\ref{adjtrans4} shows four different layouts for this graph.
Most layout methods used in GGobi are available through {\tt
graphviz}; the exception is the {\tt radial} layout algorithm, which
was implemented based on \citeasnoun{Wills99}.

The first layout reveals a very structured object, but it includes
many edge crossings and offers no illumination about the structure of
the data.  The algorithm used ({\tt radial}) takes the first point as
its center and lays out the rest of the points in concentric circles
around it.  It is obviously inappropriate for this data, but it can be
very helpful with some kinds of social network data.

The second layout is even more regular, but it is not right either.
It was produced using {\tt dot}, which is a hierarchical layout
algorithm that is designed for trees.  Since we know this graph far
from being a tree, we just tried out this algorithm for fun.

With the third and fourth layouts, we hit pay dirt.  They were both
produced with {\tt neato}, which is a layout algorithm designed for
just this kind of data: \Term{undirected} graphs, where it makes no
difference whether we say that an edge is from 1243 to 1234 or from
1234 to 1243.  Neato constructs a virtual physical model, as if each
edge were a spring, and runs an iterative solver to minimize the
energy of the model.  The layout shown on the left was produced in 2D,
and the one on the right in 3D.  They do not look much different here,
but after producing the 3D layout, we can manipulate it in a 2D tour
and look at it from all angles.  When we do that, the edge crossings
that interfere with interpretation in the 2D projection are not a
problem.

What do we see?  The resulting shape is formed of hexagons and
squares; one of each is highlighted in the two {\tt neato} plots.  The
shape is a \Term{truncated octahedron}.  Its component shapes, in the
language of graph theory, are \Term{six cycles} and \Term{four
cycles}, i.e. ``loops'' composed of six and four points.
% Each six cycle is the
% collection of all permutations in which one end is held fixed and all
% permutations of the remaining three digits enumerated. Each square
% consists of the permutations which have the same symbols in the first
% two positions and the last two positions.  
We leave it to you to look at the graph in GGobi and see what
characterizes these structures \cite{LSDB92}.

%What was invisible is now clear. We can see in Figs.~7 and 8 that
%2134 and 1423 are just 3 flips apart and that there are two paths of
%that length. Interestingly, the sequence 1243 lies on both of
%them. The pair of nodes corresponding to 2134 and 1243 are both on a
%single square face. This face consists of all sequences which start
%with 1 and 2 and end with 3 and 4. The pair 1243 and 1423 are together
%on 2 hexagonal faces, one represents all sequences ending with 3 and
%the other all those starting with 1.

In summary, network data analysis requires methods and algorithms that
illuminate the structure in the graph itself, but it may also require
the full toolkit used for multivariate data analysis.  Even when only
limited data is available for the nodes and edges, laying out a graph
in a high-dimensional space can make it easier to interpret complex
structures.


\section{Multidimensional scaling}

%Note: Most of this section is excerpted, with only slight
%modifications, from \cite{ggvisjcgs}

\index{multidimensional scaling (MDS)}
% motivation for MDS first
Multidimensional scaling (MDS) is a family of methods for studying the
similarity between objects, in particular for finding a low-dimensional
representation where more similar objects are close to each other. For
example, researchers in the social sciences may study the similarity
in the ratings for pairs of stimuli such as tastes, colors, and
sounds. In marketing studies the pairs may be brands or products, such
as soft drinks or breakfast cereals.  MDS can also be used for graph
layout, which was introduced in the preceding section.  The number of
edges in the shortest path between each pair of nodes is used to
derive a set of distances that can be subjected to MDS.

% Technical details next 
% changing to lower case n, instead of N, to be consistent with
% other notation in this book, even though it may not be the
% standard MDS notation.
MDS begins with proximity data, which consists of similarity (or
dissimilarity) information for pairs of objects.  If the objects are
labeled $1, ..., n$, the proximity data is described by dissimilarity
values $D_{i,j}, i,j=1, ..., n$: Pairs of similar objects will have
small values of $D_{i,j}$, and pairs of dissimilar objects will have
larger values. If the data is provided as similarities, it can be
converted to dissimilarities by some monotone decreasing
transformation. The goal of MDS is to map the $n$ objects to points
$x_1, ..., x_n$ in some lower-dimensional space in such a way that the
$D_{i,j}$ are well approximated by the distances $||x_i - x_j||$ in
the reduced dimension, so as to minimize a \Term{stress function}, for
example:

\begin{eqnarray*}
{\rm Stress}_D \left( x_1, ..., x_n \right) = 
  \left( \sum_{ i \neq j = 1...n} \left( D_{i,j} - || x_i - x_j || \right) ^ 2 \right) ^ {1/2}.
\end{eqnarray*}

\noindent The outer square root is just a convenience that gives
greater spread to small values.  The minimization can be carried out
by straightforward gradient descent applied to Stress$_D$, viewed as a
function on $R^{kn}$. This formulation of MDS is from
\citeasnoun{Kr64a} and \citeasnoun{Kr64b}, which refined the original
classical scaling proposed by \citeasnoun{To52}.

%MDS is a little different from most multivariate problems. In many
%multivariate data problems the analyst starts with an $n\times p$ data
%matrix, where the rows correspond to the $n$ objects, or samples, and
%$p$ the variables measured on each. We may calculate an $n\times n$
%interpoint distance matrix, for example to cluster the objects.  This
%could also be analyzed using MDS. Most MDS problems however start at
%this point, with only the interpoint dissimilarities.

% How about this?  dfs

The data MDS is applied to is therefore a bit different than the input
data for most multivariate methods.  In a more typical multivariate
problem, the analyst starts with an $n\times p$ data matrix, where the
rows correspond to the $n$ objects, or samples, and the columns
correspond to the $p$ variables recorded for each object. We may
sometimes calculate an $n\times n$ interpoint distance matrix, for
example, as a step in cluster analysis.  This $n\times n$ distance
matrix is exactly the starting point for MDS.

The distance matrix in Fig.~\ref{grid0} corresponds to the simple
$5\times 5$ grid pictured.  The distances are obviously not Euclidean;
rather, they represent the number of edges in the shortest path
between each pair of nodes.

% This minipage thing is nice, but sensitive to white space, I think.
\begin{figure}[!h]
\centering
\begin{minipage}[c]{0.45\textwidth}
  \centering
  $\blD = \left[ \begin{array}{p{1.2em}p{1.2em}p{1.2em}p{1.2em}p{1em}}
    0 & 1 & 2 & 3 & 4\\
    1 & 0 & 3 & 4 & 5\\
    2 & 3 & 0 & 5 & 6\\
    3 & 4 & 5 & 0 & 7\\
    4 & 5 & 6 & 7 & 0 \end{array} \right]$
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
  \includegraphics[width=\textwidth]{chap-misc/grid0.pdf}
\end{minipage}
\caption[A grid and its distance matrix]{A $5\times 5$ grid and the
corresponding distance matrix, with entries representing the number of
edges in the shortest path between pairs of nodes.}
\label{grid0}
\end{figure}


%\subsection{Types of multidimensional scaling}

Many MDS methods exist, and the stress function is the main source of
difference among them.  Here are two dichotomies that allow us to
structure some possibilities:

\begin{itemize}

\item Kruskal--Shepard distance scaling versus classical
Torgerson--Gower inner-product scaling: Distance scaling is based on
direct fitting of distances to dissimilarities, whereas the older
classical scaling is based on a detour whereby dissimilarities are
converted to a form that is naturally fitted by inner products 
$<xi, xj>$.

\item Metric scaling versus nonmetric scaling: Metric scaling uses the
actual values of the dissimilarities, whereas nonmetric scaling
effectively uses only their ranks (Shepard 1962, Kruskal 1964{\it
a}). Nonmetric MDS is realized by estimating an optimal monotone
transformation $f(D_{i,j})$ of the proximities simultaneously with the
configuration.

\end{itemize}

A conceptual difference between classical and distance scaling is that
inner products rely on an origin, whereas distances do not; a set of
inner products determines uniquely a set of distances, but a set of
distances determines a set of inner products only modulo change of
origin. To avoid arbitrariness, one constrains classical scaling to
configurations with the mean at the origin.

The computational difference between classical and distance scaling is
that the minimization problem for classical scaling can be solved with
a simple eigen-decomposition, whereas distance scaling requires iterative
minimization. 

\begin{figure}[htbp]
\centerline{
  \includegraphics[width=1.4in]{chap-misc/grid-a.pdf}
  \includegraphics[width=1.4in]{chap-misc/grid-b.pdf}
  \includegraphics[width=1.4in]{chap-misc/grid-c.pdf}}
\centerline{
  \includegraphics[width=1.4in]{chap-misc/grid-d.pdf}
  \includegraphics[width=1.4in]{chap-misc/grid-e.pdf}
  \includegraphics[width=1.4in]{chap-misc/grid-f.pdf}}
\centerline{
  \includegraphics[width=1.4in]{chap-misc/grid-g.pdf}
  \includegraphics[width=1.4in]{chap-misc/grid-h.pdf}
  \includegraphics[width=1.4in]{chap-misc/grid-i.pdf}}
\caption[Snapshots from MDS animation]{ Snapshots showing nine stages
of a Stress minimization of a $5\times 5$ grid in three dimensions.}
\label{gridlayout}
\end{figure}


\index{ggvis} Incorporating MDS into dynamic and interactive data
visualization software offers significant advantages over using it in
a static context.  In addition to all the tools described in earlier
chapters, such as touring and brushing, some advantages are specific
to the embedding of an iterative algorithm in a dynamic system. A tool
in GGobi, ggvis, supports interactive MDS.  It is a direct descendant
of XGvis \cite{LSDB92,xgvisjclass} and has much in common with
another earlier system, ViSta--MDS \cite{MY94}. These systems provide
two important interactive capabilities:

\index{tour}
\index{brushing!linked}
\index{animation}

\begin{enumerate}
\item Animated optimization: The configuration points are displayed
continuously as they are subjected to MDS optimization.  At the same
time, the values of the cost function are also shown in a trace plot.
\item Manual dragging: Configuration points can be moved
interactively with mouse dragging.
\end{enumerate}

Figure~\ref{gridlayout} shows nine ``snapshots'' taken during an MDS
animation.  We performed a Stress minimization in three dimensions to
the $5\times 5$ grid.  This provides a flavor of the use of MDS in
graph layout, because this simple grid is actually a highly regular
graph, with 25 nodes and 40 edges.  The final pictures show some
curvature, which is a result of the non-Euclidean distances.

\begin{figure}[htbp]
\centerline{
  \includegraphics[width=2.0in]{chap-misc/morse-MDS1b.pdf}
  \includegraphics[width=2.0in]{chap-misc/morse-MDS2b.pdf}
}
\smallskip
\centerline{
  \includegraphics[width=2.0in]{chap-misc/morse-MDS3b.pdf}
  \includegraphics[width=2.0in]{chap-misc/morse-MDS4b.pdf}
}
\caption[Applying MDS to the \Data{Morse Code} confusion data]
{Applying Kruskal--Shepard metric MDS scaling to the Morse Code
confusion data: {\bf (top left)} initial configuration, {\bf (bottom
right)} final configuration.  Generally, codes of the same length are
close to each other, as are codes with the same fraction of dots.  The
length of codes roughly increases from left to right, and the fraction
of dots decreases from bottom to top.  }
\label{morsecodes}
\end{figure}

\index{datasets!\Data{Morse Code Confusion Rates}} For an example of
applying MDS to data, we turn to the well-known Rothkopf \Data{Morse
Code Confusion Rates} data \cite{Ro1957}, which is to MDS what
Fisher's Iris data is to discriminant analysis. Morse codes represent
each letter and digit with a sequence of between two and five
characters, a combination of dots and dashes.  This data originated in
an experiment where inexperienced subjects were exposed to pairs of
Morse codes in rapid order. The subjects had to decide whether the two
codes in a pair were identical. The results were summarized in a table
of confusion rates.

Confusion rates are similarity measures: Codes that are often confused
are interpreted as similar or close. Similarities, $S_{i,j}, i,j=1,
..., n$ were converted to dissimilarities using

\begin{eqnarray*}
D_{i,j} ^ 2 = S_{i,i} + S_{j,j} - 2S_{i,j}
\end{eqnarray*}

\noindent This function also symmetrizes the dissimilarities, 
$D_{i,j}=D_{j,i}$, which is needed by MDS.


Applying metric Kruskal--Shepard distance scaling in $k$ = 2 dimensions
to the Morse code dissimilarities produced the configuration shown in
Fig.~\ref{morsecodes}.  Generally, codes of the same length are close
to each other, as are codes with the same fraction of dots.  The
length of codes roughly increases from left to right, and the fraction
of dots decreases from bottom to top. [These observations agree with
the many published accounts \cite{Sh62,KW78,BG05}.] The striping or
banding of points in the configuration corresponds to strips of codes
of the same length but different combinations of dots and
dashes. Drawing edges to connect codes of the same length makes this
pattern clearer (Fig.~\ref{morsecodes2}).

\begin{figure}[htbp]
\centerline{
  \includegraphics[width=2.0in]{chap-misc/morse-MDS5b.pdf}
  \includegraphics[width=2.0in]{chap-misc/morse-MDS6b.pdf}
}
\caption[Studying MDS results]
{Studying the results of MDS. Edges connecting code of the same length
are added, making it easier to see strips in the layout corresponding
to the codes of the same length. At right are the results of
non-metric MDS.  }
\label{morsecodes2}
\end{figure}

Choosing a different scaling metric, or different parameters, will
lead to a different layout. The right plot in Fig.~\ref{morsecodes2}
shows the result for Kruskal--Shepard non-metric MDS. Many other
parameters to the algorithm can be changed, and these as with the
choice of the initial configuration will affect the final result.

MDS can also create configurations in dimensions other than 2. For the
\Data{Morse code} data, a layout in 3D is revealing: Points are
arranged approximately on a sphere. Codes of length 1 are close to
each other but far from other length codes, and the same can be said
for codes of length 2. With identification we can also learn that
points corresponding to the letters K (-.-) and D (-..) are closer to
some of the length four codes, such as the letter L (.-..), than to
other length 3 codes.

\begin{figure}[htbp]
\centerline{
  \includegraphics[width=2.0in]{chap-misc/morse-MDS3D2.pdf}
  \includegraphics[width=2.0in]{chap-misc/morse-MDS3D3.pdf}
}
\caption[3D MDS layouts of the \Data{Morse Code}]
{Layout of the \Data{Morse Code} in 3D is revealing. The points lie
approximately on a sphere.}
\label{morsecodes3}
\end{figure}

% Needs a summary paragraph

More information about MDS metrics and the ggvis parameters is
available in \citeasnoun{ggvisjcgs}, from which this section has
largely been excerpted.

% Does this need a recap for the chapter, reminding people to 
% look on the web for the full chapters?

% Exercises?
\section*{Exercises}
\addcontentsline{toc}{section}{Exercises}

\begin{enumerate}
\item
For the \Data{Music} data, permute the \Vbl{Type} and search for the best
separation. Is the separation as good as the separation for the true
class variable?
\item Subset the \Data{Australian crabs} data, choosing only the blue
males. Compute the mean and variance--covariance of this subset, and
use this information to generate a sample from a multivariate normal
distribution having the same population mean and variance.  Compare
this simulated data with the actual data. Do you think the blue male
crabs subset is consistent with a sample from a multivariate normal?
Justify your opinion.
\item For the \Data{Wages} data:
\begin{enumerate}
\item Identify a person whose wages steadily increase with experience.
\item Identify a person whose wages are very volatile throughout their 
experience.
\item Compute the maximum minus minimum wage for each person, and use this to 
plot the profiles of 10 people having the widest range in wages.
\item Compute the correlation between \Vbl{uerate} and \Vbl{lnw} for each 
person.  Use this to find the person with the strongest relationship between
unemployment rate and wages. Plot the profiles of \Vbl{lnw} and
\Vbl{uerate} against \Vbl{exper}.
\end{enumerate}

\item For the \Data{Personal Social Network} data:
\begin{enumerate}
\item Using Kruskal--Shepard metric MDS, lay out the social network in
3D, using the default settings.  While MDS is running, play with the
sliders for \Button{Data power} and \Button{Weight}, and note the
changing impact of short and long distances.  With MDS still running,
activate GGobi's \Button{Move points} Interaction mode, and experiment
with grabbing points and moving them around the screen.
\begin{enumerate}
\item Who is at the center of the network? %Abraham
\item Name two other people who connect the center person with at least 
5 other people. %Katherine, Oberon, Holden, Norman, Caitlin, Banquo...
\item Name the two people connected to Nikki but to nobody else. %Vanessa
% and Gabriel
% I don't know what this means.
%\item Name someone who is not in a group but is connected with 
% people from two groups? % Yvette, Rachel, 
\item Is there a relationship between \Vbl{maritalstat} and 
position in the network?
% Looks like green card holders a more central, slightly than eirht oe the other two.
\item Does the central person work part time or full time? % part time!
\end{enumerate}
\item Choose a radial layout.  Open a linked 1D ASH of
\Vbl{log10(interactions)}, and find the pairs of people who have the
greatest number of interactions.  What do you find?  
% Most are linked to Caitlin

\end{enumerate}
\item For the \Data{Music} data:
\begin{enumerate}
\item Use this code to get the dissimilarity matrix into GGobi, and set
it up for MDS:

\begin{verbatim}
> library(rggobi)
> d.music <- read.csv("music-sub.csv", row.names=1)
> f.std.data <- function(x){(x-mean(x))/sd(x)}
> x <- apply(d.music[,3:7],2,f.std.data)
> d.music.cor <- cor(t(x))
> d.music.dist <- 1-abs(d.music.cor)
> gg <- ggobi(d.music)
> gg["distance"] <- as.vector(d.music.dist)
> names <- unlist(dimnames(d.music)[1])
> tmp <- matrix(c(rep(names,each=62),rep(names,62)), 
    ncol=2)
> edges(gg["distance"]) <- tmp
\end{verbatim}

\item Run the Kruskal--Shepard metric MDS in 2D.  Describe what you
see. You may find it useful to brush the Rock, Classical, and New Wave
tracks different colors or to add labels to the plot.
\item How does randomizing the initial configuration affect the final
configuration?
\item Explore the effect of changing the scaling measure.
\item Is a 3D configuration better than a 2D configuration?
\end{enumerate}

\item Characterize the pattern of the labels in the $n$ = 4 adjacent
transposition graph.  Lay out the $n$ = 5 adjacent transposition graph
and describe it.
\end{enumerate}

