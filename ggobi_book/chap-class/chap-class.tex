%\setcounter{chapter}{3}

\chapter{Supervised Classification}~\label{chap:class}

% Inserted four newpage commands to make sure the R code breaks
% are reasonable; removed one old newpage command.  dfs

%\section{Background}

% What is supervised classification

% What are the major references: HTF, VR, Ri

% Types of algorithms: different inputs, assumptions, nuisance variables

% Examining output: predictions, boundaries, residuals, variable importance

% Ensemble methods

When you browse your email, you can usually tell right away whether a
message is spam.  Still, you probably do not enjoy spending your
time identifying spam and have come to rely on a filter to do that
task for you, either deleting the spam automatically or filing it in
a different mailbox.  An email filter is based on a set of rules
applied to each incoming message, tagging it as spam or ``ham'' (not
spam). Such a filter is an example of a supervised classification
algorithm. It is formulated by studying a training sample of email
messages that have been manually classified as spam or
ham. Information in the header and text of each message is converted
into a set of numerical variables such as the size of the email, the
domain of the sender, or the presence of the word ``free.'' These
variables are used to define rules that determine whether an incoming
message is spam or ham.

An effective email filter must successfully identify most of the spam
without losing legitimate email messages: That is, it needs to be an
accurate classification algorithm.  The filter must also be efficient
so that it does not become a bottleneck in the delivery of mail.
Knowing which variables in the training set are useful and using only
these helps to relieve the filter of superfluous computations.

Supervised classification forms the core of what we have recently come
to call \Term{data mining}. The methods originated in statistics in
the early nineteenth century, under the moniker \Term{discriminant
analysis}. An increase in the number and size of databases in the late
twentieth century has inspired a growing desire to extract knowledge
from data, which has contributed to a recent burst of research on new
methods, especially on algorithms.

\index{classification!supervised}
There are now a multitude of ways to build classification rules, each
with some common elements. A training sample contains data with known
categorical response values for each recorded combination of
explanatory variables. The training sample is used to build the rules
to predict the response. Accuracy, or inversely error, of the
classifier for future data is also estimated from the training sample.
Accuracy is of primary importance, but there are many other
interesting aspects of supervised classification applications beyond
this:

\begin{itemize}
\item Are the classes well separated in the data space, so that they
correspond to distinct clusters?  If so, what are the shapes of the
clusters?  Is each cluster sufficiently ellipsoidal so that we can
assume that the data arises from a mixture of multivariate normal
distributions?  Do the clusters exhibit characteristics that suggest
one algorithm in preference to others?

\item Where does the boundary between classes fall?  Are the classes
linearly separable, or does the difference between classes suggest
a non-linear boundary?  How do changes in the input parameters affect
these boundaries? How do the boundaries generated by different methods
vary?

\item What cases are misclassified, or have more uncertain
predictions?  Are there places in the data space where predictions are
especially good or bad?

\item Is it possible to reduce the set of explanatory variables?
\end{itemize}

This chapter discusses the use of interactive and dynamic graphics to
investigate these different aspects of classification problems. It is
structured as follows: Sect.~\ref{class-bg} gives a brief background
of the major approaches, Sect.~\ref{class-plots} describes graphics
for viewing the classes, and Sect.~\ref{class-num} adds descriptions
of several different numerical methods and graphics to assess the
models they generate.  A good companion to this chapter is the
material presented in \citeasnoun{VR02}, which provides data and code
for practical examples of supervised classification using R. For the
most part, methods described here are well documented in the
literature. Consequently, our descriptions are brief, and we provide
references to more in-depth explanations.  We have gone into more
detail at the beginning of the chapter, to set the tone for the rest of
the chapter, and in the section on support vector machines at the end
of the chapter, because we find the descriptions in the literature to
be unsatisfactory.

\section{Background}~\label{class-bg}

Supervised classification arises when there is a categorical response
variable (the output) $Y_{n\times 1}$ and multiple explanatory
variables (the input) $\blX_{n\times p}$, where $n$ is the number of
cases in the data and $p$ is the number of variables. Because $Y$ is
categorical, the user may represent it using character strings that
the software will recode as integers.  A binary variable may be
converted from $\{Male, Female\}$ or $\{T, F \}$ to $\{1, 0\}$ or
$\{-1, 1\}$, whereas multiple classes may be recoded using the values
$\{1, \dots, g\}$.  Coding of the response really matters and can
alter the formulation or operation of a classifier.

Since supervised classification is used in several disciplines, the
terminology used to describe the elements can vary widely.  The
attributes may also be called features, independent variables, or
explanatory variables.  The instances may be called cases, rows, or
records.

\subsection{Classical multivariate statistics}\label{CMS}

\index{classification methods!linear discriminant analysis (LDA)}

Discriminant analysis dates to the early 1900s. Fisher's linear
discriminant \cite{Fi36} determines a linear combination of the
variables that separates two classes by comparing the differences
between class means with the variance of values within each class. It
makes no assumptions about the distribution of the data. Linear
discriminant analysis (LDA), as proposed by \citeasnoun{Rao48},
formalizes Fisher's approach by imposing the assumption that the data
values for each class arise from a $p$-dimensional multivariate normal
distribution, which shares a common variance--covariance matrix with data
from other classes. Under this assumption, Fisher's linear
discriminant gives the optimal separation between the two groups.

For two equally weighted groups, where $Y$ is coded as $\{0, 1\}$, the LDA rule is:

\begin{quote}
{\em Allocate a new observation $\blX_0$ to group 1 if}

\[
(\bar{\blX}_1-\bar{\blX}_2)'\blS^{-1}_{\rm pooled}\blX_0 \geq 
  \frac{1}{2}(\bar{\blX}_1-\bar{\blX}_2)'\blS^{-1}_{\rm pooled}
  (\bar{\blX}_1+\bar{\blX}_2) 
\]

{\em else allocate it to group 2, }
\end{quote}

\noindent where $\bar{\blX}_k $ are the class mean vectors of an 
$n\times p$ data matrix $\blX_k ~~(k=1,2)$, 

\[
\blS_{\rm pooled} = \frac{(n_1-1)
\blS_1}{(n_1-1)+(n_2-1)} + \frac{(n_2-1) \blS_2}{(n_1-1)+(n_2-1)}
\]

\noindent is the pooled variance--covariance matrix, and

\[ 
\blS_k = \frac{1}{n-1}\sum_{i=1}^{n}
(\blX_{ki}-\bar{\blX}_k)(\blX_{ki}-\bar{\blX}_k)', ~~k=1,2
\]

\noindent is the class variance--covariance matrix. The linear
discriminant part of this rule is
$(\bar{\blX}_1-\bar{\blX}_2)'\blS^{-1}_{\rm pooled}$, which defines
the linear combination of variables that best separates the two
groups.  To define a classification rule, we compute the value of the
new observation $\blX_0$ on this line and compare it with the value of
the average of the two class means $(\bar{\blX}_1+\bar{\blX}_2)/2$ on
the same line.
%Computing the value of the new observation $\blX_0$ on this
%line and comparing it with the value of the average of the two class
%means $(\bar{\blX}_1+\bar{\blX}_2)/2$ on this line gives the
%classification rule.

For multiple $(g)$ classes, the rule and the discriminant space are
constructed using the between-group sum-of-squares matrix,

\[
\blB =
\sum_{k=1}^g n_k(\bar{\blX}_k-\bar{\blX})(\bar{\blX}_k-\bar{\blX})' 
\]

\noindent which measures the differences between the class means, 
compared with the overall data mean $\bar{\blX}$ and the within-group
sum-of-squares matrix,

\[
\blW =
\sum_{k=1}^g\sum_{i=1}^{n_k}
(\blX_{ki}-\bar{\blX}_k)(\blX_{ki}-\bar{\blX}_k)'
\]

\noindent which measures the variation of values around each class mean.
The linear discriminant space is generated by computing the
eigenvectors (canonical coordinates) of $\blW^{-1}\blB$, and this is
the space where the group means are most separated with respect to the
pooled variance--covariance. The resulting classification rule is to
allocate a new observation to the class with the highest value of

\begin{eqnarray}
\bar{\blX}_k'\blS^{-1}_{\rm pooled}\blX_0 - 
\frac{1}{2}\bar{\blX}_k'\blS^{-1}_{\rm pooled}\bar{\blX}_k ~~~k=1,...,g \label{lda-rule}
\end{eqnarray}

\noindent which results in allocating the new observation into the
class with the closest mean.

This LDA approach is widely applicable, but it is useful
to check the underlying assumptions on which it depends: (1)
that the cluster structure corresponding to each class forms an
ellipse, showing that the class is consistent with a sample from a
multivariate normal distribution, and (2) that the variance of values
around each mean is nearly the same. Figure~\ref{lda-assumptions}
illustrates two datasets, of which only one is consistent with these
assumptions. Other parametric models, such as quadratic discriminant
analysis or logistic regression, also depend on assumptions
about the data which should be validated.  \index{classification
methods!quadratic discriminant analysis (QDA)} \index{classification
methods!logistic regression}

% Figure 1
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=5in]{chap-class/lda-assumptions.pdf}}}

\index{datasets!\Data{Flea Beetles}}
\index{datasets!\Data{Italian Olive Oils}}

\caption[Evaluating model assumptions for 2D data]{Evaluating model
assumptions by comparing scatterplots of raw data with bivariate
normal variance--covariance ellipses.  For the \Data{Flea Beetles} {\bf
(top row)}, each of the three classes in the raw data {\bf (left)}
appears consistent with a sample from a bivariate normal distribution
with equal variance--covariance.  For the \Data{Olive Oils}, the
clusters are not elliptical, and the variance differs from cluster to
cluster.}
\label{lda-assumptions}
\end{figure*}

Our description is derived from \citeasnoun{VR02} and
\citeasnoun{Ri96}. A good general treatment of parametric methods for
supervised classification can be found in \citeasnoun{JW02} or another
similar multivariate analysis textbook. Missing from multivariate
textbooks is a good explanation of the use of interactive graphics
both to check the assumptions underlying the methods and to explore
the results. This chapter fills this gap.

\index{data mining}
\subsection{Data mining}

Algorithmic methods have overtaken parametric methods in the practice
of supervised classification.  A parametric method such as linear
discriminant analysis yields a set of interpretable output parameters, so
it leaves a clear trail helping us to understand what was done to
produce the results.  An algorithmic method, on the other hand, is
more or less a black box, with various input parameters that are
adjusted to tune the algorithm.  The algorithm's input and output
parameters do not always correspond in any obvious way to the
interpretation of the results.  All the same, these methods can be
very powerful and their use is not limited by requirements about
variable distributions as is the case with parametric methods.

%Algorithmic methods have advanced intensively to overtake parametric
%methods in the general practice of supervised classification. An
%algorithmic method is a black box, with several to many input
%parameters that are adjusted as the data is entered, and some
%outputs are cranked out. Some boxes are more opaque than others.

\index{classification methods!tree}

The tree algorithm \cite{BFOS84} is a widely used algorithmic method.
The tree algorithm generates a classification rule by sequentially
splitting the data into two buckets. Splits are made between sorted
data values of individual variables, with the goal of obtaining pure
classes on each side of the split. The inputs for a simple tree
classifier commonly include (1) an impurity measure, an indication of
the relative diversity among the cases in the terminal nodes; (2) a
parameter that sets the minimum number of cases in a node, or the
minimum number of observations in a terminal node of the tree; and (3)
a complexity measure that controls the growth of a tree, balancing the
use of a simple generalizable tree against a more accurate tree
tailored to the sample.  When applying tree methods, exploring the
effects of the input parameters on the tree is instructive; for
example, it helps us to assess the stability of the tree model.

Although algorithmic models do not depend on distributional assumptions,
that does not mean that every algorithm is suitable for all data.  For
example, the tree model works best when all variables are independent
within each class, because it does not take such dependencies into
account.  As always, visualization can help us to determine whether a
particular model should be applied.  In classification problems, it is
useful to explore the cluster structure, comparing the clusters with
the classes and looking for evidence of correlation within each class.
The upper left-hand plot in Fig.~\ref{lda-assumptions} shows a strong
correlation between \Vbl{tars1} and \Vbl{tars2} within each cluster,
which indicates that the tree model may not give good results for the
\Data{Flea Beetles}.  The plots in Fig.~\ref{misclassifications}
provide added evidence.  They use background color to display the
class predictions for LDA and a tree. The LDA boundaries, which are
formed from a linear combination of \Vbl{tars1} and \Vbl{tars2}, look
more appropriate than the rectangular boundaries of the tree
classifier.

% Figure 2
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=5in]{chap-class/missclassifications.pdf}}}
\caption[Classification of the data space for the \Data{Flea
Beetles}]{Classification of the data space for the \Data{Flea
Beetles}, as determined by LDA {\bf (left)} and a tree model {\bf
(right)}. Misclassified cases are highlighted.}
\label{misclassifications}
\end{figure*}

\citeasnoun{HTF01} and \citeasnoun{Bi06} include thorough discussions of
algorithms for supervised classification presented from a modeling
perspective with a theoretical emphasis. \citeasnoun{Ri96} is an early
volume describing and illustrating both classical statistical methods
and algorithms for supervised classification. All three books contain
some excellent examples of the use of graphics to examine
two-dimensional (2D) boundaries generated by different
classifiers. The discussions in these and other writings on data
mining algorithms take a less exploratory approach than that of this
chapter, and they lack treatments of the use of graphics to examine
the high-dimensional spaces in which the classifiers operate.

\subsection{Studying the fit}

\index{classification!error}
\index{classification!misclassification table}

A classifier's performance is usually assessed using its error or,
conversely, its accuracy. Error is calculated by comparing the
predicted class with the known true class, using a misclassification
table. For example, below are the respective misclassification tables
for LDA and the tree classifier applied to the \Data{Flea Beetles}:

\bigskip
\begin{center}
\begin{tabular}{l@{\hspace{.15in}}r@{\hspace{.15in}}r@{\hspace{.08in}}r@{\hspace{.08in}}r@{\hspace{.3in}}rp{0.4in}
                l@{\hspace{.15in}}r@{\hspace{.15in}}r@{\hspace{.08in}}r@{\hspace{.08in}}r@{\hspace{.3in}}r} 

\multicolumn{6}{l}{LDA} & & \multicolumn{6}{l}{Tree} \\
\cline{1-6}\cline{8-13}\\

 & & \multicolumn{3}{c}{Predicted} & \multicolumn{1}{c}{Error}& &
 & & \multicolumn{3}{c}{Predicted} & \multicolumn{1}{c}{Error}\\

 & & \multicolumn{3}{c}{Class} & \multicolumn{1}{c}{}& &
 & & \multicolumn{3}{c}{Class} & \multicolumn{1}{c}{}\\

 & & 1 & 2 & 3 &  & & 
 & \B & 1 & 2 & 3 &   \\ \cline{3-6} \cline{10-13}

\T & 1 & 20 & 0 & {\bf 1} & 0.048 & & 
   & 1 & 19 & 0 & {\bf 2} & 0.095\\

Class & 2 & 0 & 22 & 0 & 0.000 & & 
Class & 2 & 0 & 22 & 0 & 0.000\\

 & 3  & {\bf 3} & 0 & 28   & 0.097 & & 
 & 3  & {\bf 3} & 0 & 28   & 0.097\\ \cline{3-6} \cline{10-13}
 & \T &         &   &      & 0.054 & &
 &    &         &   &      & 0.068\\
\end{tabular}
\end{center}
\bigskip

\noindent The total error is the number of misclassified samples
divided by the total number of cases: $4/74=0.054$ for LDA and
$5/74=0.068$ for the tree classifier. 

It is informative to study the misclassified cases and to see which
pockets of the data space contain more error. The misclassified cases
for LDA and tree classifiers are highlighted (large orange $\times$es
and large green circles) in Fig.~\ref{misclassifications}. Some errors
made by the tree classifier, such as the uppermost large green circle,
seem especially egregious.  As noted earlier, they result from the
limitations of the algorithm when variables are correlated.

\index{cross-validation}

To be useful, the error estimate should predict the performance of the
classifier on new samples not yet seen.  However, if the error is
calculated using the same data that was used by the classifier, it is
likely to be too low.  Many methods are used to avoid
double-dipping from the data, including several types of
\Term{cross-validation}.  A simple example of cross-validation is to
split the data into a training sample (used by the classifier) and a
test sample (used for calculating error).

\index{ensemble method}
\index{classification methods!random forest}

Ensemble methods build cross-validation into the error
calculations. Ensembles are constructed by using multiple classifiers
and by pooling the predictions using a voting scheme.  A random forest
\cite{Br01,Cu04}, for example, builds in cross-validation by
constructing multiple trees, each of which is generated by randomly
sampling the input variables and the cases.  Because each tree is
built using a sample of the cases, there is in effect a training
sample and a test sample for each tree.  (See
Sect.~\ref{random-forests} for more detail.)

%\subsection{Further Exploration with Interactive and Dynamic Graphics}
%\subsection{Recapping}

% This section needs to make the connection between investigative needs and 
% how interactive graphics helps

%In Figs.~\ref{lda-assumptions} and \ref{misclassifications} static
%graphics were used to examine assumptions, boundaries and
%misclassifications for 2D problems. Data is $p$-D. The rest of the
%chapter describes how to the pictures of higher dimensions using
%interactive and dynamic graphics.

%\begin{itemize}
%\item To investigate the shape of clusters, differences between clusters, and 
%boundaries created by different classifiers we look at projections of
%$p$-D with tours.
%\item Brushing plots and linking between plots is used to explore misclassifications.
%\end{itemize}

\section{Purely graphics: getting a picture of the class structure}~\label{class-plots}

To visualize the class structure, we start by coding the response
variable $Y$ using color and symbol to represent class, and then
we explore a variety of plots of the explanatory variables $\blX$.  Our
objective is to learn how distinctions between classes arise.  If we
are lucky, we will find views in which there are gaps between clusters
corresponding to the classes. A gap indicates a well-defined
distinction between classes and suggests that there will be less
error in predicting future samples.  We will also study the shape of
the clusters.

If the number of classes is large, keep in mind that it is difficult
to digest information from plots having more than three or four colors
or symbols.  You may be able to simplify the displays by grouping
classes into a smaller set of ``super-classes.''  Alternatively, you can
partition the data, looking at a few classes at a time.

If the number of dimensions is large, it takes much longer to get a
sense of the data, and it is easy to get lost in high-dimensional
plots. There are many possible low-dimensional plots to examine, and
that is the place to start.  Explore plots one or two variables at a
time before building up to multivariate plots.

\subsection{Overview of \Data{Italian Olive Oils}}

The \Data{Olive Oils} data has eight explanatory variables (levels of
fatty acids in the oils) and nine classes (areas of Italy).  The goal
of the analysis is to develop rules that reliably distinguish oils
from the nine different areas.  It is a problem of practical interest,
because oil from some areas is more highly valued and unscrupulous
suppliers sometimes make false claims about the origin of their oil.
The content of the oils is a subject of study in its own right: Olive
oil has high nutritional value, and some of its constituent fatty
acids are considered to be more beneficial than others.

In addition, this is a good dataset for students of supervised
classification because it contains a mix of straightforward
separations, difficult separations, and unexpected finds.

% Figure 3
\begin{figure*}[htbp]
\centerline{ 
 {\includegraphics[width=2.2in]{chap-class/olive-1da.pdf}}
 {\includegraphics[width=2.2in]{chap-class/olive-1db.pdf}}}
\smallskip
\centerline{ 
 {\includegraphics[width=2.2in]{chap-class/olive-1dc.pdf}}
 {\includegraphics[width=2.2in]{chap-class/olive-1dd.pdf}}}
\caption[Differentiating the oils from the three regions in the
\Data{Olive Oils} data in univariate plots]{Differentiating the oils
from the three regions in \Data{Olive Oils} in univariate plots.  
{\bf (top row)} \Vbl{eicosenoic} separates Southern oils (in
orange $\times$es and $+$es) from the others, as shown in both an ASH
and a textured dot plot.  {\bf (bottom row)} In plots where Southern
oils have been removed, we see that Northern (purple circles) and
Sardinian (green rectangles) oils are separated by \Vbl{linoleic},
although there is no gap between the two clusters.}
\label{olive-1d}
\end{figure*}

As suggested above, we do not start by trying to visualize or classify
the oils by \Vbl{area}, because nine groups are too many.  Instead, we divide
the classification job into a two-stage process.  We start by grouping
the nine \Vbl{areas} into three ``super-classes'' corresponding to
a division of Italy into South, North, and Sardinia, and we call this
new variable \Vbl{region}.  In the first stage, we classify the oils
by \Vbl{region} into three groups.  In the second stage, we work with
the oils from one region at a time, building classifiers to predict
\Vbl{area} within \Vbl{region}.

\subsection{Building classifiers to predict region}

\index{average shifted histogram (ASH)}
\noindent{\em Univariate plots:} We first paint the points according
to \Vbl{region}.  Using univariate plots, we look at each explanatory
variable in turn, looking for separations between pairs of
regions. This table describes the correspondence between \Vbl{region}
and symbol for the next few figures:

\begin{center}
\begin{tabular}{l@{\hspace{.2in}}l} %\hline
\T \B \Vbl{region} & Symbol \\\hline
\T South    &  orange $+$ and $\times$ \\
   Sardinia  &   green rectangle \\
\B North  &    purple circle \\ \hline
\end{tabular}
\end{center}

We can cleanly separate the oils of the South from those of the other
regions using just one variable, \Vbl{eicosenoic}
(Fig.~\ref{olive-1d}, top row).  Both of these univariate plots show
that the oils from the other two regions contain no eicosenoic acid.

In order to differentiate the oils from the North and Sardinia, we
remove the Southern oils from view and continue plotting one variable
at a time (Fig.~\ref{olive-1d}, bottom row).  Several
variables show differences between the oils of the two regions,
and we have plotted two of them: \Vbl{oleic} and \Vbl{linoleic}.  Oils
from Sardinia contain lower amounts of oleic acid and higher amounts
of linoleic acid than oils from the north.  The two regions are
perfectly separated by \Vbl{linoleic}, but since there is no gap
between the two groups of points, we will keep looking.

\bigskip
\noindent{\em Bivariate plots:} If one variable is not enough to
distinguish Northern oils from Sardinian oils, perhaps we can find a
pair of variables that will do the job.  Starting with \Vbl{oleic} and
\Vbl{linoleic}, which were so promising when taken singly, we look at
pairwise scatterplots (Fig.~\ref{olive-2d}, left and middle).
Unfortunately, the combination of \Vbl{oleic} and \Vbl{linoleic} is no
more powerful than each one was alone.  They are strongly negatively
associated, and there is still no gap between the two groups.

We explore other pairs of variables.  Something interesting emerges
from a plot of \Vbl{arachidic} and \Vbl{linoleic}: There is big gap
between the points of the two regions!  \Vbl{Arachidic} alone seems to
have no power to separate, but it improves the power of
\Vbl{linoleic}.  Since the gap between the two groups follows a
non-linear, almost quadratic path, we must do a bit more work to
define a functional boundary.

% Figure 4
\begin{figure*}[htbp]
\centerline{
 {\includegraphics[width=1.5in]{chap-class/olive-2da.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-2db.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-t1da.pdf}}}
\caption[Separation between the Northern and Sardinian
oils]{Separation between the Northern (purple circles) and Sardinian
(green squares) oils.  Two bivariate scatterplots {\bf (left)} and a
linear combination of \Vbl{linoleic} and \Vbl{arachidic} viewed in a
1D tour {\bf (right)}.}
\label{olive-2d}
\end{figure*}

We move on, using the 1D tour \index{tour!grand} to look for a linear
combination of \Vbl{linoleic} and \Vbl{arachidic} that will show a
clear gap between the Northern and Sardinian oils, and we find one
(Fig.~\ref{olive-2d}, right).  The linear combination is composed
mostly of \Vbl{linoleic} with a small contribution from
\Vbl{arachidic}. (The numbers generating this projection, recorded
from the tour coefficients, are $\frac{0.969}{1022}\times
\mbox{\Vbl{linoleic}} + \frac{0.245}{105}\times \mbox{\Vbl{arachidic}}$. 
The numerator is the projection coefficient, and the denominator is the
range of the variable, which was used to scale the measurements into
the plot.)  We usually think of this as a multivariate plot, but here
we were successful using a linear combination of only two variables.

\bigskip
\index{parallel coordinate plot}
\noindent{\em Multivariate plots:} A parallel coordinate plot can also
be used to select important variables for
classification. Figure~\ref{olive-parallel} shows a parallel coordinate
plot for the \Data{Olive Oils} data, where the three colors represent
the three large regions. As we found earlier, \Vbl{eicosenoic} is
useful for separating Southern oils (orange, the color drawn first)
from the others.  In addition, Southern oils have higher values on
\Vbl{palmitic} and \Vbl{palmitoleic} and low values on \Vbl{oleic}.
Northern oils have high values of \Vbl{oleic} and low values of
\Vbl{linoleic} relative to Sardinian oils.

Parallel coordinate plots are not as good as tours for visualizing the
shape of the clusters corresponding to classes and the shape of the
boundaries between them, but they are attractive because they can hold
so many variables at once and still be clearly labeled.

% Figure 5
\begin{figure}[htp]
\centerline{\includegraphics[width=4.8in]{chap-class/olive-parallel.pdf}}
\caption[Finding variables for classification using a parallel
coordinate plot of \Data{Olive Oils}]{Finding variables for
classification using a parallel coordinate plot of \Data{Olive Oils}.
Color represents \Vbl{region}, with South in orange (the color that is
drawn first), North in purple (the color that is drawn last), and
Sardinia in green. \Vbl{Eicosenoic}, and to some extent
\Vbl{palmitic}, \Vbl{palmitoleic} and \Vbl{oleic} distinguish Southern
oils from the others. \Vbl{Oleic} and \Vbl{linoleic} distinguish
Northern from Sardinian oils.}
\label{olive-parallel}
\end{figure}

\subsection{Separating the oils by area within each region}

It is now clear that the oils from the three large regions can be
distinguished by their fatty acid composition.  For the second stage
of the classification task, we explore one \Vbl{region} at a time,
looking for separations among the oils of each \Vbl{area}.  We plan to
separate them visually just as we did in the preceding section, by
starting with univariate plots and adding dimensions using bivariate
and multivariate plots if necessary.

\bigskip
\noindent{\em Northern Italy:} The \Vbl{region} ``North'' was created
by aggregating three \Vbl{areas}: Umbria, East Liguria, and West
Liguria.  This table describes the correspondence between \Vbl{area}
and symbol for the next figure:

\begin{center}
\begin{tabular}{l@{\hspace{.2in}}l}
 \B \Vbl{area} & Symbol \\\hline
\T Umbria    &  pink open circles \\
  East Liguria  &    purple solid circles \\
  West Liguria  &    blue solid circles \\ \hline
\end{tabular}
\end{center}

\noindent The univariate plots show no clear separations of oils by
\Vbl{area}, although several variables are correlated with \Vbl{area}.
For example, oils from West Liguria have higher linoleic acid content
than those from other two areas (Fig.~\ref{olive-nth}, top
left). The bivariate plots, too, fail to show clear separations by
\Vbl{area}, but two variables, \Vbl{stearic} and \Vbl{linoleic}, look
useful (Fig.~\ref{olive-nth}, top right). Oils from West Liguria
have the highest linoleic and stearic acid content, and oils from
Umbria have the lowest linoleic and stearic acid content.

% Figure 6
\begin{figure*}[htb]
\centerline{
  {\includegraphics[width=2.2in]{chap-class/olive-nth-1d.pdf}}
  {\includegraphics[width=2.2in]{chap-class/olive-nth-2d.pdf}}
}
\smallskip
\centerline{
  {\includegraphics[width=2.2in]{chap-class/olive-nth-t1d.pdf}}
  {\includegraphics[width=2.2in]{chap-class/olive-nth-t2d.pdf}}
}
\caption[Separating the oils of the Northern \Vbl{region} by
\Vbl{area}]{Separating the oils of the Northern \Vbl{region} by
\Vbl{area}.  The 1D ASH {\bf (top left)} shows that oils from West
Liguria (the blue cluster at the right of the plot) have a higher
percentage of \Vbl{linoleic}.  Looking for other variables, we see in
the bivariate scatterplot {\bf (top right)} that \Vbl{stearic} and
\Vbl{linoleic} almost separate the three areas.  The 1D and 2D tour
plots {\bf bottom row} show that linear combinations of
\Vbl{palmitoleic}, \Vbl{stearic}, \Vbl{linoleic}, and \Vbl{arachidic}
are useful for classification.  }
\label{olive-nth}
\end{figure*}

\index{tour!projection pursuit guided}
Starting with these two variables, we explore linear combinations
using a 1D tour, looking for other variables that further separate the
oils by \Vbl{area}.  We used projection pursuit guidance, with the LDA
index, to find the linear combination shown in Fig.~\ref{olive-nth}
(bottom left). West Liguria is almost separable from the other two
areas using a combination of \Vbl{palmitoleic}, \Vbl{stearic},
\Vbl{linoleic}, and \Vbl{arachidic}.

At this point, we could proceed in two different directions.  We could
remove the points corresponding to West Ligurian oils and look for
differences between the other two areas of the Northern \Vbl{region},
or we could use 2D projections to look for a better separation.  We
choose the latter, and find a 2D linear combination of the same four
variables (\Vbl{palmitoleic}, \Vbl{stearic}, \Vbl{linoleic}, and
\Vbl{arachidic}), which almost separates the oils from the three areas
(Fig.~\ref{olive-nth}, bottom right). We found it using projection
pursuit guidance with the LDA index, followed by some manual tuning.

There are certainly clusters corresponding to the three areas, but we
have not found a projection in which they do not overlap. It may not be
possible to build a classifier that perfectly predicts the areas
within the \Vbl{region} North, but the error should be very small.

\bigskip
\noindent{\em Sardinia:} This \Vbl{region} is composed of two areas,
Coastal and Inland Sardinia.  This is an easy one!  We leave it to the
reader to look at a scatterplot of \Vbl{oleic} and \Vbl{linoleic}, which
shows a big gap between two clusters corresponding to the two areas.

\bigskip
\noindent{\em Southern Italy:} In this data, four areas are
grouped into the \Vbl{region} ``South.'' These are North Apulia, South
Apulia, Calabria, and Sicily. This table describes the correspondence
between \Vbl{area} and symbol for the next figure:

\begin{center}
\begin{tabular}{l@{\hspace{.3in}}l} %\hline
\T \B \Vbl{area} & Symbol \\\hline
\T  North Apulia   &  orange $+$ \\
  Calabria  &    red $+$ \\
  South Apulia  &    pink $\times$ \\
\B  Sicily  &    yellow $\times$ \\\hline
\end{tabular}
\end{center}

The prospect of finding visual separations between these four areas
looks dismal. In a scatterplot of \Vbl{palmitoleic} and \Vbl{palmitic}
(Fig.~\ref{olive-sth}, top row), there is a big gap between North
Apulia (low on both variables) and South Apulia (high on both
variables), with Calabria in the middle.  The troublemaker is Sicily:
The cluster of oils from Sicily overlaps those of the other three
areas.  (The plots in both rows are the same, except that the Sicilian
oils have been excluded from the bottom row of plots.)

This pattern does not change much when more variables are used.  We
can separate oils from Calabria, North Apulia, and South Apulia pretty
well in a 2D tour, \index{tour!grand} but oils from Sicily overlap
the oils from every other area in every projection.  The plot at the
top right of Fig.~\ref{olive-sth} is a typical example; it was found
using projection pursuit with the LDA index and then adjusted using
manual controls. \index{tour!projection pursuit guided}\index{tour!manual}

% Figure 7
\begin{figure*}[htbp]
\centerline{
 {\includegraphics[width=2.2in]{chap-class/olive-sth-2d.pdf}}
 {\includegraphics[width=2.2in]{chap-class/olive-sth-t2d.pdf}} }
\smallskip
\centerline{
 {\includegraphics[width=2.2in]{chap-class/olive-sth-2d-noS.pdf}}
 {\includegraphics[width=2.2in]{chap-class/olive-sth-t2d-noS.pdf}}
}
\caption[Separating the oils of the Southern \Vbl{region} by
\Vbl{area}]{Separating the oils of the Southern \Vbl{region} by
\Vbl{area}.  2D tour projections including all variables {\bf (right)}
show that the oils of Southern Italy are almost separable by
\Vbl{area}.  The samples from Sicily (yellow $\times$), however,
overlap the points of the other three areas, as can be seen by
comparing the plots in the top row with those in the bottom row, from
which Sicilian oils have been excluded.  }
\label{olive-sth}
\end{figure*}

\subsection{Taking stock} 

The \Data{Olive Oils} have dramatically different fatty acid
composition depending on geographic region. The three geographic
\Vbl{regions}, created by aggregating the \Vbl{areas} into North,
South and Sardinia, are well separated based on \Vbl{eicosenoic},
\Vbl{linoleic}, and \Vbl{arachidic}.  

% A paragraph break might make it more obvious that these are
% three distinct observations.  dfs

The oils from the North are mostly separable from each other by
\Vbl{area}, using all variables. The oils from the inland and coastal
regions of Sardinia have different amounts of oleic and linoleic
acids.

The oils from three areas in the South are almost separable, but the
oils from Sicily can not be separated.  Why are these oils
indistinguishable from the oils of the other areas in the South?  Is
there a problem with the quality of these samples?

\section{Numerical methods}~\label{class-num}

In this section, we show how classification algorithms can be
supported and enhanced by graphical methods.  Graphics should be used
before modeling, to make sure the data conforms to the assumptions of
the model, and they are equally useful after modeling, to assess the
fit of the model, study the failures, and compare the results of
different models.  Without graphics, it is easy to apply an algorithm
inappropriately and to achieve results that seem convincing but have
little or no meaning.

We will discuss graphical methods for linear discriminant analysis,
trees, random forests, neural networks, and support vector machines,
as applied to the \Data{Olive Oils}.  References are provided for the
reader who wants to know more about the algorithms, which are only
briefly described here.


\index{classification methods!linear discriminant analysis (LDA)}
\subsection{Linear discriminant analysis}

LDA, which has already been discussed extensively in this chapter, is
used to find the linear combination of variables that best separates
classes.  When the variance--covariance structure of each class is (1)
ellipsoidal and (2) roughly equal, LDA may be used to build a
classifier.  Even when those assumptions are violated, the linear
discriminant space may provide a good low-dimensional view of the
cluster structure.  This low-dimensional view can then be used by
other classifiers with less stringent requirements.

% Figure 8
\begin{figure*}[htbp]
\centerline{ {\includegraphics[width=2in]{chap-class/flea-varcov7a.pdf}}
 {\includegraphics[width=2in]{chap-class/flea-varcov7b.pdf}}}
\smallskip
\centerline{ {\includegraphics[width=2in]{chap-class/flea-varcov6a.pdf}}
 {\includegraphics[width=2in]{chap-class/flea-varcov6b.pdf}}}
\caption[Evaluating model assumptions for the 6D \Data{Flea Beetles}
data]{Evaluating model assumptions by comparing scatterplots of the
\Data{Flea Beetles} data {\bf (left)} with corresponding projections
of 6D normal variance--covariance ellipsoids {\bf (right)}.}
\label{flea-lda}
\end{figure*}

To determine whether it is safe to use LDA to classify a particular
dataset, we can use the 2D tour \index{tour!grand} to test the
assumptions made by the model.  We view a large number of projections
of the data, and we want to see ellipses of roughly the same size in
all projections.  Turning back to look at Fig.~\ref{lda-assumptions},
we are reminded that LDA is not an appropriate classifier for the
\Data{Olive Oils}, because the variance--covariances are neither
equal nor elliptical for all classes.

Earlier in the chapter, LDA seemed suitable for another of our
datasets, so we detour briefly from our examination of \Data{Olive
Oils} to consider \Data{Flea Beetles} again.  We have already seen
that the first two variables of the \Data{Flea Beetles} data
(Fig.~\ref{lda-assumptions}) are consistent with the equal
variance--covariance, multivariate normal model, but before using LDA
we would need to be sure that the assumptions hold for all six
variables.  We rotated six variables in the 2D tour for a while,
\index{tour!grand} and Fig.~\ref{flea-lda} shows two of the many
projections we observed.  In this figure, we compare the projection of
the data (at left) with the corresponding projection of the 6D
variance--covariance ellipsoids (at right). In some projections
(bottom row), there are a few slight deviations from the equal
ellipsoidal structure, but the differences are small enough to be due
to sampling variability.  We are satisfied that it would be
appropriate to apply LDA to the \Data{Flea Beetle} data.

% Figure 9
\begin{figure*}[htb]
\centerline{
  {\includegraphics[width=2in]{chap-class/olive-lda-missclassifications.pdf}}
  {\includegraphics[width=2in]{chap-class/olive-lda-missclassifications2.pdf}}}
\smallskip
\centerline{
  {\includegraphics[width=2in]{chap-class/olive-lda-missclassifications3.pdf}}
  {\includegraphics[width=2in]{chap-class/olive-lda-missclassifications4.pdf}}}
\caption[Misclassifications from an LDA classifier of the \Data{Olive
oils}.]{Misclassifications from an LDA classifier of the \Data{Olive
oils} by \Vbl{region}.  In the misclassification table {\bf (top
right)}, erroneously classified samples are brushed as large filled
circles, and studied in the discriminant space {\bf (top left)}, and
the 2D tour {\bf (bottom row)}.}
\label{olive-lda-misclassifications}
\end{figure*}
\index{brushing!linked}

\index{classification!discriminant space}
Even when the LDA model assumptions are violated, we can still use it
to find a good low-dimensional view of the cluster structure (as
discussed in Sect.~\ref{CMS}).  This is the discriminant space, the
linear combination of variables where the class means are most
separated relative to the pooled variance--covariance.  For the
\Data{Olive Oils}, it is computed by:

\begin{verbatim}
> library(MASS)
> library(rggobi)
> d.olive <- read.csv("olive.csv", row.names=1)
> d.olive.sub <- subset(d.olive,
   select=c(region,palmitic:eicosenoic))
> olive.lda <- lda(region~., d.olive.sub)
> pregion <- predict(olive.lda, d.olive.sub)$class
\end{verbatim}
\newpage  % Insert page break to avoid breaking the R output.
\begin{verbatim}
> table(d.olive.sub[,1], pregion)
   pregion
      1   2   3
  1 322   0   1
  2   0  98   0
  3   0   4 147
> plot(predict(olive.lda, d.olive.sub)$x)
> gd <- ggobi(cbind(d.olive, pregion))[1]
> glyph_color(gd) <- c(rep(6,323), rep(5,98), rep(1,151))
\end{verbatim}

\noindent The top left plot in Fig.~\ref{olive-lda-misclassifications} 
shows data projected into the discriminant space for the \Data{Olive
oils}.  Oils from the South form the large well-separated cluster at
the left; oils from the North are at the top right of the plot, and
are not well separated from the Sardinian oils just below them.

The misclassifications made by the model are highlighted in the plot,
drawn with large filled circles, and we can learn more about LDA by
exploring them:

\bigskip
\begin{center}
\begin{tabular}{l@{\hspace{.15in}}r@{\hspace{.1in}}rrr@{\hspace{.15in}}r}

\B & & \multicolumn{3}{c}{Predicted \Vbl{region}} & \multicolumn{1}{c}{Error} \\ 

\T            &    &          South & Sardinia & North & \\ \cline{3-6}
\T            &  South     &  322   & 0        &  {\bf 1} & 0.003\\
 \Vbl{region} &  Sardinia  &   0    & 98       &   0 & 0.000\\
              &  North     &   0    & {\bf 4}  & 147 & 0.026\\\cline{3-6}
\T            &            &        &          &     & 0.009

\end{tabular}
\end{center}
\bigskip


\noindent If we use LDA as a classifier, five samples are
misclassified.  It is not at all surprising to see misclassifications
where clusters overlap, as the Northern and Sardinian regions do, so
the misclassification of four Northern samples as Sardinian is not
troubling.

One very surprising misclassification is represented by the
orange circle, showing that, despite the large gap between these
clusters, one of the oils from the South has been misclassified as a
Northern oil.  As discussed earlier, LDA is blind to the size of the
gap when its assumptions are violated.  Since the variance--covariance
of these clusters is so different, LDA makes obvious mistakes, placing
the boundary too close to the Southern oils, the group with largest
variance.

These misclassified samples are examined in other projections shown by
a tour \index{tour!grand} (bottom row of plots).  The Southern oil
sample that is misclassified is on the outer edge of the cluster of
oils from the South, but it is very far from the points from the other
regions. It really should not be confused ~---~ it is clearly a Southern
oil.  Actually, even the four misclassified samples from the North
should not be confused by a good classifier, because even though they
are at one edge of the cluster of Northern oils, they are still far
from the cluster of Sardinian oils.

\index{tour!projection pursuit guided}
As we mentioned above, even when LDA fails as a classifier, the
discriminant space can be a good starting place to manually search the
neighborhood for a clearer view, that is, to sharpen the image.  It
can usually be re-created in a projection pursuit guided tour using
the LDA index. The projection shown in the lower left-hand plot of
Fig.~\ref{olive-lda-misclassifications} in which the three regions,
especially North and Sardinia, are better separated, was found using
manual controls starting from a local maximum of the LDA index.

% I think these points have been made so recently that they don't
% need to be re-stated here.  dfs

%In summary, LDA has useful tools for examining different aspects of a
%classification problem.  Graphics can be used to assess whether it is
%an appropriate model to use for the classification, and even if isn't,
%the discriminant space may provide a good launching pad for exploring
%the class structure.

%Checking misclassifications

%Australian crabs?


%\subsection{Logistic Regression}



%\subsection{MANOVA?} 

%Here or in the chapter on ANOVA? Use the sweat data...

\index{classification methods!tree}
\subsection{Trees}

% Difference between trees and LDA

% Construction

% Use with graphics

Tree classifiers generally provide a simpler solution than linear
discriminant analysis. For example, on the \Data{Olive Oils}, the tree
formed here:

\begin{verbatim}
> library(rpart)
> olive.rp <- rpart(region~., d.olive.sub, method="class")
> olive.rp
\end{verbatim}
 
\noindent yields this solution:

\begin{quote}
\begin{tabbing}
\= {\em if eicosenoic} $>=$ {\em 6.5  assign the sample to South}\\
\> {\em else}\\
\> \hspace{0.2in} \= {\em if linoleic} $>=$ {\em 1053.5  assign the sample to Sardinia}\\
\>  \>  {\em else                   assign the sample to North}
\end{tabbing}
\end{quote}

\noindent (There may be very slight variation in solutions depending
on the tree implementation and input to the algorithm.)  This rule is
simple because it uses only two of the eight variables, and it is also
accurate, yielding no misclassifications; that is, it has zero
prediction error.

The tree classifier is constructed by an algorithm that examines
the values of each variable for locations where a split would separate
members of different classes; the best split among the choices in all
the variables is chosen.  The data is then divided into the two
subsets falling on each side of the split, and the process is then
repeated for each subset, until it is not prudent to make further splits.

% Figure 10
\begin{figure*}[htbp]
\centerline{
  {\includegraphics[width=2.0in]{chap-class/olive-tree2.pdf}}
  {\includegraphics[width=2.0in]{chap-class/olive-tree.pdf}}}
\smallskip
\centerline{
  {\includegraphics[width=2.0in]{chap-class/olive-tree-manip.pdf}}
  {\includegraphics[width=2.0in]{chap-class/olive-tree-manip2.pdf}}}
\caption[Improving on the results of the tree classifier using the
manual tour]{Improving on the results of the tree classifier using
the manual tour.  The tree classifier determines that only
\Vbl{eicosenoic} and \Vbl{linoleic} acid are necessary to separate the
three regions {\bf (R plot, top left)}.  This view is duplicated in
GGobi {\bf (top right)} and sharpened using manual controls {\bf
(bottom left)}.  The improved result is then returned to R and
re-plotted with reconstructed boundaries {\bf (bottom right)}.
% (*** Arachidic label should be visible in tour plots. ***)
}
\label{olive-tree}
\end{figure*}

The numerical criteria of accuracy and simplicity suggest that the
tree classifier for the \Data{Olive Oil} is perfect. However, by
examining the classification boundary plotted on the data
(Fig.~\ref{olive-tree}, top left), we see that it is not: Oils from
Sardinia and the North are not clearly differentiated. The tree method
does not consider the variance--covariance of the groups ~---~ it simply
slices the data on single variables.  The separation between the
Southern oils and the others is wide, so the algorithm finds that
first, and slices the data right in the middle of the gap.  It next
carves the data between the Northern and Sardinian oils along the
\Vbl{linoleic} axis ~---~ even though there is no gap between these
groups along that axis.  With so little separation between these two
classes, the solution may be quite unstable for future samples of
Northern and Sardinian oils: A small difference in linoleic acid
content may cause a new observation to be assigned into the wrong
region.

Tree classifiers are usually effective at singling out the most
important variables for classification.  However, since they define
each split using a single variable, they are likely to miss any model
improvements that might come from using linear combinations of
variables.  Some tree implementations consider linear
combinations of variables, but they are not in common use.  The more
commonly used models might, at best, approximate a linear combination
by using many splits along the different variables, zig-zagging a
boundary between clusters.

Accordingly the model produced by a tree classifier can sometimes be
improved by exploring the neighborhood using the manual tour controls
\index{tour!manual} (Fig.~\ref{olive-tree}, top right and bottom
left). Starting from the projection of the two variables selected by
the tree algorithm,
\Vbl{linoleic} and \Vbl{eicosenoic}, we find an improved projection by
including just one other variable, \Vbl{arachidic}.  The gap between
the Northern and Sardinian regions is distinctly wider.

We can capture the coefficients of rotation that generate this
projection, and we can use them to create a new variable.  We define
$linoarach$ to be $ \frac{0.969}{1022}\times$ {\tt linoleic}$ +
\frac{0.245}{105} \times$ {\tt arachidic}.  We can add this new
variable to the olive oils data and run the tree classifier on the
augmented data.  The new tree is:

\begin{quote}
\begin{tabbing}
\= {\em if eicosenoic} $>=$ {6.5  assign the sample to South} \\
\> {\em else}\\
\> \hspace{0.2in}\= {\em  if linoarach $>=$ 1.09 assign the sample to Sardinia} \\
\>\> {\em else                assign the sample to North}
\end{tabbing}
\end{quote}

\noindent Is this tree better than the original?  The error for both
models is zero, so there is no difference numerically.  However, the
plots in Fig.~\ref{olive-tree} (top left and bottom right) suggest
that the sharpened tree would be more robust to small variations in
the fatty acid content and would thus classify new samples with less
error.

%Matt, and Eun-Kyung's RGGobi code: need a tree library, not rpart,
%need randomForest library,

\index{classification methods!random forest}
\subsection{Random forests}
\label{random-forests}

A random forest \cite{Br01,Cu04} is a classifier that is built from
multiple trees generated by randomly sampling the cases and the
variables.  The random sampling (with replacement) of cases has the
fortunate effect of creating a training (``in-bag'') and a test
(``out-of-bag'') sample for each tree computed.  The class of each
case in the out-of-bag sample for each tree is predicted, and the
predictions for all trees are combined into a vote for the class
identity.  

A random forest is a computationally intensive method, a ``black box''
classifier, but it produces various diagnostics that make the outcome
less mysterious.  Some diagnostics that help us to assess the
model are the votes, the measures of variable importance, the error
estimate, and as usual, the misclassification tables.

\index{R package!\RPackage{randomForest}}

We test the method on the \Data{Olive Oils} by building a random
forest classifier of 500 trees, using the R package \RPackage{randomForest}
\cite{Li06}:

\begin{verbatim}
> library(randomForest)
> olive.rf <- randomForest(as.factor(region)~., 
  data=d.olive.sub, importance=TRUE, proximity=TRUE, mtry=4)
> order(olive.rf$importance[,5], decreasing=T)
[1] 8 5 4 1 7 2 6 3
> pred <- as.numeric(olive.rf$predicted)
\end{verbatim}
\newpage  % Insert page break to avoid breaking the R output.
\begin{verbatim}
> table(d.olive.sub[,1], olive.rf$predicted)
      1   2   3
  1 323   0   0
  2   0  98   0
  3   0   0 151
> margin <- olive.rf$vote
> colnames(margin) <- c("Vote1", "Vote2", "Vote3")
> d.olive.rf <- cbind(pred, margin, d.olive)
> gd <- ggobi(d.olive.rf)[1]
> glyph_color(gd) <- c(rep(6,323), rep(5,98), rep(1,151))
\end{verbatim}

% Figure 11
\begin{figure*}[h]
\centerline{
  {\includegraphics[width=2.0in]{chap-class/olive-forest1.pdf}}
  {\includegraphics[width=2.0in]{chap-class/olive-forest2.pdf}}}
\smallskip
\centerline{
  {\includegraphics[width=2.0in]{chap-class/olive-forest9.pdf}}}
\caption[Examining the results of a forest classifier of \Data{Olive
Oils}]{Examining the results of a forest classifier of \Data{Olive
Oils} by \Vbl{region}.  The votes assess the uncertainty associated
with each sample.  The cases classified with the greatest uncertainty
lie far from the corners of the triangles.  These points are brushed
{\bf (top left)}, and we examine their location using the linked tour
plot {\bf (top right)}.  The introduction of \Vbl{linoarach} {\bf
(bottom)} eliminates the confusion between Sardinia and the North. }
\label{olive-forest}
\end{figure*}

\noindent Each tree used a random sample of four of the eight
variables, as well as a random sample of about a third of the 572
cases.  The votes are displayed in the left-hand plot of
Fig.~\ref{olive-forest}, next to a projection from a
\index{tour!grand} 2D tour. Since there are three classes, the votes
form a triangle, with one vertex for each region, with oils from the
South at the far right, Sardinian oils at the top, and Northern oils
at the lower left. Samples that are consistently classified correctly
are close to the vertices; cases that are commonly misclassified are
further from a vertex.  Although forests perfectly classify this data,
the number of points falling between the Northern and the Sardinian
vertices suggests some potential for error in classifying future
samples.

For more understanding of the votes, we turn to another diagnostic:
variable importance. Forests return two measures of variable
importance, both of which give similar results. Based on the Gini
measure, the most important variables, in order, are \Vbl{eicosenoic},
\Vbl{linoleic}, \Vbl{oleic}, \Vbl{palmitic}, \Vbl{arachidic},
\Vbl{palmitoleic}, \Vbl{linolenic}, and \Vbl{stearic}.  

Some of this ordering is as expected, given the initial graphical
inspection of the data (Sect.~\ref{class-plots}).  The importance of
\Vbl{eicosenoic} was our first discovery, as shown in the top row of
Fig.~\ref{olive-1d}.  And yes, \Vbl{linoleic} is next in importance:
The first two plots in Fig.~\ref{olive-2d} make that clear.  The
surprise is that the forest should consider \Vbl{arachidic} to be less
important than \Vbl{palmitic}.  This is not what we found, as shown in
the right-hand plot in that figure.

Did we overlook something important in our earlier investigation?  We
return to the use of the manual manipulation of the tour
\index{tour!manual} to see whether \Vbl{palmitic} does in fact perform
better than \Vbl{arachidic} at finding a gap between the two regions.
But it does not.  By overlooking the importance of \Vbl{arachidic},
the random forest never finds an adequate gap between the oils of the
Northern and the Sardinian regions, and that probably explains why there
is more confusion about some Northern samples than there should be.

We rebuild the forest using a new variable constructed from a linear
combination of \Vbl{linoleic} and \Vbl{arachidic} (\Vbl{linoarach}),
just as we did when applying the single tree classifier.  Since
correlated variables reduce each other's importance, we need to remove
\Vbl{linoleic} and \Vbl{oleic} when we add \Vbl{linoarach}.  Once we
have done this, the confusion between Northern and Sardinian oils
disappears (Fig.~\ref{olive-forest}, lower plot): The points are now
tightly clumped at each vertex, which indicates more certainty in
their class predictions. The new variable becomes the second most
important variable according to the importance diagnostic.

% Need R code here and output - variable importance

Classifying the oils by the three large \Vbl{region}s is too easy a
problem for forests; they are designed to tackle more challenging
classification tasks. We will use them to examine the oils from the
areas in the Southern region (North and South Apulia, Calabria, and
Sicily). Remember the initial graphical inspection of the data, which
showed that oils from the four areas were not completely
separable. The samples from Sicily overlapped those of the three other
areas.  We will use a forest classifier to see how well it can
differentiate the Southern oils by \Vbl{area}:

\newpage % Insert newpage to pull the first two lines to the next page
\begin{verbatim}
> d.olive.sth <- subset(d.olive, region==1, 
   select=area:eicosenoic)
> olive.rf <- randomForest(as.factor(area)~., 
   data=d.olive.sth, importance=TRUE, proximity=TRUE, 
   mtry=2, ntree=1500)
> order(olive.rf$importance[,5], decreasing=T)
[1] 5 2 4 3 1 6 7 8
> pred <- as.numeric(olive.rf$predicted)
> table(d.olive.sth[,1], olive.rf$predicted)
      1   2   3   4
  1  22   2   0   1
  2   0  53   2   1
  3   0   1 202   3
  4   3   4   5  24
> margin <- olive.rf$vote
> colnames(margin) <- c("Vote1", "Vote2", "Vote3", "Vote4")
> d.olive.rf <- cbind(pred, margin, d.olive.sth)
> gd <- ggobi(d.olive.rf)[1]
> glyph_color(gd) <- c(6,3,2,9)[d.olive.rf$area]
\end{verbatim} 

\noindent After experimenting with several input parameters, we show
the results for a forest of 1,500 trees, sampling two variables at each
tree node, and yielding an error rate of 0.068.  The misclassification
table is:

\bigskip
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\\cline{3-7}

\T         & North Apulia & 22 & {\bf 2} & 0 & {\bf 1} & 0.120 \\
\Vbl{area} & Calabria & 0 & 53 & {\bf 2} & {\bf 1} & 0.054 \\
           & South Apulia & 0 & {\bf 1} & 202 & {\bf 3} & 0.019 \\
           & Sicily & {\bf 3} & {\bf 4} & {\bf 5} & 24 & 0.333 \\ \cline{3-7}
\T         &        &         &         &         &    &  0.068
\end{tabular}
\end{center}
\bigskip

\noindent The error of the forest is surprisingly low, but the 
error is definitely not uniform across classes. Predictions for Sicily
are wrong about a third of the time. Figure~\ref{olive-forest2} shows
some more interesting aspects of the results.  For this figure, the
following table describes the correspondence between area and symbol:

\begin{center}
\begin{tabular}{l@{\hspace{.3in}}l} %\hline
\T \B \Vbl{area} & symbol \\\hline
\T  North Apulia   &  orange $+$ \\
  Calabria  &    red $+$ \\
  South Apulia  &    pink $\times$ \\
\B  Sicily  &    yellow $\times$ \\\hline
\end{tabular}
\end{center}

\index{jittering}

\noindent Look first at the top row of the figure.  The
misclassification table is represented by a jittered scatterplot, at
the left.  A plot from a 2D tour \index{tour!grand} of the four
voting variables is in the center.  Because there are four groups, the
votes lie on a 3D tetrahedron (a simplex).  The votes from three of
the areas are pretty well separated, one at each ``corner,'' but those
from Sicily overlap all of them.  Remember that when points are
clumped at the vertex, class members are consistently predicted
correctly.  Since this does not occur for Sicilian oils, we see that
there is more uncertainty in the predictions for this area.

The plot at right confirms this observation. It is a projection from a
2D tour \index{tour!grand} of the four most important variables,
showing a pattern we have seen before.  We can achieve pretty good
separation of the oils from North Apulia, Calabria, and South Apulia,
but the oils from Sicily overlap all three clusters.  Clearly these
are tough samples to classify correctly.

% Figure 12
\begin{figure*}[htbp]
\centerline{
  {\includegraphics[width=1.5in]{chap-class/olive-sth-forest1.pdf}}
  {\includegraphics[width=1.5in]{chap-class/olive-sth-forest7.pdf}}
  {\includegraphics[width=1.5in]{chap-class/olive-sth-forest3.pdf}}}
\smallskip
\centerline{
  {\includegraphics[width=1.5in]{chap-class/olive-sth-forest4.pdf}}
  {\includegraphics[width=1.5in]{chap-class/olive-sth-forest5.pdf}}
  {\includegraphics[width=1.5in]{chap-class/olive-sth-forest6.pdf}}}
\caption[Examining the results of a random forest after classifying
the oils of the South]{Examining the results of a random forest after
classifying the oils of the South by \Vbl{area}.  A representation of
the misclassification table {\bf (left)} is linked to plots of the
votes {\bf (middle)} and a 2D tour {\bf (right)}.  The Sicilian oils
have been excluded from the plots in the bottom row.}
\label{olive-forest2}
\end{figure*}

We remove the Sicilian oils from the plots so we can focus on the
other three areas (bottom row of plots). The points representing North
Apulian oils form a very tight cluster at a vertex, with three
exceptions. Two of these points are misclassified as Calabrian, and we
have highlighted them as large filled circles by painting the
misclassification plot.

The pattern of the votes (middle plot) suggests that there is high
certainty in the predictions for North Apulian oils, with the
exception of these two samples.  When we watch the votes in the tour
\index{tour!grand} for a while, we see that the votes of these two
samples travel as if they were in a cluster all their own, which is
distinct from the remaining North Apulian oils.

However, when we look at the data, we find the votes for these two
samples a bit puzzling.  We watch the four most important variables in
the tour for a while (as in the right plot), and these two points
do not behave as if they were in a distinct cluster; they travel with
the rest of the samples from North Apulia.  They do seem to be
outliers with respect their class, but they are not so far from their
group ~---~ it is a bit surprising that the forest has trouble
classifying these cases.

Rather than exploring the other misclassifications, we leave that for
the reader.

In summary, a random forest is a useful method for tackling tough
classification problems.  Its diagnostics provide a rich basis for
graphical exploration, which helps us to digest and evaluate the solution.

\index{classification methods!neural network}
\subsection{Neural networks}

Neural networks for classification can be thought of as additive
models where explanatory variables are transformed, usually through a
logistic function, added to other explanatory variables, transformed
again, and added again to yield class predictions. Aside from the data
mining literature, mentioned earlier, a good comprehensive and
accessible description for statisticians can be found in
\citeasnoun{CT94}. The model can be formulated as:

\[
\hat{y} = f(x) = \phi(\alpha+\sum_{h=1}^{s}
w_{h}\phi(\alpha_h+\sum_{i=1}^{p} w_{ih}x_i))
\]

\index{R package!\RPackage{nnet}}

\noindent where $x$ is the vector of explanatory variable values, $y$
is the target value, $p$ is the number of variables, $s$ is the number
of nodes in the single hidden layer, and $\phi$ is a fixed function,
usually a linear or logistic function. This model has a single hidden
layer and univariate output values.  The model is fit by minimizing
the sum of squared differences between observed values and fitted
values, and the minimization does not always converge. A neural
network is a black box that accepts inputs, computes, and spits out
predictions.  With graphics, some insight into the black box can be
gained. We use the feed-forward neural network provided in the {\tt
nnet} package of R \cite{VR02} to illustrate.

We continue to work with \Data{Olive Oils}, and we look at the
performance of the neural network in classifying the oils in the four
areas of the South, a difficult challenge. Because the software
does not include a method for computing the predictive error, we break
the data into training and test samples so we can better estimate the
predictive error.  (We could tweak the neural network to perfectly fit
all the data, but then we could not estimate how well it would perform
with new data.)

\begin{verbatim}
> indx.tst <- c(1,7,12,15,16,22,27,32,34,35,36,41,50,54,61,
 68,70,75,76,80,95,101,102,105,106,110,116,118,119,122,134,
 137,140,147,148,150,151,156,165,175,177,182,183,185,186,
 187,190,192,194,201,202,211,213,217,218,219,225,227,241,
 242,246,257,259,263,266,274,280,284,289,291,292,297,305,
 310,313,314,323,330,333,338,341,342,347,351,352,356,358,
 359,369,374,375,376,386,392,405,406,415,416,418,420,421,
 423,426,428,435,440,451,458,460,462,466,468,470,474,476,
 480,481,482,487,492,493,500,501,509,519,522,530,532,541,
 543,545,546,551,559,567,570)
> d.olive.train <- d.olive[-indx.tst,]
> d.olive.test <- d.olive[indx.tst,]
> d.olive.sth.train <- subset(d.olive.train, region==1, 
   select=area:eicosenoic)
> d.olive.sth.test <- subset(d.olive.test, region==1, 
   select=area:eicosenoic)
\end{verbatim}

% this may need a few more words

After trying several values for $s$, the number of nodes in the hidden
layer, we chose $s=4$; we also chose a linear $\phi$, $decay=0.005$,
and $range=0.06$. We fit the model using many different random
starting values, rejecting the results until it eventually converged
to a solution with a reasonably low error:

\begin{verbatim}
> library(nnet)
> olive.nn <- nnet(as.factor(area)~., d.olive.sth.train, 
  size=4, linout=T, decay=0.005, range=0.06, maxit=1000)
> targetr <- class.ind(d.olive.sth.train[,1])
> targets <- class.ind(d.olive.sth.test[,1])
> test.cl <- function(true, pred){
    true <- max.col(true)
    cres <- max.col(pred)
    table(true, cres)
  }
> test.cl(targetr, predict(olive.nn, 
  d.olive.sth.train[,-1]))
    cres
true   1   2   3   4
   1  16   0   1   2
   2   0  42   0   0
   3   0   1 155   2
   4   1   1   1  24
\end{verbatim}
\newpage  % Insert page break to avoid breaking the R output.
\begin{verbatim}
> test.cl(targets, predict(olive.nn, d.olive.sth.test[,-1]))
    cres
true  1  2  3  4
   1  3  2  0  1
   2  0 12  2  0
   3  0  2 45  1
   4  1  2  1  5
> parea <- c(max.col(predict(olive.nn, 
   d.olive.sth.train[,-1])),
   max.col(predict(olive.nn, d.olive.sth.test[,-1])))
> d.olive.nn <- cbind(rbind(d.olive.sth.train, 
    d.olive.sth.test), parea)
> gd <- ggobi(d.olive.nn)[1]
> glyph_color(gd) <- c(6,3,2,9)[d.olive.nn$area]
\end{verbatim}

Below are the misclassification tables for the training and test
samples.

\bigskip
\emph{Training:}
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\ \cline{3-7}

\T         & North Apulia & 16 & 0 & {\bf 1} & {\bf 2} & 0.158\\
\Vbl{area} & Calabria & 0 & 42 & 0 &  0 & 0.000\\
           & South Apulia & 0 & {\bf 1} & 155 & {\bf 2} & 0.019\\
\B         & Sicily & {\bf 1} & {\bf 1} & {\bf 1} & 24 & 0.111\\ \cline{3-7}
\T         &        &         &         &         &    & 0.037
\end{tabular}
\end{center}

\bigskip

\emph{Test:}
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\  \cline{3-7}

\T         & North Apulia & 3 & {\bf 2} & 0 & {\bf 1} & 0.333\\
\Vbl{area} & Calabria & 0 & 12 & {\bf 2} &  0 & 0.143\\
           & South Apulia & 0 & {\bf 2} & 45 & {\bf 1} & 0.063\\
\B         & Sicily & {\bf 1} & {\bf 2} & {\bf 1} & 5 & 0.444\\  \cline{3-7}
\T         &        &         &         &         &    & 0.156
\end{tabular}
\end{center}

\bigskip

\noindent The training error is $9/246=0.037$, and the test error is
$12/77=0.156$.  The overall errors, as in the random forest model, are
not uniform across classes.  This is particularly obvious in the test
error table: The error in classifying North Apulian oils is close to a
third, and it is even worse for Sicilian oils, which have an almost
even chance of being misclassified.

Our exploration of the misclassifications is shown in
Fig.~\ref{olive-nn}.  (The troublesome Sicilian oils have been
excluded from all plots in this figure.)  Consider first the plots in
the top row.  The left-hand plot shows the misclassification table.
Two samples of oils from North Apulia (orange $+$) have been
incorrectly classified as South Apulian (pink $\times$), and these two
points have been brushed as filled orange circles.  Note where these
points fall in the next two plots, which are linked 2D tour
projections. \index{brushing!linked}\index{tour!grand} One of the two
misclassified points is on the edge of the cluster of North Apulian
points, close to the Calabrian cluster. It is understandable that
there might be some confusion about this case. The other sample is on
the outer edge of the North Apulian cluster, but it is far from the
Calabrian cluster ~---~ this should not have been confused.

% Figure 13
\begin{figure*}[htbp]
\centerline{
 {\includegraphics[width=1.5in]{chap-class/olive-nn1.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-nn2.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-nn8.pdf}}}
\smallskip
\centerline{
 {\includegraphics[width=1.5in]{chap-class/olive-nn3.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-nn4.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-nn6.pdf}}}
\caption[Misclassifications of a feed-forward neural network
classifying the oils from the South]{Misclassifications of a
feed-forward neural network classifying the oils from the South by
\Vbl{area}. A representation of the misclassification table {\bf (left
column)} is linked to projections viewed in a 2D tour. Different
misclassifications are examined in the top and bottom rows. (The
Sicilian oils, which would have appeared in the top row of the
misclassification tables, have been removed from all plots.)  }
\label{olive-nn}
\end{figure*}

In the bottom row of plots, we follow the same procedure to examine
the single North Apulian sample misclassified as South Apulian. It is
painted as a filled orange circle in the misclassification plot and
viewed in a \index{tour!grand} tour. This point is on the outer edge
of the North Apulian cluster, but it is closer to the Calabrian cluster
than the South Apulian cluster. It would be understandable for it to
be misclassified as Calabrian, so it is puzzling that it is
misclassified as South Apulian.

In summary, a neural network is a black box method for tackling tough
classification problems. It will generate different solutions each
time the net is fit, some much better than others. When numerical
measures suggest that a reasonable model has been found, graphics can
be used to inspect the model in more detail.

\index{classification methods!support vector machine (SVM)}
\subsection{Support vector machine}

% http://www.support-vector-machines.org/SVM_osh.html
% wikipedia

A support vector machine (SVM) \cite{Va99} is a binary classification
method.  An SVM looks for gaps between clusters in the data, based on the
extreme observations in each class. In this sense it mirrors the
graphical approach described at the start of this chapter, in which we
searched for gaps between groups. We describe this method more fully
than we did the other algorithms for two reasons: first, because of
its apparent similarity to the graphical approach, and second, because
it is difficult to find a simple explanation of the method in the
literature.

The algorithm takes an $n \times p$ data matrix, where each column is
scaled to [$-$1,1] and each row is labeled as one of two classes
($y_i=+1$ or $-1$), and finds a hyperplane that separates the two
groups, if they are separable. Each row of the data matrix is a vector
in $p$-dimensional space, denoted as

% Should this be represented as a row instead of a column?  dfs

\[
\blX=\left[ \begin{array}{c}
  x_1 \\ x_2 \\ \vdots \\ x_p \end{array} \right]
\]

\noindent and the separating hyperplane can be written as

\[
\blW'\blX + b = 0
\]

\noindent where $\blW = [ w_1~~ w_2 ~~ \dots ~~ w_p]'$ is the normal
vector to the separating hyperplane and $b$ is a constant.  The best
separating hyperplane is found by maximizing the margin of separation
between the two classes as defined by two parallel hyperplanes:

\[
\blW'\blX + b = 1, ~~~~~ \blW'\blX + b = -1.
\]

\noindent These hyperplanes should maximize the distance from the
separating hyperplane and have no points between them, capitalizing on
any gap between the two classes. The distance from the origin to the
separating hyperplane is $|b|/||\blW||$, so the distance between the
two parallel margin hyperplanes is $2/||\blW||=2/\sqrt{w_1^2+\dots
+w_p^2}$. Maximizing this is the same as minimizing $||\blW||/2$. To
ensure that the two classes are separated, and that no points lie
between the margin hyperplanes we need:

\[
\blW'\blX_i + b \geq 1, ~~~\mbox{  or  } ~~~\blW'\blX_i + b \leq -1 ~~~\forall i=1, ..., n
\]

\noindent which corresponds to

\begin{eqnarray}
y_i(\blW'\blX_i+b)\geq 1 ~~~\forall i=1, ..., n
\label{svm-crit}
\end{eqnarray}

\noindent Thus the problem corresponds to
\begin{quote} 
{\em minimizing} $\frac{||\blW||}{2}$ {\em subject to }
$y_i(\blX_i\blW+b)\geq~1 ~~~\forall i=1, ..., n$.
\end{quote}

\index{SVMLight}

\noindent Interestingly, only the points closest to the margin
hyperplanes are needed to define the separating hyperplane. We might
think of these points as lying on or close to the convex hull of each
cluster in the area where the clusters are nearest to each other.
These points are called support vectors, and the coefficients of the
separating hyperplane are computed from a linear combination of the
support vectors $\blW = \sum_{i=1}^{s} y_i\alpha_i\blX_i$, where $s$
is the number of support vectors.  We could also use $\blW =
\sum_{i=1}^n y_i\alpha_i\blX_i$, where $\alpha_i=0$ if $\blX_i$ is not
a support vector. For a good fit the number of support vectors $s$
should be small relative to $n$. Fitting algorithms can achieve gains
in efficiency by using only samples of the cases to find suitable
support vector candidates; this approach is used in the SVMLight
\cite{Jo99} software.

In practice, the assumption that the classes are completely separable
is unrealistic. Classification problems rarely present a gap between
the classes, such that there are no misclassifications.
\citeasnoun{CV95} relaxed the separability condition to allow some
misclassified training points by adding a tolerance value $\epsilon_i$
to Equation \ref{svm-crit}, which results in the modified criterion
$y_i(\blW'\blX_i+b)>1-\epsilon_i, \epsilon_i\geq 0$. Points that meet
this criterion but not the stricter one are called slack vectors.

Nonlinear classifiers can be obtained by using nonlinear
transformations of $\blX_i$, $\phi(\blX_i)$ \cite{BGV92}, which is
implicitly computed during the optimization using a kernel function
$K$. Common choices of kernels are linear
$K(\blx_i,\blx_j)=\blx_i'\blx_j$, polynomial
$K(\blx_i,\blx_j)=(\gamma\blx_i'\blx_j+r)^d$, radial basis
$K(\blx_i,\blx_j)=\exp(-\gamma||\blx_i-\blx_j||^2)$, or sigmoid
functions $K(\blx_i,\blx_j)=\mbox{tanh}(\gamma\blx_i'\blx_j+r)$, where
$\gamma>0, r,$ and $d$ are kernel parameters.

% She didn't say to delete the terminating colon here, but by 
% analogy with these rest, I will.  dfs

The ensuing minimization problem is formulated as

\[
\mbox{\em minimizing } \frac{1}{2}||\blW|| + C\sum_{i=1}^n \epsilon_i ~~ \mbox{\em subject to } 
y_i(\blW'\phi(\blX)+b)>1-\epsilon_i
\]

\noindent where $\epsilon_i\geq 0$, $C>0$ is a penalty parameter guarding 
against over-fitting the training data and $\epsilon$ controls the
tolerance for misclassification. The normal to the separating
hyperplane $\blW$ can be written as $\sum_{i=1}^{n}
y_i\alpha_i{\phi(\blX_i)}$, where points other than the support and
slack vectors will have $\alpha_i=0$.  Thus the optimization problem
becomes

\begin{eqnarray*}
\mbox{\em minimizing } \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n y_iy_j\alpha_i\alpha_jK(\blX_i,\blX_j)+C\sum_{i=1}^n \epsilon_i \\ ~~~~~~~~~~~\mbox{\em subject to } 
y_i(\blW'\phi(\blX)+b)>1-\epsilon_i
\end{eqnarray*}

\index{R package!\RPackage{e1071}}
\index{R package!\RPackage{libsvm}}

We use the {\tt svm} function in the \RPackage{e1071} package
\cite{DHLMW} of R, which uses \RPackage{libsvm} \cite{CL}, to classify
the oils of the four areas in the Southern region.  SVM is a binary
classifier, but this algorithm overcomes that limitation by comparing
classes in pairs, fitting six separate classifiers, and then using a
voting scheme to make predictions. To fit the SVM we also need to
specify a kernel, or rely on the internal tuning tools of the
algorithm to choose this for us.  Automatic tuning in the algorithm
chooses a radial basis, but we found that a linear kernel performed
better, so that is what we used. (This accords with our earlier visual
inspection of the data in Sect.~\ref{class-plots}.)  Here is the R
code used to fit the model:

\begin{verbatim}
> library(e1071)
> olive.svm <- best.svm(factor(area) ~ ., data=d.olive.train)
> olive.svm <- svm(factor(area) ~ ., data=d.olive.sth.train, 
  type="C-classification", kernel="linear")
> table(d.olive.sth.train[,1], predict(olive.svm, 
  d.olive.sth.train))
   
      1   2   3   4
  1  19   0   0   0
  2   0  42   0   0
  3   0   0 155   3
  4   1   2   3  21
> table(d.olive.sth.test[,1], predict(olive.svm, 
  d.olive.sth.test))
   
     1  2  3  4
  1  6  0  0  0
  2  1 12  1  0
  3  0  0 46  2
  4  1  1  0  7
> support.vectors <- olive.svm$index[
    abs(olive.svm$coefs[,1])<1 &
    abs(olive.svm$coefs[,2])<1 & abs(olive.svm$coefs[,3])<1]
> pointtype <- rep(0,323) # training
> pointtype[247:323] <- 1 # test
> pointtype[olive.svm$index] <- 2 # slack vectors
> pointtype[support.vectors] <- 3 # support vectors
> parea <- c(predict(olive.svm, d.olive.sth.train),
    predict(olive.svm, d.olive.sth.test))
> d.olive.svm <- cbind(rbind(d.olive.sth.train, 
    d.olive.sth.test), parea, pointtype)
> gd <- ggobi(d.olive.svm)[1]
> glyph_color(gd) <- c(6,3,2,9)[d.olive.svm$area]
\end{verbatim}

\noindent These are our misclassification tables:

\smallskip
\noindent \emph{Training:}
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\  \cline{3-7}

\T         & North Apulia & 19 & 0 & 0 & 0 & 0.000\\
\Vbl{area} & Calabria & 0 & 42 & 0 &  0 & 0.000\\
           & South Apulia & 0 & 0 & 155 & {\bf 3} & 0.019\\
\B         & Sicily & {\bf 1} & {\bf 2} & {\bf 3} & 21 & 0.222 \\ \cline{3-7}
\T         &        &         &         &         &    & 0.037
\end{tabular}
\end{center}

\bigskip

\noindent \emph {Test:}
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\  \cline{3-7}

\T         & North Apulia & 6 & 0 & 0 & 0 & 0.000\\
\Vbl{area} & Calabria & {\bf 1} & 12 & {\bf 1} &  0 & 0.143\\
           & South Apulia & 0 & 0 & 46 & {\bf 2} & 0.042\\
\B         & Sicily & {\bf 1} & {\bf 1} & 0 & 7 & 0.286 \\  \cline{3-7}
\T         &        &         &         &         &    & 0.078
\end{tabular}
\end{center}
\bigskip

\noindent The training error is $9/246=0.037$, and the test error is
$6/77=0.078$.  (The training error is the same as that of the neural
network classifier, but the test error is lower.)  Most error
is associated with Sicily, which we have seen repeatedly to be an
especially difficult class to separate.  In the training data there
are no other errors, and in the test data there are just two samples
from Calabria mistakenly classified. Figure~\ref{olive-svm}
illustrates our examination of the misclassified cases, one in each
row of the figure. (Points corresponding to Sicily were removed from
all four plots.)  Each of the two cases is brushed (using a filled red
circle) in the plot of misclassification table and viewed in a linked
2D tour.  \index{brushing!linked}\index{tour!grand} Both of these
cases are on the edge of their clusters so the confusion of classes is
reasonable.

% Figure 14
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=2.2in]{chap-class/olive-svm6.pdf}}
 {\includegraphics[width=2.2in]{chap-class/olive-svm7.pdf}}}
\smallskip

\centerline{{\includegraphics[width=2.2in]{chap-class/olive-svm8.pdf}}
 {\includegraphics[width=2.2in]{chap-class/olive-svm9.pdf}}}
\caption[Misclassifications of a support vector machine classifying
 the oils of the South]{Misclassifications of a support vector machine
 classifying the oils of the South by \Vbl{area}.  The
 misclassification table {\bf (left)} is linked to 2D tour plots {\bf
 (right)}; different misclassifications are examined in each row of
 plots. (The oils from Sicily, the fourth area, have been removed from
 all plots.)  }
\label{olive-svm}
\end{figure*}

% Figure 15
\begin{figure*}[htbp]
\centerline{
 {\includegraphics[width=1.5in]{chap-class/olive-svm3.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-svm4.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-svm5.pdf}}}
\caption[Using the tour to examine the choice of support
vectors]{Using the tour to examine the choice of support vectors when
classifying Southern oils by \Vbl{area}.  Support vectors are open
circles, and slack vectors are open rectangles; the data points are
represented by $+$es and $\times$es.}
\label{olive-svm2}
\end{figure*}

The linear SVM classifier uses 20 support vectors and 29 slack vectors
to define the separating planes between the four areas. It is
interesting to examine which points are selected as support vectors,
and where they are located in the data space.  For each pair of
classes, we expect to find some projection in which the support vectors
line up on either side of the margin of separation, whereas the slack
vectors lie closer to the boundary, perhaps mixed in with the points
of other classes.

\index{tour!grand} \index{tour!manual} The plots in
Fig.~\ref{olive-svm2} represent our use of the 2D tour, augmented by
manual manipulation,~to look for these projections.  (The Sicilian
points are again removed.) The support vectors are represented by open
circles and the slack vectors by open rectangles, and we have been
able to find a number of projections in which the support vectors are
on the opposing outer edge of the point clouds for each cluster.

The linear SVM does a very nice job with this difficult
classification. The accuracy is almost perfect on three classes,
and the misclassifications are quite reasonable mistakes, being points
that are on the extreme edges of their clusters.  However, this method
joins the list of those defeated by the difficult problem of
distinguishing the Sicilian oils from the rest.

\subsection{Examining boundaries}

\index{classification!examining boundaries}

For some classification problems, it is possible to get a good picture
of the boundary between two classes. With LDA and SVM classifiers the
boundary is described by the equation of a hyperplane. For others the
boundary can be determined by evaluating the classifier on points
sampled in the data space, using either a regular grid or some more
efficient sampling scheme.

% Figure 16
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=2in]{chap-class/olive-classifly-lda.pdf}}
 {\includegraphics[width=2in]{chap-class/olive-classifly-svm.pdf}}}
\smallskip
\centerline{{\includegraphics[width=2in]{chap-class/olive-classifly-svm3.pdf}}
 {\includegraphics[width=2in]{chap-class/olive-classifly-svm5.pdf}}}
\caption[Classification boundaries for different
models]{Classification boundaries for different models shown in the 2D
tour. Points on the boundary are gray stars.  We first compare LDA
{\bf (top left)} with linear SVM {\bf (top right)} in finding the
boundary between oils from the North and Sardinia.  Both boundaries
are too close to the cluster of Northern oils.  We also compare linear
SVM {\bf (bottom left)} and radial kernel SVM {\bf (bottom right)} in
finding the boundary between oils from South Apulia and other Southern
oils.  }
\label{olive-classifly} 
\end{figure*}

\index{R package!\RPackage{classifly}} \index{tour!grand}
\index{tour!manual} We use the R package \RPackage{classifly}
\cite{Wi06} to generate points illustrating boundaries, add those
points to the original data, and display them in GGobi.
Figure~\ref{olive-classifly} shows projections of boundaries between
pairs of classes in the \Data{Olive Oils}.  In each example, we used
the 2D tour with manual control~to focus the view on a projection that
revealed the boundary between two groups.

% Needs to be checked
\begin{verbatim}
> library(classifly)
> d.olive.sub <- subset(d.olive,region!=1,
   select=c(region,palmitic:eicosenoic))
> classifly(d.olive.sub, region~linoleic+oleic+arachidic, 
   lda)
> classifly(d.olive.sub, region~linoleic+oleic+arachidic, 
   svm, probability=TRUE, kernel="linear")
\end{verbatim}

The top two plots show tour projections of the North (purple) and
Sardinia (green) oils where the two classes are separated and the
boundary appears in gray.  The LDA boundary (shown at left) slices too
close to the Northern oils. This might be due to the violation of the
LDA assumption that the two groups have equal variance; since that is
not true here, it places the boundary too close to the group with the
larger variance.  The SVM boundary (at right) is a bit closer to the
Sardinian oils than the LDA boundary is, yet it is still a tad too
close to the oils from the North.

The bottom row of plots examines the more difficult classification of
the areas of the South, focusing on separating the South Apulian oils
(in pink), which is the largest sample, from the oils of the other
areas (all in orange). Perfect separation between the classes does not
occur. Both plots are tour projections showing SVM boundaries, the
left plot generated by a linear kernel and the right one by a radial
kernel.  Recall that the radial kernel was selected automatically by
the SVM software we used, whereas we actually chose to use a linear
kernel.  These pictures illustrate that the linear basis yields a more
reasonable boundary between the two groups. The shape of the clusters
of the two groups is approximately the same, and there is only a small
overlap of the two. The linear boundary fits this structure
neatly. The radial kernel wraps around the South Apulian oils.

\section{Recap}

% Olive oils classification story, how extends to data examples
% 

% Getting a picture of the cluster structure
% Model assessment: 
%       parametric model assumptions     
%       misclassification table
%       forest diagnostics
%       SVM: support vectors
%       boundaries

These partial analyses of the \Data{Italian Olive Oils} demonstrate
that it is possible to get a good mental image of cluster structure in
relation to class identity in high-dimensional space.  This is
possible with many multivariate datasets. Having a good mental image
of the class structure can help with many tasks in a classification
analysis: choosing an appropriate classifier, validating (or
rejecting!) the results of a classification, and simplifying the final
model.

\index{classification strategy}

The \Data{Olive Oils} data has nine classes. Jumping straight into
classifying the oils into nine classes by \Vbl{area} would have led to
dismal results.  Instead, we aggregated areas to form a new class
variable, \Vbl{region}, with only three levels.  That allowed us to
start with the simpler problem of classifying the oils into only three
classes, and then use the hierarchical nature of the classes to
structure the analysis.  This strategy can often be used to
simplify classification problems with many classes: divide and
conquer.

Graphics can be used to check whether the variance--covariance
structure is consistent with a multivariate normal model for classical
classifiers, or whether the separations between groups fall along
single variable axes so that trees can be used effectively. Linked
plots allow us to examine the rich diagnostics provided by random
forests and to explore misclassifications exposed by the
misclassification table. We can see how well the support vectors mark
the separation between classes. It can be surprising to examine the
boundary generated by a classifier, even when it has an extremely low
error rate.

For the \Data{Olive Oils}, we saw that the data has a story to tell:
The olive oils of Italy are remarkably different in composition based
on geographic boundaries.  There is something fishy about the Sicilian
oils in this data, and the most plausible story is that the Sicilian
oils used borrowed olives from neighboring areas. This is interesting!
Data analysis is detective work.

\section*{Exercises}
\addcontentsline{toc}{section}{Exercises}

\begin{enumerate}
\item For the \Data{Flea Beetles}:
\begin{enumerate} 
\item
Generate a scatterplot matrix. Which variables would contribute to
separating the three species?
\item
Generate a parallel coordinate plot.  Characterize the three species
by the pattern of their traces.
\item
Watch the data in a grand tour. Stop the tour when you see a
separation and describe the variables that contribute to the
separation.
\item
Using the projection pursuit guided tour with the holes index, find a
projection that neatly separates all three species. Put the axes onto
the plot, and explain the variables that are contributing to the
separation. Using univariate plots, confirm that these variables are
important to separate species. (Hint: Transform the data into principal
components and enter these variables into the projection pursuit
guided tour running the holes index.)
\end{enumerate}
\item For the \Data{Australian crabs}:
\begin{enumerate}
\item
From univariate plots assess whether any individual variables are good
classifiers of crabs by \Vbl{species} or \Vbl{sex}.
\item From either a scatterplot matrix or pairwise plots, determine
which pairs of variables best distinguish the crabs by \Vbl{Species}
and by \Vbl{sex} within \Vbl{species}.
\item Examine the parallel coordinate plot of the five measured
variables.  Why is a parallel coordinate plot not helpful in determining
the importance of variables for this data?
\item Using Tour1D (and perhaps projection pursuit with the LDA
index), find a 1D projection that mostly separates the crabs by
\Vbl{species}. Report the projection coefficients.
\item Now transform the five measured variables into principal components
and run Tour1D on these new variables. Can you find a better
separation of the crabs by \Vbl{species}?
\end{enumerate}
\item For the \Data{Italian Olive Oils}:
\begin{enumerate}
\item Split the samples from North Italy into $2/3$ training and
$1/3$ test samples for each area.
\item Build a tree model to classify the oils by \Vbl{area} for the
three areas of North Italy. Which are the most important
variables? Make plots of these variables. What is the accuracy of the
model for the training and test sets?
\item Build a random forest to classify oils into the three areas of
North Italy. Compare the order of importance of variables with what
you found from a single tree. Make a parallel coordinate plot in the
order of variable importance.
\item Fit a support vector machine model and a feed-forward neural
network model to classify oils by \Vbl{area} for the three areas of
the North. Using plots, compare the predictions of each point for SVM,
a feed-forward neural network and random forests.
\end{enumerate}
\item For the \Data{TAO} data:
\begin{enumerate}
\item Build a classifier to distinguish between the normal and the El
Ni\~{n}o years. Depending on the classifier you use, you may need to
impute the missing values first.
\item Which variables are important for distinguishing an El Ni\~no 
year from a normal year?
\end{enumerate}
\item For \Data{spam}:
\begin{enumerate}
\item Create a new variable \Vbl{domain.reduced} that reduces the
number of categories of \Vbl{domain} to ``edu,''
``com,''``gov,''``org,'' ``net,'' and ``other.''
\item Using \Vbl{spam} as the class variable, and using explanatory
variables \Vbl{day of week}, \Vbl{time of day}, \Vbl{size.kb},
\Vbl{box}, \Vbl{domain.reduced}, \Vbl{local}, \Vbl{digits},
\Vbl{name}, \Vbl{capct}, \Vbl{special}, \Vbl{credit},
\Vbl{sucker}, \Vbl{porn}, \Vbl{chain}, \Vbl{username}, and
\Vbl{large text}, build a random forest classifier using $mtry=2$.
\item What is the order of importance of the variables?
\item How many non-spam emails are misclassified as spam?
\item Examine a scatterplot of predicted class against actual class,
using jittering to spread the values, and a parallel coordinate plot
of the explanatory variables in the order of importance returned by
the forest. Brush the cases corresponding to non-spam email that has
been predicted to be spam.  Characterize these email messages
(e.g., all from the local box, small number of digits). Now look
at the emails that are spam and correctly classified as spam. Is there
something special about them?
\item Examine the relationship between \Vbl{Spam} (actual class) and
\Vbl{Spam.Prob} (probability of being spam as estimated by Iowa
State University's mail administrators). How many cases that are not 
spam are rated as more than 50\% likely to be spam?
\item Examine the probability rating for cases corresponding to
non-spam that your random forest classified as spam. Write a
description of the email that has the highest probability of being
spam and is also considered to be very likely to be spam by random
forests.
\item Which user has the highest proportion of non-spam email classified
as spam?
\item Based on your exploration of this data, which variables would
you suggest are the most important in determining if an email 
message is spam?
\end{enumerate}
\item In this exercise, your goal is to build a classifier for the
\Data{music} data that will distinguish between rock and classical
tracks.
\begin{enumerate}
\item This data has 70 explanatory variables for 62 samples.  Reduce
the number of variables to fewer than 10, choosing those that are the
most suitable candidates on which to build a classifier. (Hint: One of
the problems to consider is that there are several missing values. It
might be possible to reduce the number of variables in a way that also
fixes the missing values problem.)
\item Split the data into two samples, with 2/3 of the data in the
training sample and 1/3 in the test sample.  Report which cases are in
each sample.
\item Build your best classifier for distinguishing rock from
classical tracks.
\item Predict the five new tracks as either rock or classical.
\end{enumerate}
\end{enumerate}
