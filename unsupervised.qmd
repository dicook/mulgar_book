# Unsupervised learning

Unsupervised classification, or cluster analysis, organizes observations into similar groups. Cluster analysis is a commonly used, appealing, and conceptually intuitive statistical method. Some of its uses include
market segmentation, where customers are grouped into clusters with similar attributes for targeted marketing; gene expression analysis, where genes with similar expression patterns are grouped together; and the creation of taxonomies for animals, insects, or plants. Clustering can be used as a way of reducing a massive amount of data because observations within a cluster can be summarized by its centre. Also, clustering effectively subsets the data thus simplifying analysis because observations in each cluster can be analyzed separately.

Organizing objects into groups is a task that comes naturally to humans, even to small children. Perhaps this is why it is an appealing method of data analysis. However, cluster analysis is more complex than it
initially appears. Many people imagine that it will produce neatly separated clusters like those in the top left plot of @fig-ideal-clusters, but it almost never does. Such ideal clusters are rarely encountered in real data, so we often need to modify our objective from
*find the natural clusters in this data*. Instead, we need to organize the *cases into groups that are similar in some way*. Even though this may seem disappointing when compared with the ideal, it is still often
an effective means of simplifying and understanding a dataset.

```{r}
#| label: fig-ideal-clusters
#| echo: FALSE
#| fig-width: 5
#| fig-height: 5
#| out-width: "100%"
#| fig-cap: "Different structures in data and their impact on cluster analysis.  When there are well-separated groups (top left), it is simple to group similar observations. Even when there are not (top right), partitioning observations into groups may still be useful. There may be nuisance variables that do not contribute to the clustering (bottom left), and there may oddly shaped clusters (bottom right)."
d.separated.clusters <- matrix(rnorm(99*2), ncol=2)
d.separated.clusters[1:33,1] <-
  d.separated.clusters[1:33,1]+8
d.separated.clusters[34:66,2] <-
  d.separated.clusters[34:66,2]+8
d.separated.clusters[34:66,1] <-
  d.separated.clusters[34:66,1]+4
d.unseparated.clusters <- matrix(rnorm(99*2), ncol=2)
vc <- matrix(c(1,0.6,0.6,1),ncol=2,byrow=T)
d.unseparated.clusters <- d.unseparated.clusters%*%vc
d.nuisance <- matrix(rnorm(99*2), ncol=2)
d.nuisance[1:49,1] <- d.nuisance[1:49,1]+8
d.odd.shapes <- matrix(rnorm(99*2),ncol=2)
d.odd.shapes[1:66,2] <- (d.odd.shapes[1:66,1])^2-5 + rnorm(66)*0.6
d.odd.shapes[1:66,1] <- d.odd.shapes[1:66,1]*3

d.separated.clusters <-
  data.frame(d.separated.clusters)
d.unseparated.clusters <-
  data.frame(d.unseparated.clusters)
d.nuisance <-
  data.frame(d.nuisance)
d.odd.shapes <-
  data.frame(d.odd.shapes)
library(ggplot2)
library(patchwork)
p1 <- ggplot(d.separated.clusters, aes(x=X1, y=X2)) + 
  geom_point() + 
  theme(aspect.ratio=1,
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        panel.background = element_rect("white"),
        panel.border = element_rect("black", fill=NA, 
             linewidth = 0.5))
p2 <- ggplot(d.unseparated.clusters, aes(x=X1, y=X2)) + 
  geom_point() + 
  theme(aspect.ratio=1,
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        panel.background = element_rect("white"),
        panel.border = element_rect("black", fill=NA, 
             linewidth = 0.5))
p3 <- ggplot(d.nuisance, aes(x=X1, y=X2)) + 
  geom_point() + 
  theme(aspect.ratio=1,
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        panel.background = element_rect("white"),
        panel.border = element_rect("black", fill=NA, 
             linewidth = 0.5))
p4 <- ggplot(d.odd.shapes, aes(x=X1, y=X2)) + 
  geom_point() + 
  theme(aspect.ratio=1,
        axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        panel.background = element_rect("white"),
        panel.border = element_rect("black", fill=NA, 
             linewidth = 0.5))
print(p1 + p2 + p3 + p4 + plot_layout(ncol=2))
```

At the heart of the clustering process is the work of discovering which variables are most important for defining the groups. It is often true that we only require a subset of the variables for finding clusters,
whereas another subset (called \Term{nuisance variables}) has no impact. In the bottom left plot of @fig-ideal-clusters, it is clear that the variable plotted horizontally is important for splitting this data into two clusters, whereas the variable plotted vertically is a nuisance variable. Nuisance is an apt term for these variables, because they can radically change the interpoint distances and impair the clustering
process. \index{cluster analysis!interpoint distance}
\index{cluster analysis!nuisance variable}

Dynamic graphical methods help us to find and understand the cluster structure in high dimensions. With the tools in our toolbox, primarily tours, along with linked scatterplots and parallel coordinate plots, we can see clusters in high-dimensional spaces. We can detect gaps between clusters, the shape and relative positions of clusters, and the presence of nuisance variables. We can even find unusually shaped clusters, like those in the bottom right plot in @fig-ideal-clusters. In simple
situations we can use graphics alone to group observations into clusters, using a "spin and brush" method. In more difficult data problems, we can assess and refine numerical solutions using graphics.\index{brushing!persistent}
\index{cluster analysis!spin and brush}

This part of the book discusses the use of interactive and dynamic graphics in the clustering of data. @sec-clust-bg introduces cluster analysis, focusing on interpoint distance measures. @sec-clust-graphics
describes an example of a purely graphical approach to cluster analysis, the spin and brush method. In the example shown in that section, we were able to find simplifications of the data that had not been found using
numerical clustering methods, and to find a variety of structures in high-dimensional space. @sec-hclust describes methods for reducing the interpoint distance matrix to an intercluster distance matrix using
hierarchical algorithms, @sec-mclust covers model-based clustering, and @sec-som described clustering with self-organising maps. Each of these chapters shows how graphical tools can be used to assess the results of
numerical methods. @sec-clust-compare summarizes the chapter and revisits the data analysis strategies used in the examples. Additional references that provide good companions to the material presented in these chapters are @VR02, @HOML, @hennig, @giordani, @kassambara, and the CRAN Task View [@ctv-clustering]. @sec-clust-compare summarizes the chapter and revisits the data analysis strategies used in the examples.

## Background {#sec-clust-bg}

Before we can begin finding groups of cases that are similar, we need to decide on a definition of similarity. How is similarity defined? Consider a dataset with three cases $(a_1, a_2, a_3)$ and four variables $(V_1, V_2, V_3, V_4)$, described in matrix format as

::: {.hidden}
$$
\require{mathtools}
\definecolor{grey}{RGB}{192, 192, 192}
$$
:::

\begin{align*}
X = \begin{bmatrix}
& {\color{grey} V_1} & {\color{grey} V_2} & {\color{grey} V_3} & {\color{grey} V_4} \\\hline
{\color{grey} a_1} | & x_{11} & x_{12} & x_{13} & x_{14} \\
{\color{grey} a_2} | & x_{21} & x_{22} & x_{23} & x_{24} \\
{\color{grey} a_3} | & x_{31} & x_{32} & x_{33} & x_{34}    
\end{bmatrix}
=  \begin{bmatrix}
& {\color{grey} V_1} & {\color{grey} V_2} & {\color{grey} V_3} & {\color{grey} V_4} \\\hline
{\color{grey} a_1} | & 7.3 & 7.6 & 7.7 & 8.0 \\
{\color{grey} a_2} | & 7.4 & 7.2 & 7.3 & 7.2 \\
{\color{grey} a_3} | & 4.1 & 4.6 & 4.6 & 4.8 
\end{bmatrix}

\end{align*}


\noindent which is plotted in @fig-similarity1. The Euclidean distance
between two cases (rows of the matrix) with $p$ elements is defined as

\begin{align*}
d_{\rm Euc}(a_i,a_j) &=& ||a_i-a_j|| %\\
% &=& \sqrt{(x_{i1}-x_{j1})^2+\dots + (x_{ip}-x_{jp})^2},
~~~~~~i,j=1,\dots, n,
\end{align*}

\noindent where $||x_i||=\sqrt{x_{i1}^2+x_{i2}^2+\dots +x_{ip}^2}$. For
example, the Euclidean distance between cases 1 and 2 in the above data,
is

\begin{align*}
d_{\rm Euc}(a_1,a_2) &= \sqrt{(7.3-7.4)^2+(7.6-7.2)^2+ (7.7-7.3)^2+(8.0-7.2)^2} \\
&= 1.0 
\end{align*}

\index{cluster analysis!interpoint distance}

\noindent For the three cases, the interpoint Euclidean distance matrix
is

\begin{align*}
d_{\rm Euc} =
\left[ \begin{array}{ccc}
 0.0  ~&     &   \\ 
 1.0 ~&  0.0 ~  &  \\
 6.3 ~& 5.5 ~&  0.0 ~ \\
\end{array} \right]
\begin{array}{r}
a_1 \\ a_2 \\ a_3 \\
\end{array}
\end{align*}

::: {#fig-similarity1 layout-ncol=2}

```{r}
#| message: FALSE
#| fig-width: 4
#| fig-height: 4
x <- tibble::tibble(V1 = c(7.3, 7.4, 4.1),
                    V2 = c(7.6, 7.2, 4.6),
                    V3 = c(7.7, 7.3, 4.6),
                    V4 = c(8.0, 7.2, 4.8),
                    point = factor(c("a1", "a2", "a3")))
library(GGally)
library(colorspace)
library(gridExtra)
pscat <- ggpairs(x, columns=1:4,
                 upper=list(continuous="points"),
                 diag=list(continuous="blankDiag"),
                 axisLabels="internal",
                 ggplot2::aes(colour=point)) +
    scale_colour_discrete_qualitative(
      palette = "Dark 3") +
    theme(aspect.ratio=1)
pscat
```
```{r}
#| fig-width: 3
#| fig-height: 3.4
ppar <- ggparcoord(x, columns=1:4, 
                   groupColumn = 5, 
                   scale = "globalminmax") +
          scale_colour_discrete_qualitative(
            palette = "Dark 3") +
  xlab("") + ylab("") + 
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank(),
        legend.title = element_blank())
ppar
```

The scatterplot matrix (left) shows that cases $a_1$ and $a_2$ have similar values.  The parallel coordinate plot (right) allows a comparison of other structure, which shows the similarity in the trend of the profiles on cases $a_1$ and $a_3$. 
:::


\noindent Cases $a_1$ and $a_2$ are more similar to each other than they are to case $a_3$, because the Euclidean distance between cases $a_1$ and $a_2$ is much smaller than the distance between cases $a_1$ and $a_3$ and between cases $a_2$ and $a_3$.

There are many different ways to calculate similarity. Similarity measures based on correlation distance have become common. Correlation distance is typically used where similarity of structure is more important than similarity in magnitude.

\index{parallel coordinate plot}

As an example, see the parallel coordinate plot of the sample data at the right of @fig-similarity1. Cases $a_1$ and $a_3$ are widely separated, but their shapes are similar (low, medium, medium, high). Case $a_2$, although
overlapping with Case $a_1$, has a very different shape (high, medium, medium, low). The correlation between two cases is defined as

\begin{align*}
\rho(a_i,a_j) = \frac{(a_i-c_i)'(a_j-c_j)}
{\sqrt{(a_i-c_i)'(a_i-c_i)} \sqrt{(a_j-c_j)'(a_j-c_j)}}
\label{corc}
\end{align*}

\noindent When $c_i, c_j$ are the sample means $\bar{a}_i,\bar{a}_j$, then $\rho$ is the Pearson correlation coefficient. If, indeed, they are
set at 0, as is commonly done, $\rho$ is a generalized correlation that describes the angle between the two data vectors. The correlation is then converted to a distance metric; one equation for doing so is as follows:


\begin{align*}
d_{\rm Cor}(a_i,a_j) = \sqrt{2(1-\rho(a_i,a_j))}
\end{align*}

The above distance metric will treat cases that are strongly negatively correlated as the most distant.

The interpoint distance matrix for the sample data using $d_{\rm Cor}$ and the Pearson correlation coefficient is

\begin{align*}
d_{\rm Cor} = 
\left[ \begin{array}{rrrrrrrrr}
 0.0  ~&     &  \\
 3.6 ~ & 0.0 ~ &  \\
 0.1 ~ & 3.8 ~ &  0.0 ~\\
\end{array} \right]
\begin{array}{r}
a_1 \\ a_2 \\ a_3 \\
\end{array}
\end{align*} 

\noindent By this metric, cases $a_1$ and $a_3$ are the most similar, because the correlation distance is smaller between these two cases than the other pairs of cases. \index{cluster analysis!interpoint distance}

Note that these interpoint distances differ dramatically from those for Euclidean distance. As a consequence, the way the cases would be clustered is also be very different. Choosing the appropriate distance
measure is an important part of a cluster analysis.

After a distance metric has been chosen and a cluster analysis has been performed, the analyst must evaluate the results, and this is actually a difficult task. A cluster analysis does not generate $p$-values or other
numerical criteria, and the process tends to produce hypotheses rather than testing them. Even the most determined attempts to produce the "best" results using modeling and validation techniques may result
in clusters that, although seemingly significant, are useless for practical purposes. As a result, cluster analysis is best thought of as an exploratory technique, and it can be quite useful despite the lack of formal validation because of its power in data simplification.

The context in which the data arises is the key to assessing the results. If the clusters can be characterized in a sensible manner, and they increase our knowledge of the data, then we are on the right track.
To use an even more pragmatic criterion, if a company can gain an economic advantage by using a particular clustering method to carve up their customer database, then that is the method they should use.

### Purely graphics {#sec-clust-graphics}

\index{brushing!persistent} \index{tour}
\index{cluster analysis!spin and brush}

A purely graphical spin and brush approach to cluster analysis works
well when there are good separations between groups, even when there are
marked differences in variance structures between groups or when groups
have non-linear boundaries. It does not work very well when there are
clusters that overlap, or when there are no distinct clusters but rather
we simply wish to partition the data. In these situations it may be
better to begin with a numerical solution and to use visual tools to
evaluate it, perhaps making refinements subsequently. Several examples
of the spin and brush approach are documented in the literature, such as
@CBCH95 and @WWS99.

\index{datasets!\Data{PRIM7}}

This description of the spin and brush approach on \Data{PRIM7}, a
particle physics dataset, follows that in \citeasnoun{CBCH95}. The data
contains seven variables. We have no labels for the data, so when we
begin, all the points have the same color and glyph. Watch the data in a
tour for a few minutes and you will see that there are no natural
clusters, but there is clearly structure.

\index{projection pursuit!indexes}
\index{projection pursuit!indexes!holes}
\index{projection pursuit!indexes!central mass}
\index{principal component analysis}

We will use the projection pursuit guided tour to help us find that
structure. We will tour on the principal components, rather than the raw
variables, because that improves the performance of the projection
pursuit indexes. Two indexes are useful for detecting clusters: holes
and central mass. The holes index is sensitive to projections where
there are few points (i.e., a hole) in the center. The central mass
index is the opposite: It is sensitive to projections that have too many
points in the center. These indexes are explained in @chap-toolbox.

The holes index is usually the most useful for clustering, but not for
the particle physics data, because it does not have a \`\`hole'' at the
center. The central mass index is the most appropriate here. Alternate
between optimization (a guided tour) and the unguided grand tour to find
local maxima, each of which is a projection that is potentially useful
for revealing clusters. The process is illustrated in @fig-prim7-tour.

The top left plot shows the initial default projection, the second
principal component plotted against the first. The plot next to it shows
the projected data corresponding to the first local maximum found by the
guided tour. It has three strands of points stretching out from the
central clump and several outliers. We brush the points along each
strand, in red, blue, and orange, and we paint the outliers with open
circles. (See the next two plots.) We continue by choosing a new random
start for the guided tour, and then waiting until new territory in the
data is discovered. \index{brushing!persistent}

The optimization settles on a projection where there are three strands
visible, as observed in the leftmost plot in the second row. Two strands
have been previously brushed, but a new one has appeared; this is
painted yellow.

We also notice that there is another new strand hidden below the red
strand. It is barely distinguishable from the red strand in this
projection, but the two strands separate widely in other projections. It
is tricky to brush it, because it is not well separated in this
projection. We use a trick: Hide the red points, brush the new strand
green, and \`\`unhide'' the red points again (middle plot in the second
row).

Five clusters have been easily identified, and now finding new clusters
in this data is increasingly difficult. After several more alternations
between the grand tour and the guided tour, we find something new (shown
in the rightmost plot in the second row): One more strand has emerged,
and we paint it pink.

```{=html}
<!-- % Figure 3
\begin{figure}[htp]
\centerline{{\includegraphics[width=1.5in]{chap-clust/prim7-pp1.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/prim7-pp2.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/prim7-pp5.pdf}}}
\smallskip
\centerline{{\includegraphics[width=1.5in]{chap-clust/prim7-pp7.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/prim7-pp8.pdf}}
 {\includegraphics[width=1.5in]{chap-clust/prim7-pp9.pdf}}}
\centerline{{\includegraphics[width=1.5in]{chap-clust/prim7-pp10.pdf}}
  {\includegraphics[width=1.5in]{chap-clust/prim7-pp11.pdf}}
  {\includegraphics[width=1.5in]{chap-clust/prim7-pp13.pdf}}}
\caption[Stages of ``spin and brush'' on \Data{PRIM7}]{Stages of spin
and brush on \Data{PRIM7}.  The high-dimensional geometry emerges as
the clusters are painted.}
\label{prim7-tour}
\end{figure}
-->
```
The results at this stage are summarized by the bottom row of plots.
There is a very visible triangular component (in gray) revealed when all
of the colored points are hidden. We check the shape of this cluster by
drawing lines between outer points to contain the inner ones. Touring
after the lines are drawn helps to check how well they match the shape
of the clusters. The colored groups pair up at each vertex, and we draw
in the shape of these too --- a single line matches the structures
reasonably well.

The final step of the spin and brush clustering is to clean up this
solution, touching up the color groups by continuing to tour, and
repainting a point here and there. When we finish, we have found seven
clusters in this data that form a very strong geometric object in the
data space: a two-dimensional (2D) triangle, with two one-dimensional
(1D) strands extending in different directions from each vertex. The
lines confirm our understanding of this object's shape, because the
points stay close to the lines in all of the projections observed in a
tour.

```{=html}
<!-- % Figure 4
\begin{figure}
\centerline{
   \includegraphics[width=1.5in]{chap-clust/prim7-pp13-model.pdf}
   \includegraphics[width=3in]{chap-clust/prim7-par.pdf}
}
\caption[The \Data{PRIM7} model summarized]{The \Data{PRIM7} model
summarized.  The model summary {\bf (left)} was formed by adding line
segments manually.  In the parallel coordinate plot, the profiles
highlighted in dark gray correspond to the points in the 2D triangle
at the center of the model.  }
\label{prim7-model}
\end{figure}
-->
```
The next stage of cluster analysis is to characterize the nature of the
clusters. To do that, we would calculate summary statistics for each
cluster, and plot the clusters (@fig-prim7-model). When we plot the
clusters of the particle physics data, we find that the 2D triangle
exists primarily in the plane defined by X3 and X5. If you do the same,
notice that the variance in measurements for the gray group is large in
variables X3 and X5, but negligible in the other variables. The linear
pieces can also be characterized by their distributions on each of the
variables. With this example, we have shown that it is possible to
uncover very unusual clusters in data without any domain knowledge.

Here are several tips about the spin and brush approach.

-   Save the dataset frequently during the exploration of a complex
    dataset, being sure to save your colors and glyphs, because it may
    take several sessions to arrive at a final clustering.\
-   Manual controls are useful for refining the optimal projection
    because another projection in the neighborhood may be more
    revealing.\
-   The holes index is usually the most successful projection pursuit
    index for finding clusters.
-   Principal component coordinates may provide a better starting point
    than the raw variables.

Finally, the spin and brush method will not work well if there are no
clear separations in the data, and the clusters are high-dimensional,
unlike the low-dimensional clusters found in this example.
