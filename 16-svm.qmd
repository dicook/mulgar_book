# Support vector machines
\index{classification!support vector machines (SVM)}

A support vector machine (SVM) [@Va99] looks for gaps between clusters in the data, based on the extreme observations in each class. In this sense it mirrors the graphical approach described @sec-clust-graphics, in which we searched for gaps between groups. It can be viewed as similar to LDA, in that the boundary between classes is a hyperplane.  The difference between LDA and SVM is the placement of the boundary. LDA uses the means and covariance matrices of the classes to place the boundary, but SVM uses extreme observations.

::: {.content-visible when-format="html"}
::: info
The key elements of the SVM model to extract are:

- support vectors
- separating hyperplane.

:::
:::

::: {.content-visible when-format="pdf"}
\infobox{
The key elements of the SVM model to extract are:
\begin{itemize} \itemsep 0in
\item support vectors
\item separating hyperplane.
\end{itemize}
}
:::

SVM is widely used for it's ability to fit non-linear classification models in a simple fashion using kernels in the boundary equation. We are focusing on linear methods here because it makes for a useful comparison with how the models differ from those provided by SVM. SVM tends to place the boundary between groups in a gap, if it exists. This is nice from a visual perspective because when we look at differences between classes using a tour, we naturally focus on the gaps. SVM better fits this perception than LDA. 

Non-linear SVM models are interesting to examine also. Mostly one would examine the boundaries between classes which can be done in the same way that is documented in the @sec-lda and @sec-trees-forests.

## Components of the SVM model

To illustrate the approach, we use two simple simulated data examples. Both have only two variables, and two classes. Explaining SVM is easier when there are just two groups. In the first data set the two classes have different covariances matrices, which will cause trouble for LDA, but SVM should see the gap between the two clusters and place the separating hyperplane in the middle of the gap. In the second data set the two groups are concentric circles, with the inner one solid. A non-linear SVM should be fitted to this data, which should see circular gap between the two classes. 

Note that the `svm` function in the `e1071` package will automatically scale observations into the range $[0,1]$. To make it easier to examine the fitted model, it is best to scale your data first, and then fit the model.

```{r echo=knitr::is_html_output()}
#| code-summary: "Code to simulate data examples"
# Toy examples
library(mulgar)
library(ggplot2)
library(geozoo)
library(tourr)

set.seed(1071)
n1 <- 162
vc1 <- matrix(c(1, -0.7, -0.7, 1), ncol=2, byrow=TRUE)
c1 <- rmvn(n=n1, p=2, mn=c(-2, -2), vc=vc1)
vc2 <- matrix(c(1, -0.4, -0.4, 1)*2, ncol=2, byrow=TRUE)
n2 <- 138
c2 <- rmvn(n=n2, p=2, mn=c(2, 2), vc=vc2)
df1 <- data.frame(x1=mulgar:::scale2(c(c1[,1], c2[,1])), 
                 x2=mulgar:::scale2(c(c1[,2], c2[,2])), 
                 cl = factor(c(rep("A", n1), 
                               rep("B", n2))))
c1 <- sphere.hollow(p=2, n=n1)$points*3 + 
  c(rnorm(n1, sd=0.3), rnorm(n1, sd=0.3))
c2 <- sphere.solid.random(p=2, n=n2)$points
df2 <- data.frame(x1=mulgar:::scale2(c(c1[,1], c2[,1])), 
                  x2=mulgar:::scale2(c(c1[,2], c2[,2])), 
                  cl = factor(c(rep("A", n1), 
                               rep("B", n2))))
```

```{r}
#| code-fold: false
#| message: false
library(classifly)
library(e1071)
df1_svm <- svm(cl~., data=df1, 
                     probability=TRUE, 
                     kernel="linear", 
               scale=FALSE)
df1_svm_e <- explore(df1_svm, df1)

df2_svm <- svm(cl~., data=df2,  
                     probability=TRUE, 
                     kernel="radial")
df2_svm_e <- explore(df2_svm, df2)
```

```{r echo=knitr::is_html_output()}
#| label: fig-svm-toy
#| fig-cap: "SVM classifier fit overlaid on two simulated data examples: (a) groups with different variance-covariance, fitted using a linear kernel, (b) groups with non-linear separation, fitted using a radial kernel. The band of points shown as '+' mark the SVM boundary, and points marked by 'x' are the support vectors used to define the boundary. "
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%
#| code-summary: "Code to make plots"
library(patchwork)
library(colorspace)
s1 <- ggplot() + 
  geom_point(data=df1, aes(x=x1, y=x2, colour=cl),
             shape=20) +
  scale_colour_discrete_divergingx(palette="Zissou 1") +
  geom_point(data=df1_svm_e[(!df1_svm_e$.BOUNDARY)&(df1_svm_e$.TYPE=="simulated"),], 
             aes(x=x1, y=x2, colour=cl), shape=3) +
  geom_point(data=df1[df1_svm$index,], 
             aes(x=x1, y=x2, colour=cl), 
             shape=4, size=4) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position = "none") +
  ggtitle("(a)")

s2 <- ggplot() + 
  geom_point(data=df2, aes(x=x1, y=x2, colour=cl), shape=20) +
  scale_colour_discrete_divergingx(palette="Zissou 1") +
  geom_point(data=df2_svm_e[(!df2_svm_e$.BOUNDARY)&(df2_svm_e$.TYPE=="simulated"),], 
             aes(x=x1, y=x2, colour=cl), 
             shape=3) +
  geom_point(data=df2[df2_svm$index,], 
             aes(x=x1, y=x2, colour=cl), 
             shape=4, size=4) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position = "none") +
  ggtitle("(b)")

s1+s2
```

@fig-svm-toy shows the two data sets and the important aspects of the fitted SVM model for each. The observations are represented by dots, the separating hyperplane (just a line for 2D) is represented by '+'. Where the two colours merge is the actual location of the boundary between classes. It can be seen that this is located right down the middle of the gap, for both data sets. Even though the boundary is circular for the second data set, in a transformed high-dimensional space it would be linear.

SVMs use a subset of the observations to define the boundary, and these are called the support vectors. For each of the data sets these are marked with 'x'. For the linear boundary, there are nine support vectors, five in one group and four in the other. There is one interesting observation in the red group, which falls on the other side of the boundary. It is marked as a support vector, but its contribution to the fitted hyperplane is limited by a control parameter in the model fitting process. 

Linear SVMs can be assessed similarly to regression models. The components of the model are:

1. The points that are the support vectors:

```{r}
#| code-fold: false
df1_svm$index
```

2. Their coefficients:

```{r}
#| code-fold: false
df1_svm$coefs
```

which indicate that all but 15, 45 and 180 are actually bounded support vectors (their coefficients are bounded to magnitude 1). 

3. that when used with the intercept:

```{r}
#| code-fold: false
df1_svm$rho
```

can be used to compute the equation of the fitted hyperplane. 

```{r}
#| code-fold: false
w = t(df1_svm$SV) %*% df1_svm$coefs
w
```

Giving the equation to be `r round(w[1], 2)` $x_1 +$ `r round(w[2], 2)` $x_2 +$ `r round(-df1_svm$rho, 2)` $=0$, or alternatively, $x_2 =$ `r round(-w[1]/w[2], 2)` $x_1 +$ `r round(df1_svm$rho/w[2], 2)`.

which can be used to generate a line to show the boundary with the data. 

```{r}
#| eval: false
#| code-fold: false
s1 + geom_abline(intercept=df1_svm$rho/w[2],
                 slope=-w[1]/w[2])
```

**Note that** care in scaling of data is important to get the intercept calculated exactly. We have standardised the data, and set the `scale=FALSE` parameter in the `svm` function. The slope calculation is quite robust to the data scaling.

::: {.content-visible when-format="html"}
::: info
Like LDA, a linear SVM model for two groups can be written using the equation of a hyperplane. The fitted model coefficients are then used to generate points on this plane, to examine the boundary between groups.
:::
:::

::: {.content-visible when-format="pdf"}
\infobox{Like LDA, a linear SVM model for two groups can be written using the equation of a hyperplane. The fitted model coefficients are then used to generate points on this plane, to examine the boundary between groups.
}
:::

## Examining the model components in high-dimensions

For higher dimensions, the procedures are similar, with the hyperplane and support vectors being examined using a tour. Here we examine the model for differentiating male and female Chinstrap penguins. The Chinstrap penguins have a noticeable difference in size of the sexes, unlike the other two species. Working with a two-class problem is easier for explaining SVM, but multi-class calculations can also follow this approach.

```{r}
#| code-fold: false
#| warning: false
#| message: false
library(dplyr)
load("data/penguins_sub.rda")
chinstrap <- penguins_sub %>%
  filter(species == "Chinstrap") %>%
  select(-species) %>%
  mutate_if(is.numeric, mulgar:::scale2)
chinstrap_svm <- svm(sex~., data=chinstrap, 
                     kernel="linear",
                     probability=TRUE, 
                     scale=FALSE)
chinstrap_svm_e <- explore(chinstrap_svm, chinstrap)
```

```{r echo=knitr::is_html_output()}
#| eval: false
#| code-summary: "Code to make the tours"
# Tour raw data
animate_xy(chinstrap[,1:4], col=chinstrap$sex)
# Add all SVs, including bounded
c_pch <- rep(20, nrow(chinstrap))
c_pch[chinstrap_svm$index] <- 4
animate_xy(chinstrap[,1:4], col=chinstrap$sex, pch=c_pch)
# Only show the SVs with |coefs| < 1
c_pch <- rep(20, nrow(chinstrap))
c_pch[chinstrap_svm$index[abs(chinstrap_svm$coefs)<1]] <- 4
c_cex <- rep(1, nrow(chinstrap))
c_cex[chinstrap_svm$index[abs(chinstrap_svm$coefs)<1]] <- 2
animate_xy(chinstrap[,1:4], col=chinstrap$sex, 
           pch=c_pch, cex=c_cex)
render_gif(chinstrap[,1:4],
           grand_tour(),
           display_xy(col=chinstrap$sex, pch=c_pch, cex=c_cex),
           gif_file="gifs/chinstrap_svs.gif",
           width=400,
           height=400,
           frames=500)

# Tour the separating hyperplane also
symbols <- c(3, 20)
c_pch <- symbols[as.numeric(chinstrap_svm_e$.TYPE[!chinstrap_svm_e$.BOUNDARY])]
animate_xy(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4], 
           col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY],
           pch=c_pch)
render_gif(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4],
           grand_tour(),
           display_xy(col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY], pch=c_pch),
           gif_file="gifs/chinstrap_svm.gif",
           width=400,
           height=400,
           frames=500)
```

::: {.content-visible when-format="html"}
::: {#fig-p-svm-html layout-ncol=2}

![Support vectors](gifs/chinstrap_svs.gif){#fig-chinstrap-svs fig-alt="FIX ME" width=300}

![SVM boundary](gifs/chinstrap_svm.gif){#fig-chinstrap-svm fig-alt="FIX ME" width=300}

SVM model for distinguishing the sexes of the Chinstrap penguins. The separating hyperplane is 3D, and separates primarily on variables `bl` and `bd`, as seen because these two axes extend out from the plane when it is seen on its side, separating the two groups.
:::
:::

::: {.content-visible when-format="pdf"}
::: {#fig-p-svm-pdf layout-ncol=2}

![Support vectors](images/chinstrap_svs.png){#fig-chinstrap-svs fig-alt="FIX ME" width=200}

![SVM boundary](images/chinstrap_svm.png){#fig-chinstrap-svm fig-alt="FIX ME" width=200}


SVM model for distinguishing the sexes of the Chinstrap penguins. The separating hyperplane is 3D, and separates primarily on variables `bl` and `bd`, as seen because these two axes extend out from the plane when it is seen on its side, separating the two groups.
:::
:::
\index{classification!separating hyperplane}
\index{classification!support vectors}

::: {.content-visible when-format="html"}
::: info
Mark the support vectors by point shape, and examine where these are relative to the difference between groups. 
:::
:::

::: {.content-visible when-format="pdf"}
\infobox{Mark the support vectors by point shape, and examine where these are relative to the difference between groups.
}
:::


Examining the hyperplane in a grand tour display (`r ifelse(knitr::is_html_output(), '@fig-p-svm-html', '@fig-p-svm-pdf')`) indicates that two of the variables, `bl` and `bd`, are important for separating the two classes. We can check this interpretation using the radial tour. Using the components from the model, the coefficients of the hyperplane are: 


```{r}
#| code-fold: false
t(chinstrap_svm$SV) %*% chinstrap_svm$coefs
```

The coefficients for `bl` and `bd` are the largest (in magnitude) which supports the the interpretation that they are most important. This vector can be used to set the starting point for radial tour, once it is normalised. Any orthonormal vector serves to turn this into a 2D projection, to visualise the boundary. 


```{r}
#| code-fold: false
set.seed(1022)
prj1 <- mulgar::norm_vec(t(chinstrap_svm$SV) %*%
                           chinstrap_svm$coefs)
prj2 <- basis_random(4, 1)
prj <- orthonormalise(cbind(prj1, prj2))
prj
```

```{r echo=knitr::is_html_output()}
#| eval: false
#| code-summary: "Code to conduct the radial tours"
animate_xy(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4], 
           tour_path = radial_tour(start=prj, mvar = 2),
           col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY],
           pch=c_pch)
render_gif(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4],
           radial_tour(start=prj, mvar = 2),
           display_xy(col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY], pch=c_pch),
           gif_file="gifs/chinstrap_rad_bd.gif",
           apf = 1/30,
           width=400,
           height=400,
           frames=500)
render_gif(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4],
           radial_tour(start=prj, mvar = 1),
           display_xy(col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY], pch=c_pch),
           gif_file="gifs/chinstrap_rad_bl.gif",
           apf = 1/30,
           width=400,
           height=400,
           frames=500)
render_gif(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4],
           radial_tour(start=prj, mvar = 3),
           display_xy(col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY], pch=c_pch),
           gif_file="gifs/chinstrap_rad_fl.gif",
           apf = 1/30,
           width=400,
           height=400,
           frames=500)
render_gif(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4],
           radial_tour(start=prj, mvar = 4),
           display_xy(col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY], pch=c_pch),
           gif_file="gifs/chinstrap_rad_bm.gif",
           apf = 1/30,
           width=400,
           height=400,
           frames=500)
```

This projection is show in `r ifelse(knitr::is_html_output(), '@fig-chinstrap-radial-html', '@fig-chinstrap-radial-pdf')`. You can see the boundary between the two sexes as a clear line, marked by a sample of points on either side. We use the radial tour to remove each of the variables from the projection using the radial tour to examine it's importance on the model, and hence the boundary. If the clear view of the boundary gets jumbled when a variable is removed we infer that this variable is very important for the model (as seen for `bl` and `bd`). If there is little change in the clarity when a variable is removed, then it is less important (as seen for `fl` and `bm`). 
\index{tour!radial}

::: {.content-visible when-format="html"}
::: {#fig-chinstrap-radial-html layout-ncol=2}

![bl](gifs/chinstrap_rad_bl.gif){#fig-chinstrap-radial-bl}

![bd](gifs/chinstrap_rad_bd.gif){#fig-chinstrap-radial-bd}

![fl](gifs/chinstrap_rad_fl.gif){#fig-chinstrap-radial-fl}

![bm](gifs/chinstrap_rad_bm.gif){#fig-chinstrap-radial-bm}

Exploring the importance of the four variables to the separating hyperplane using a radial tour where the contribution of each variable is reduced to 0, and then increased to it's original value. You can see that `bl` and `bd` contribute most to the plane, because when they are removed the plane is no longer on it side marking the boundary. Variables `fl` and `bm` contribute a small amount to the separating hyperplane, but it is possible that these two could be removed without affecting the strength of the separation between the sexes. 
:::
:::

::: {.content-visible when-format="pdf"}
::: {#fig-chinstrap-radial-pdf layout-ncol=2}

![bl in](images/chinstrap_rad_bl1.png){#fig-chinstrap-radial-bl1 width=200}

![bl reduced](images/chinstrap_rad_bl2.png){#fig-chinstrap-radial-bl2 width=200}

![bd reduced](images/chinstrap_rad_bd.png){#fig-chinstrap-radial-bd width=200}

![bm out](images/chinstrap_rad_bm.png){#fig-chinstrap-radial-bm width=200}


Exploring the importance of the four variables to the separating hyperplane using a radial tour to reduce the contribution of each variable to 0, and then back to it's original value: (a) separating hyperplane visible, (b) `bl` contribution reduced, (c) `bd` contribution decreased, (d) `bm` contribution removed. You can see that `bl` and `bd` contribute most to the plane, because when they are removed the plane is no longer on it side marking the boundary. Variables `fl` (not shown) and `bm` contribute a small amount to the separating hyperplane, but it is possible that these two could be removed with only a small effect on the strength of the separation between the sexes. 
:::
:::


::: {.content-visible when-format="html"}
::: info
Use a radial tour to zero out coefficients defining the separating hyperplane to explore the variable importance. 
:::
:::

::: {.content-visible when-format="pdf"}
\infobox{Use a radial tour to zero out coefficients defining the separating hyperplane to explore the variable importance. 
}
:::

In this example, we can see that clarity of the boundary changes substantially when either `bl` and `bd` are removed. There is a small change when `fl` and `bm` are removed, so they are less important. This interpretation matches the interpretation that would be made from the magnitude of the coefficients of the hyperplane (printed earlier). They reinforce each other. It is possible that the interpretation of the coefficients could differ after using the radial tour, most likely in terms of simplifying the vector, supporting the forcing some coefficients to zero.  

::: {.content-visible when-format="html"}
::: insight
When we use the radial tour to examine how the different variables contribute to the separating hyperplane between the sexes, we learn that `bl` and `bd` are the most important variables.  We could (almost) ignore `fl` and `bm` for this classification.
:::
:::

::: {.content-visible when-format="pdf"}
\insightbox{When we use the radial tour to examine how the different variables contribute to the separating hyperplane between the sexes, we learn that {\textsf bl} and {\textsf bd} are the most important variables.  We could (almost) ignore {\textsf fl} and {\textsf bm} for this classification.}
:::

<!-- include Distance-weighted discrimination, eg kerndwd -->


## Exercises {-}

1. Generate a small subset from the `bushfire` data: we keep the variables `log_dist_cfa`, `log_dist_road` and `cause`, and we select only observations where `cause` is either lightning or arson. Fit a linear SVM model to separate the two classes and show the decision boundary together with the data. Compare to the boundary obtained by LDA and argue how the two models place the separating hyperplane in different ways.
2. We extend this into a multivariate setting by also including `amaxt180` and `amaxt720` as predictors. Fit a linear SVM model and calculate the hyperplane to judge which of these variables are important.
3. Calculate the decision boundary and look at it with a radial tour to see the effect from including individual predictors in a projection. Also explore what happens when rotating out multiple variables together. What can you learn from this?
4. From the `sketches_train` data select all observations of class banana or boomerang For this subset use PCA to find the first 5 PCs. Fit two SVM models: once with linear kernel and once with radial kernel and default value for the gamma parameter. Compare the number of misclassified observations in the training data for the two models.
5. Compute the model predictions and compare the decision boundaries between the linear and radial SVM using a slice tour. Does the shape match what you expect given the respective kernel function?
6. SVM models are defined for separating two classes, but and ensemble of such models can be used when we want to distinguish more than two classes. Look up the documentation of the `svm` function to learn how this works, then fit an SVM model to separate the three penguin species. In this case we primarily use the model predictions to investigate the decision boundaries, you can use `explore` together with the slice tour to do this. You can use different kernels and compare the resulting decision boundaries.


```{r}
#| eval: false
#| echo: false
library(mulgar)
library(tourr)
library(tidyverse)
library(MASS)
library(e1071)
library(classifly)
data(bushfires)

bushfires_sub <- dplyr::select(bushfires, log_dist_cfa,
                               log_dist_road,
                               cause) %>%
  filter(cause %in% c("lightning", "arson")) %>%
  mutate(cause = as.factor(cause))

ggplot(bushfires_sub, aes(log_dist_cfa, log_dist_road, color = cause)) +
  geom_point()

bf_lda <- lda(cause ~ ., bushfires_sub)#, prior = c(0.5, 0.5))
bf_lda_b <- as.data.frame(explore(bf_lda, bushfires_sub))
ggplot(filter(bf_lda_b, .BOUNDARY != TRUE)) +
  geom_point(aes(x=log_dist_cfa, y=log_dist_road, colour=cause, shape=.TYPE)) + 
  #scale_color_discrete_divergingx("Zissou 1") +
  scale_shape_manual(values=c(46, 16)) +
  theme_minimal() +
  theme(aspect.ratio = 1, legend.position = "none")

bf_svm <- svm(cause ~ ., bushfires_sub, kernel = "linear",
              probability = TRUE)
bf_svm_b <- as.data.frame(explore(bf_svm, bushfires_sub))
ggplot(filter(bf_svm_b, .BOUNDARY != TRUE)) +
  geom_point(aes(x=log_dist_cfa, y=log_dist_road, colour=cause, shape=.TYPE)) + 
  #scale_color_discrete_divergingx("Zissou 1") +
  scale_shape_manual(values=c(46, 16)) +
  theme_minimal() +
  theme(aspect.ratio = 1, legend.position = "none")

bushfires_sub_2 <- dplyr::select(bushfires, #amaxt90,
                                 amaxt180, amaxt720,
                                 log_dist_cfa, log_dist_road,
                                 cause) %>%
  filter(cause %in% c("lightning", "arson")) %>%
  mutate(cause = as.factor(cause))

bf_2_svm <- svm(cause ~ ., data = bushfires_sub_2,
                kernel = "linear", probability = TRUE)

b1_1 <- t(bf_2_svm$SV) %*% bf_2_svm$coefs
# it seems most variables are important, maybe amaxt180

bf_2_svm_e <- explore(bf_2_svm, bushfires_sub_2, n = 50000)

symbols <- c(46, 20)
c_pch <- symbols[as.numeric(bf_2_svm_e$.TYPE[!bf_2_svm_e$.BOUNDARY])]
animate_xy(bf_2_svm_e[!bf_2_svm_e$.BOUNDARY,1:4], 
           col=as.factor(bf_2_svm_e$cause[!bf_2_svm_e$.BOUNDARY]),
           pch = c_pch)
# we see that the hyperplane is > 2D and that amaxt180 is less important
# we also see that main feature in the data is distribution for one group (lightning)
# and it is difficult to separate the groups because they fall along similar direction
# now let's check with radial tour
set.seed(383)
b_start <- orthonormalise(cbind(b1_1, basis_random(4,1)))
animate_xy(bf_2_svm_e[!bf_2_svm_e$.BOUNDARY,1:4], 
           tour_path = radial_tour(start = b_start,
                                   mvar = c(3,4)),
           col=bf_2_svm_e$cause[!bf_2_svm_e$.BOUNDARY],
           pch=c_pch)
# because we use a combination of variables we do not see
# plane turning when rotating out only one of the variables,
# but we can see that combinations of the last three is what is
# important

data("sketches_train")
banana_v_boomerang <- filter(sketches_train,
                          word %in% c("banana", "boomerang"))
bvb_pca <- prcomp(banana_v_boomerang[,1:784])
ggscree(bvb_pca, q=25, guide=FALSE) # keeping first 5
# keeping first 9 seems like a good idea, but would require very large n
bvb_pc <- as.data.frame(bvb_pca$x[,1:5]) 
bvb_pc$word <- droplevels(banana_v_boomerang$word)

bvb_pc <- mutate_if(bvb_pc, is.numeric, mulgar:::scale2)

animate_xy(bvb_pc[,1:5], col=bvb_pc$word)



bvb_svm_l <- svm(word ~ ., data = bvb_pc,
                kernel = "linear", probability = TRUE)
sum(predict(bvb_svm_l) != bvb_pc$word) # compare accuracy as well
bvb_svm_r <- svm(word ~ ., data = bvb_pc,
                kernel = "radial", probability = TRUE)
sum(predict(bvb_svm_r) != bvb_pc$word)

bvb_svm_l_e <- explore(bvb_svm_l, bvb_pc, n = 20000)

symbols <- c(46, 20)
c_pch <- symbols[as.numeric(bvb_svm_l_e$.TYPE[!bvb_svm_l_e$.BOUNDARY])]
animate_xy(bvb_svm_l_e[!bvb_svm_l_e$.BOUNDARY,1:5], 
           col=bvb_svm_l_e$word[!bvb_svm_l_e$.BOUNDARY],
           pch = c_pch)
# for linear SVM we can use projections to understand the shape

bvb_svm_r_e <- explore(bvb_svm_r, bvb_pc, n = 20000)
animate_slice(bvb_svm_r_e[bvb_svm_r_e$.TYPE=="simulated" &
                            !bvb_svm_r_e$.BOUNDARY,1:5],
              col=bvb_svm_r_e$word[bvb_svm_r_e$.TYPE=="simulated" &
                                     !bvb_svm_r_e$.BOUNDARY], v_rel = 10)
# for radial kernel better use slice tour to see shape of the decision boundary
# here we only look at simulated points on the boundary, clear that this is not linear

```
