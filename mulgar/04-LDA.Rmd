# Linear discriminant analysis and MANOVA

Discriminant analysis dates to the early 1900s. Fisher's linear
discriminant \cite{Fi36} determines a linear combination of the
variables that separates two classes by comparing the differences
between class means with the variance of values within each class. It
makes no assumptions about the distribution of the data. Linear
discriminant analysis (LDA), as proposed by \citeasnoun{Rao48},
formalizes Fisher's approach by imposing the assumption that the data
values for each class arise from a $p$-dimensional multivariate normal
distribution, which shares a common variance--covariance matrix with data
from other classes. Under this assumption, Fisher's linear
discriminant gives the optimal separation between the two groups.

For two equally weighted groups, where $Y$ is coded as $\{0, 1\}$, the LDA rule is:

\begin{quote}
{\em Allocate a new observation $\blX_0$ to group 1 if}

\[
(\bar{\blX}_1-\bar{\blX}_2)'\blS^{-1}_{\rm pooled}\blX_0 \geq 
  \frac{1}{2}(\bar{\blX}_1-\bar{\blX}_2)'\blS^{-1}_{\rm pooled}
  (\bar{\blX}_1+\bar{\blX}_2) 
\]

{\em else allocate it to group 2, }
\end{quote}

\noindent where $\bar{\blX}_k $ are the class mean vectors of an 
$n\times p$ data matrix $\blX_k ~~(k=1,2)$, 

\[
\blS_{\rm pooled} = \frac{(n_1-1)
\blS_1}{(n_1-1)+(n_2-1)} + \frac{(n_2-1) \blS_2}{(n_1-1)+(n_2-1)}
\]

\noindent is the pooled variance--covariance matrix, and

\[ 
\blS_k = \frac{1}{n-1}\sum_{i=1}^{n}
(\blX_{ki}-\bar{\blX}_k)(\blX_{ki}-\bar{\blX}_k)', ~~k=1,2
\]

\noindent is the class variance--covariance matrix. The linear
discriminant part of this rule is
$(\bar{\blX}_1-\bar{\blX}_2)'\blS^{-1}_{\rm pooled}$, which defines
the linear combination of variables that best separates the two
groups.  To define a classification rule, we compute the value of the
new observation $\blX_0$ on this line and compare it with the value of
the average of the two class means $(\bar{\blX}_1+\bar{\blX}_2)/2$ on
the same line.
%Computing the value of the new observation $\blX_0$ on this
%line and comparing it with the value of the average of the two class
%means $(\bar{\blX}_1+\bar{\blX}_2)/2$ on this line gives the
%classification rule.

For multiple $(g)$ classes, the rule and the discriminant space are
constructed using the between-group sum-of-squares matrix,

\[
\blB =
\sum_{k=1}^g n_k(\bar{\blX}_k-\bar{\blX})(\bar{\blX}_k-\bar{\blX})' 
\]

\noindent which measures the differences between the class means, 
compared with the overall data mean $\bar{\blX}$ and the within-group
sum-of-squares matrix,

\[
\blW =
\sum_{k=1}^g\sum_{i=1}^{n_k}
(\blX_{ki}-\bar{\blX}_k)(\blX_{ki}-\bar{\blX}_k)'
\]

\noindent which measures the variation of values around each class mean.
The linear discriminant space is generated by computing the
eigenvectors (canonical coordinates) of $\blW^{-1}\blB$, and this is
the space where the group means are most separated with respect to the
pooled variance--covariance. The resulting classification rule is to
allocate a new observation to the class with the highest value of

\begin{eqnarray}
\bar{\blX}_k'\blS^{-1}_{\rm pooled}\blX_0 - 
\frac{1}{2}\bar{\blX}_k'\blS^{-1}_{\rm pooled}\bar{\blX}_k ~~~k=1,...,g \label{lda-rule}
\end{eqnarray}

\noindent which results in allocating the new observation into the
class with the closest mean.

This LDA approach is widely applicable, but it is useful
to check the underlying assumptions on which it depends: (1)
that the cluster structure corresponding to each class forms an
ellipse, showing that the class is consistent with a sample from a
multivariate normal distribution, and (2) that the variance of values
around each mean is nearly the same. Figure~\ref{lda-assumptions}
illustrates two datasets, of which only one is consistent with these
assumptions. Other parametric models, such as quadratic discriminant
analysis or logistic regression, also depend on assumptions
about the data which should be validated.  \index{classification
methods!quadratic discriminant analysis (QDA)} \index{classification
methods!logistic regression}

% Figure 1

Our description is derived from @VR02 and
@Ri96. A good general treatment of parametric methods for
supervised classification can be found in @JW02 or another
similar multivariate analysis textbook. Missing from multivariate
textbooks is a good explanation of the use of interactive graphics
both to check the assumptions underlying the methods and to explore
the results. This chapter fills this gap.

Algorithmic methods have overtaken parametric methods in the practice
of supervised classification.  A parametric method such as linear
discriminant analysis yields a set of interpretable output parameters, so
it leaves a clear trail helping us to understand what was done to
produce the results.  An algorithmic method, on the other hand, is
more or less a black box, with various input parameters that are
adjusted to tune the algorithm.  The algorithm's input and output
parameters do not always correspond in any obvious way to the
interpretation of the results.  All the same, these methods can be
very powerful and their use is not limited by requirements about
variable distributions as is the case with parametric methods.
