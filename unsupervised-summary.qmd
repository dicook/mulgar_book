# Summarising and comparing results {#sec-clust-compare}

\index{cluster analysis!confusion table}

<!--
- Adding cluster summary, eg means
- Linking between confusion matrix and tour
- Using same tour path to examine cluster colour
-->

## Summarising results

The key elements for summarising cluster results are the centres of the clusters and the within-cluster variability of the observations. Adding cluster means to any plot, including tour plots, is easy. You add the additional rows, or a new data set, and set the point shape to be distinct. 

Summarising the variability is difficult. For model-based clustering, the shape of the clusters is assumed to be elliptical, so $p$-dimensional ellipses can be used to show the solution, as done in @sec-mclust. Generally, it is common to plot a convex hull of the clusters, as in @fig-penguin-hull-2D. This can also be done in high-dimensions, using the R package `cxhull` to compute the $p$-D convex hull.

```{r}
#| message: false
#| code-summary: "Load libraries"
library(mclust) 
library(tidyr)
library(dplyr)
library(gt)
library(cxhull)
library(ggplot2)
library(colorspace)
```

```{r}
#| message: false
#| code-summary: "Code to do clustering"
library(mclust) 
library(tidyr)
library(dplyr)
library(gt)
library(cxhull)
library(ggplot2)
library(colorspace)
load("data/penguins_sub.rda")
p_dist <- dist(penguins_sub[,1:4])
p_hcw <- hclust(p_dist, method="ward.D2")

p_cl <- data.frame(cl_w = cutree(p_hcw, 3))

penguins_mc <- Mclust(penguins_sub[,1:4], 
                      G=3, 
                      modelNames = "EEE")
p_cl <- p_cl %>% 
  mutate(cl_mc = penguins_mc$classification)

p_cl <- p_cl %>% 
  mutate(cl_w_j = jitter(cl_w),
         cl_mc_j = jitter(cl_mc))

# Arranging by cluster id is important to define edges 
penguins_cl <- penguins_sub %>%
  mutate(cl_w = p_cl$cl_w,
         cl_mc = p_cl$cl_mc) %>%
  arrange(cl_w)
```

```{r}
#| code-summary: "Code for convex hulls in 2D"

# Penguins in 2D
# Duplicate observations need to be removed fo convex hull calculation
psub <- penguins_cl %>%
  select(bl, bd) 
dup <- duplicated(psub)
psub <- penguins_cl %>%
  select(bl, bd, cl_w) %>%
  filter(!dup) %>%
  arrange(cl_w)

ncl <- psub %>%
  count(cl_w) %>%
  arrange(cl_w) %>%
  mutate(cumn = cumsum(n))
phull <- NULL
for (i in unique(psub$cl_w)) {
  x <- psub %>%
    dplyr::filter(cl_w == i) %>%
    select(bl, bd) 
  ph <- cxhull(as.matrix(x))$edges
  if (i > 1) {
    ph <- ph + ncl$cumn[i-1]
  }
  ph <- cbind(ph, rep(i, nrow(ph)))
  phull <- rbind(phull, ph)
}
phull <- as.data.frame(phull)
colnames(phull) <- c("from", "to", "cl_w") 
phull_segs <- data.frame(x = psub$bl[phull$from],
                         y = psub$bd[phull$from],
                         xend = psub$bl[phull$to],
                         yend = psub$bd[phull$to],
                         cl_w = phull$cl_w)
phull_segs$cl_w <- factor(phull$cl_w) 
psub$cl_w <- factor(psub$cl_w)
p_chull2D <- ggplot() +
  geom_point(data=psub, aes(x=bl, y=bd, 
                            colour=cl_w)) + 
  geom_segment(data=phull_segs, aes(x=x, xend=xend,
                                    y=y, yend=yend,
                                    colour=cl_w)) +
  scale_colour_discrete_divergingx(palette = "Zissou 1") +
  theme_minimal() +
  theme(aspect.ratio = 1)
```

```{r}
#| eval: false
#| code-summary: "Code to generate pD convex hull and view in tour"
  
ncl <- penguins_cl %>%
  count(cl_w) %>%
  arrange(cl_w) %>%
  mutate(cumn = cumsum(n))
phull <- NULL
for (i in unique(penguins_cl$cl_w)) {
  x <- penguins_cl %>%
    dplyr::filter(cl_w == i) 
  ph <- cxhull(as.matrix(x[,1:4]))$edges
  if (i > 1) {
    ph <- ph + ncl$cumn[i-1]
  }
  ph <- cbind(ph, rep(i, nrow(ph)))
  phull <- rbind(phull, ph)
}
phull <- as.data.frame(phull)
colnames(phull) <- c("from", "to", "cl_w") 
phull$cl_w <- factor(phull$cl_w)
penguins_cl$cl_w <- factor(penguins_cl$cl_w)

animate_xy(penguins_cl[,1:4], col=penguins_cl$cl_w,
           edges=as.matrix(phull[,1:2]), edges.col=phull$cl_w)
render_gif(penguins_cl[,1:4], 
           tour_path = grand_tour(),
           display = display_xy(col=penguins_cl$cl_w,
                                edges=as.matrix(phull[,1:2]),
                                edges.col=phull$cl_w),
           gif_file = "gifs/penguins_chull.gif",
           frames = 500, 
           width = 400,
           height = 400)
```

```{r}
#| eval: false
#| echo: false

# This code is checking that convex hull works
library(geozoo)

cb <- cube.solid.random(p=4, n=1000)$points
phull <- cxhull(cb)$edges

animate_xy(cb, edges=phull)

smp <- simplex(p=4)$points
smp_phull <- cxhull(smp)$edges
animate_xy(smp, edges=smp_phull)

sph <- rbind(sphere.solid.random(p=3)$points,
             sphere.hollow(p=3)$points)
sph_phull <- cxhull(sph)$edges
animate_xy(as.data.frame(sph), edges=sph_phull)
```

::: {#fig-penguins-chull layout-ncol=2}

```{r}
#| label: fig-penguin-hull-2D
#| echo: false
#| fig-width: 4
#| fig-height: 4
#| fig-cap: "2D"
p_chull2D 
```

::: {.content-hidden when-format="pdf"}

![4D](gifs/penguins_chull.gif){#fig-penguins-chull-pD}
:::

Convex hulls summarising the extent of Wards linkage clustering in 2D and 4D.
:::

## Comparing two clusterings

Each cluster analysis will result in a vector of class labels for the data. To compare two results we would tabulate and plot the pair of integer variables. The labels given to each cluster will likely differ. If the two methods agree, there will be just a few cells with large counts among mostly empty cells. 

Below is a comparison between the three cluster results of Wards linkage hierarchical clustering (rows) and model-based clustering (columns). The two methods mostly agree, as seen from the three cells with large counts, and most cells with zeros. They disagree only on eight penguins. These eight penguins would be considered to be part of cluster 1 by Wards, but model-based considers them to be members of cluster 2.

The two methods label them clusters differently: what Wards labels as cluster 3, model-based labels as cluster 2. The labels given by any algorithm are arbitrary, and can easily be changed to coordinate between methods. 

```{r}
#| code-summary: "Code for confusion table"
p_cl %>% 
  count(cl_w, cl_mc) %>% 
  pivot_wider(names_from = cl_mc, 
              values_from = n, 
              values_fill = 0) %>%
  gt() %>%
  tab_spanner(label = "cl_mc", columns=c(`2`, `3`, `1`)) %>%
  cols_width(everything() ~ px(60))
```

We can examine the disagreement by linking a plot of the table, with a tour plot. Here is how to do this with `liminal`. @fig-compare-clusters1 and @fig-compare-clusters2 show screenshots of the exploration of the eight penguins on which the methods disagree. It makes sense that there is some confusion. These penguins are part of the large clump of observations that don't separate cleanly into two clusters. The eight penguins are in the middle of this clump. Realistically, both methods result in a plausible clustering, and it is not clear how these penguins should be grouped.  

```{r}
#| eval: false
#| message: false
#| code-summary: Code to do linked brushing with liminal
library(liminal)
limn_tour_link(
  p_cl[,3:4],
  penguins_cl,
  cols = bl:bm,
  color = cl_w
)
```

![Linking the confusion table with a tour using liminal. Points are coloured according to Wards linkage. The disagreement on eight penguins is with cluster 1 from Wards and cluster 2 from model-based.](images/compare-clusters1.png){#fig-compare-clusters1}

![Highlighting the penguins where the methods disagree so we can see where these observations are located relative to the two clusters.](images/compare-clusters2.png){#fig-compare-clusters2}

## Exercises {-}

1. Compare the results of the four cluster model-based clustering with that of the four cluster Wards linkage clustering of the penguins data.

## Project {-}

Most of the time your data will not neatly separate into clusters, but partitioning it into groups of similar observations can still be useful. In this case our toolbox will be useful in comparing and contrasting different methods, understanding to what extend a cluster mean can describe the observations in the cluster, and also how the boundaries between clusters have been drawn. To explore this we will use survey data that examines the risk taking behavior of tourists. The data was collected in Australia in 2015 [@risk-survey] and includes six types of risks (recreational, health, career, financial, safety and social) with responses on a scale from 1 (never) to 5 (very often). You can download the data from XXX

1. We first examine the data in a grand tour. Do you notice that each variable was measured on a discrete scale?
2. Next we explore different solutions from hierarchical clustering of the data. For comparison we will keep the number of clusters fixed to 6 and we will perform the hierarchical clustering with different combinations of distance functions (Manhattan distance and Euclidean distance) and linkage (single, complete and Ward linkage). Which combinations make sense based on what we know about the method and the data?
3. For each of the hierarchical clustering solutions draw the dendrogram in 2D and also in the data space. You can also map the grouping into 6 clusters to different colors. How would you describe the different solutions?
4. Using the method introduced in this chapter, compare the solution using Manhattan distance and complete linkage to one using Euclidean distance and Ward linkage. First compute a confusion table and then use `liminal` to explore some of the differences.
5. Selecting your preferred solution from hierarchical clustering, we will now compare it to what is found using $k$-means clustering with $k=6$. Use a tour to show the cluster means together with the data points (make sure to pick an appropriate symbol for the data points to avoid too much overplotting). What can you say about the variation within the clusters? Can you match some of the clusters with the most relevant variables from following the movement of the cluster means during the tour?
6. Use a projection pursuit guided tour to best separate the clusters identified with $k$-means clustering. How are the clusters related to the different types of risk?
7. Use the approaches from this chapter to summarize and compare the $k$-means solution to one of the hierarchical clustering results. Draw the convex hulls for the two solutions in a tour and compare the shapes.
8. Some other possible activities include examining how model-based methods would cluster the data. We expect it should be similar to Wards hierarchical or $k$-means, that it will partition into roughly equal chunks with an EII variance-covariance model being optimal. Also examining an SOM fit. SOM is not ideal for this data because the data fills the space. If the SOM model is fitted properly it should be a tangled net where the nodes (cluster means) are fairly evenly spread out. Thus the result should again be similar to Wards hierarchical or $k$-means. A common problem with fitting an SOM is that optimisation stops early, before fully capturing the data set. This is the reasons to use the tour for SOM. If the net is bunched  in one part of the data space, it means that the optimisation wasn't successful. <!-- XXX also model based?
9. XXX SOM here-->

```{r}
#| eval: false
#| echo: false
#| message: false
#| warning: false
# install.packages("https://homepage.boku.ac.at/leisch/MSA/packages/MSA_0.3-1.tar.gz", repos = NULL, type = "source")
library(MSA)
library(tidyverse)
data("risk", package = "MSA")
# looking at the data
library(tourr)
animate_xy(risk)
# hierarchical clustering solutions
risk_h_mc <- hclust(dist(risk, method = "manhattan"),
                  method = "complete")
risk_h_ms <- hclust(dist(risk, method = "manhattan"),
                  method = "single")
risk_h_ew <- hclust(dist(risk, method = "euclidean"),
                  method = "ward.D2")
# adding the clustering information to the data
risk_clw <- risk %>% 
  as_tibble() %>%
  mutate(cl_h_mc = factor(cutree(risk_h_mc, 6))) %>%
  mutate(cl_h_ms = factor(cutree(risk_h_ms, 6))) %>%
  mutate(cl_h_ew = factor(cutree(risk_h_ew, 6)))
# drawing 2D dendrograms
library(ggdendro)
ggplot() +
  geom_segment(data=dendro_data(risk_h_mc)$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  theme_dendro()
ggplot() +
  geom_segment(data=dendro_data(risk_h_ms)$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  theme_dendro()
ggplot() +
  geom_segment(data=dendro_data(risk_h_ew)$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  theme_dendro()
# dendrogram in the data space
library(mulgar)
risk_hfly_mc <- hierfly(risk_clw, risk_h_mc, scale=FALSE)
risk_hfly_ms <- hierfly(risk_clw, risk_h_ms, scale=FALSE)
risk_hfly_ew <- hierfly(risk_clw, risk_h_ew, scale=FALSE)
glyphs <- c(16, 46)
pchw_mc <- glyphs[risk_hfly_mc$data$node+1]
pchw_ms <- glyphs[risk_hfly_ms$data$node+1]
pchw_ew <- glyphs[risk_hfly_ew$data$node+1]
# manhattan + complete
# we can see hierarchical structure, small groups at the
# edges that get connected first and then combined into
# larger cluster, looks like some of the clusters are
# really spread out across the data space
animate_xy(risk_hfly_mc$data[,1:6], 
           col=risk_clw$cl_h_mc, 
           tour_path = grand_tour(),
           pch = pchw_mc,
           edges=risk_hfly_mc$edges, 
           axes="bottomleft",
           rescale=FALSE)
# Manhattan + single
# pretty much all the edges point in towards the center!
animate_xy(risk_hfly_ms$data[,1:6], 
           col=risk_clw$cl_h_ms, 
           tour_path = grand_tour(),
           pch = pchw_ms,
           edges=risk_hfly_ms$edges, 
           axes="bottomleft",
           rescale=FALSE)
# euclidean + ward
# at this stage looks mostly similar to mc case
animate_xy(risk_hfly_ew$data[,1:6], 
           col=risk_clw$cl_h_ew, 
           tour_path = grand_tour(),
           pch = pchw_ew,
           edges=risk_hfly_ew$edges, 
           axes="bottomleft",
           rescale=FALSE)

# comparison of two solutions
# confusion table shows quite some disagreements
risk_clw %>% 
  count(cl_h_mc, cl_h_ew) %>% 
  pivot_wider(names_from = cl_h_mc, 
              values_from = n, 
              values_fill = 0)
# explore with liminal
liminal::limn_tour_link(
  tibble::as.tibble(cbind(jitter(as.numeric(risk_clw$cl_h_ew)),
                          jitter(as.numeric(risk_clw$cl_h_mc)))),
  risk_clw,
  cols = 1:6,
  color = cl_h_ew)
# for example we can see cluster 2 for mc solution is very large,
# in the ew solution this is split up into 3 larger clusters,
# and in the tour we see how the smaller clusters 5 and 6 are different
# and less compact than the largest cluster 2 for the ew solution

# k-means + tour with cluster means
r_km <- kmeans(risk, centers=6, 
                     iter.max = 500, nstart = 5)
r_km_means <- data.frame(r_km$centers) %>%
  mutate(cl = factor(rownames(r_km$centers)))
r_km_d <- as.tibble(risk) %>% 
  mutate(cl = factor(r_km$cluster))
r_km_means <- r_km_means %>%
  mutate(type = "mean")
r_km_d <- r_km_d %>%
  mutate(type = "data")
r_km_all <- bind_rows(r_km_means, r_km_d)
r_km_all$type <- factor(r_km_all$type, levels=c("mean", "data"))
r_pch <- c(3, 46)[as.numeric(r_km_all$type)]
r_cex <- c(3, 1)[as.numeric(r_km_all$type)]
animate_xy(r_km_all[,1:6], col=r_km_all$cl, 
           pch=r_pch, cex=r_cex, axes="bottomleft")

# guided tour + interpretation in terms of variables
set.seed(543)
animate_xy(r_km_all[,1:6],
           tour_path = guided_tour(lda_pp(r_km_all$cl)) ,
           col=r_km_all$cl, 
           pch=r_pch, cex=r_cex, axes="bottomleft")
# from this solution it seems there is an overall
# risk behavior captured in the clustering, i.e. most
# variable contribute to separating the clusters along the
# x direction
# health risks seem to be a bit different, with one cluster
# containing average risk scores apart from high risk 
# scores for the health variable

# comparison k-means vs hierarchical clustering solution
# using convex hull in the data space
# is this a good idea? can we have facets to compare?
# I think probably remove this, it gets too messy, not sure the
# convex hull is right, especially when using the grouped display..
library(cxhull)
dup <- duplicated(risk_clw[,1:6])
#risk_clw <- risk_clw[!dup,]
risk_clw$cl_h_ew <- as.numeric(risk_clw$cl_h_ew)
ncl_h <- risk_clw %>%
  count(cl_h_ew) %>%
  arrange(cl_h_ew) %>%
  mutate(cumn = cumsum(n))
phull <- NULL
for (i in unique(risk_clw$cl_h_ew)) {
  x <- risk_clw %>%
    dplyr::filter(cl_h_ew == i) 
  ph <- cxhull(as.matrix(x[,1:6]))$edges
  if (i > 1) {
    ph <- ph + ncl_h$cumn[i-1]
  }
  ph <- cbind(ph, rep(i, nrow(ph)))
  phull <- rbind(phull, ph)
}
phull <- as.data.frame(phull)
colnames(phull) <- c("from", "to", "cl_h_ew") 
phull$cl_h_ew <- factor(phull$cl_h_ew)
risk_clw$cl_h_ew <- factor(risk_clw$cl_h_ew)
animate_groupxy(risk_clw[,1:6], col=risk_clw$cl_h_ew, pch=".",
           edges=as.matrix(phull[,1:2]), edges.col=phull$cl_h_ew,
           group_by=risk_clw$cl_h_ew)

## what if we use groupxy to compare only the cluster assignments?
risk_compare <- risk_clw %>%
  dplyr::select(-cl_h_mc, -cl_h_ms) %>%
  dplyr::rename(cl_h = cl_h_ew) %>%
  add_column(cl_k = r_km_d$cl) %>%
  pivot_longer(7:8, names_to = "clustering", names_prefix = "cl_",
               values_to = "cl")
# using different color schemes for the two solutions does not
# work in current setting since it does not allow too many colors
animate_groupxy(risk_compare[,1:6], col=risk_compare$cl,
           group_by=risk_compare$clustering, plot_xgp = FALSE)


# SOM
library(kohonen)
library(aweSOM)
set.seed(947)
r_grid <- somgrid(xdim = 5, ydim = 5,
                           topo = 'rectangular')
r_init <- somInit(risk, 5, 5)
r_som <- som(risk, 
             rlen=500,
             grid = r_grid,
             init = r_init)
r_som_df_net <- som_model(r_som)
r_som_map <- r_som_df_net$net %>%
  mutate(type="net")
r_som_data <- mutate(as.tibble(risk), type = "data")
r_som_map_data <- bind_rows(r_som_map, r_som_data)
r_som_map_data$type <- factor(r_som_map_data$type,
  levels=c("net", "data"))
animate_xy(r_som_map_data[,1:6],
           edges=as.matrix(r_som_df_net$edges),
           pch = 46,
           edges.col = "black",
           axes="bottomleft")
# what do we learn from this?
# We wouldn't expect SOM to work well for this data because it's
# pretty fully spread in the 6D. So the net is fairly tangled 
# filling out the space. Each node is where a cluster mean is
# placed, so you would see the placements of these means are
# fairly well spread - if the model fit optimisation has worked 
# properly. Probably the best use is to check that the model was fit
# well, and the final results should look similar to k-means
# because there is no benefit from having the net structure here.
# compare with
plot(r_som, main = "")
# can we link this with the tour somehow?
```


