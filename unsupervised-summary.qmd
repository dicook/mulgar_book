# Summarising and comparing results {#sec-clust-compare}

\index{cluster analysis!confusion table}

<!--
- Adding cluster summary, eg means
- Linking between confusion matrix and tour
- Using same tour path to examine cluster colour
-->

## Summarising results

The key elements for summarising cluster results are the centres of the clusters and the within-cluster variability of the observations. Adding cluster means to any plot, including tour plots, is easy. You add the additional rows, or a new data set, and set the point shape to be distinct. 

Summarising the variability is difficult. For model-based clustering, the shape of the clusters is assumed to be elliptical, so $p$-dimensional ellipses can be used to show the solution, as done in @sec-mclust. Generally, it is common to plot a convex hull of the clusters, as in @fig-penguin-hull-2D. This can also be done in high-dimensions, using the R package `cxhull` to compute the $p$-D convex hull.

```{r}
#| message: false
#| code-summary: "Load libraries"
library(mclust) 
library(tidyr)
library(dplyr)
library(gt)
library(cxhull)
library(ggplot2)
library(colorspace)
```

```{r}
#| message: false
#| code-summary: "Code to do clustering"
library(mclust) 
library(tidyr)
library(dplyr)
library(gt)
library(cxhull)
library(ggplot2)
library(colorspace)
load("data/penguins_sub.rda")
p_dist <- dist(penguins_sub[,1:4])
p_hcw <- hclust(p_dist, method="ward.D2")

p_cl <- data.frame(cl_w = cutree(p_hcw, 3))

penguins_mc <- Mclust(penguins_sub[,1:4], 
                      G=3, 
                      modelNames = "EEE")
p_cl <- p_cl %>% 
  mutate(cl_mc = penguins_mc$classification)

p_cl <- p_cl %>% 
  mutate(cl_w_j = jitter(cl_w),
         cl_mc_j = jitter(cl_mc))

# Arranging by cluster id is important to define edges 
penguins_cl <- penguins_sub %>%
  mutate(cl_w = p_cl$cl_w,
         cl_mc = p_cl$cl_mc) %>%
  arrange(cl_w)
```

```{r}
#| code-summary: "Code for convex hulls in 2D"

# Penguins in 2D
# Duplicate observations need to be removed fo convex hull calculation
psub <- penguins_cl %>%
  select(bl, bd) 
dup <- duplicated(psub)
psub <- penguins_cl %>%
  select(bl, bd, cl_w) %>%
  filter(!dup) %>%
  arrange(cl_w)

ncl <- psub %>%
  count(cl_w) %>%
  arrange(cl_w) %>%
  mutate(cumn = cumsum(n))
phull <- NULL
for (i in unique(psub$cl_w)) {
  x <- psub %>%
    dplyr::filter(cl_w == i) %>%
    select(bl, bd) 
  ph <- cxhull(as.matrix(x))$edges
  if (i > 1) {
    ph <- ph + ncl$cumn[i-1]
  }
  ph <- cbind(ph, rep(i, nrow(ph)))
  phull <- rbind(phull, ph)
}
phull <- as.data.frame(phull)
colnames(phull) <- c("from", "to", "cl_w") 
phull_segs <- data.frame(x = psub$bl[phull$from],
                         y = psub$bd[phull$from],
                         xend = psub$bl[phull$to],
                         yend = psub$bd[phull$to],
                         cl_w = phull$cl_w)
phull_segs$cl_w <- factor(phull$cl_w) 
psub$cl_w <- factor(psub$cl_w)
p_chull2D <- ggplot() +
  geom_point(data=psub, aes(x=bl, y=bd, 
                            colour=cl_w)) + 
  geom_segment(data=phull_segs, aes(x=x, xend=xend,
                                    y=y, yend=yend,
                                    colour=cl_w)) +
  scale_colour_discrete_divergingx(palette = "Zissou 1") +
  theme_minimal() +
  theme(aspect.ratio = 1)
```

```{r}
#| eval: false
#| code-summary: "Code to generate pD convex hull and view in tour"
  
ncl <- penguins_cl %>%
  count(cl_w) %>%
  arrange(cl_w) %>%
  mutate(cumn = cumsum(n))
phull <- NULL
for (i in unique(penguins_cl$cl_w)) {
  x <- penguins_cl %>%
    dplyr::filter(cl_w == i) 
  ph <- cxhull(as.matrix(x[,1:4]))$edges
  if (i > 1) {
    ph <- ph + ncl$cumn[i-1]
  }
  ph <- cbind(ph, rep(i, nrow(ph)))
  phull <- rbind(phull, ph)
}
phull <- as.data.frame(phull)
colnames(phull) <- c("from", "to", "cl_w") 
phull$cl_w <- factor(phull$cl_w)
penguins_cl$cl_w <- factor(penguins_cl$cl_w)

animate_xy(penguins_cl[,1:4], col=penguins_cl$cl_w,
           edges=as.matrix(phull[,1:2]), edges.col=phull$cl_w)
render_gif(penguins_cl[,1:4], 
           tour_path = grand_tour(),
           display = display_xy(col=penguins_cl$cl_w,
                                edges=as.matrix(phull[,1:2]),
                                edges.col=phull$cl_w),
           gif_file = "gifs/penguins_chull.gif",
           frames = 500, 
           width = 400,
           height = 400)
```

```{r}
#| eval: false
#| echo: false

# This code is checking that convex hull works
library(geozoo)

cb <- cube.solid.random(p=4, n=1000)$points
phull <- cxhull(cb)$edges

animate_xy(cb, edges=phull)

smp <- simplex(p=4)$points
smp_phull <- cxhull(smp)$edges
animate_xy(smp, edges=smp_phull)

sph <- rbind(sphere.solid.random(p=3)$points,
             sphere.hollow(p=3)$points)
sph_phull <- cxhull(sph)$edges
animate_xy(as.data.frame(sph), edges=sph_phull)
```

::: {#fig-penguins-chull layout-ncol=2}

```{r}
#| label: fig-penguin-hull-2D
#| echo: false
#| fig-width: 4
#| fig-height: 4
#| fig-cap: "2D"
p_chull2D 
```

::: {.content-hidden when-format="pdf"}

![4D](gifs/penguins_chull.gif){#fig-penguins-chull-pD}
:::

Convex hulls summarising the extent of Wards linkage clustering in 2D and 4D.
:::

## Comparing two clusterings

Each cluster analysis will result in a vector of class labels for the data. To compare two results we would tabulate and plot the pair of integer variables. The labels given to each cluster will likely differ. If the two methods agree, there will be just a few cells with large counts among mostly empty cells. 

Below is a comparison between the three cluster results of Wards linkage hierarchical clustering (rows) and model-based clustering (columns). The two methods mostly agree, as seen from the three cells with large counts, and most cells with zeros. They disagree only on eight penguins. These eight penguins would be considered to be part of cluster 1 by Wards, but model-based considers them to be members of cluster 2.

The two methods label them clusters differently: what Wards labels as cluster 3, model-based labels as cluster 2. The labels given by any algorithm are arbitrary, and can easily be changed to coordinate between methods. 

```{r}
#| code-summary: "Code for confusion table"
p_cl %>% 
  count(cl_w, cl_mc) %>% 
  pivot_wider(names_from = cl_mc, 
              values_from = n, 
              values_fill = 0) %>%
  gt() %>%
  tab_spanner(label = "cl_mc", columns=c(`2`, `3`, `1`)) %>%
  cols_width(everything() ~ px(60))
```

We can examine the disagreement by linking a plot of the table, with a tour plot. Here is how to do this with `liminal`. @fig-compare-clusters1 and @fig-compare-clusters2 show screenshots of the exploration of the eight penguins on which the methods disagree. It makes sense that there is some confusion. These penguins are part of the large clump of observations that don't separate cleanly into two clusters. The eight penguins are in the middle of this clump. Realistically, both methods result in a plausible clustering, and it is not clear how these penguins should be grouped.  

```{r}
#| eval: false
#| message: false
#| code-summary: Code to do linked brushing with liminal
library(liminal)
limn_tour_link(
  p_cl[,3:4],
  penguins_cl,
  cols = bl:bm,
  color = cl_w
)
```

![Linking the confusion table with a tour using liminal. Points are coloured according to Wards linkage. The disagreement on eight penguins is with cluster 1 from Wards and cluster 2 from model-based.](images/compare-clusters1.png){#fig-compare-clusters1}

![Highlighting the penguins where the methods disagree so we can see where these observations are located relative to the two clusters.](images/compare-clusters2.png){#fig-compare-clusters2}

## Exercises {-}

1. Compare the results of the four cluster model-based clustering with that of the four cluster Wards linkage clustering of the penguins data.

## Project {-}

Most of the time your data will not neatly separate into clusters, but partitioning it into groups of similar observations can still be useful. In this case our toolbox will be useful in comparing and contrasting different methods, understanding to what extend a cluster mean can describe the observations in the cluster, and also how the boundaries between clusters have been drawn. To explore this we will use survey data that examines the risk taking behavior of tourists. The data was collected in Australia in 2015 [@risk-survey] and includes six types of risks (recreational, health, career, financial, safety and social) with responses on a scale from 1 (never) to 5 (very often). You can download the data from XXX

1. We first examine the data in a grand tour. Do you notice that each variable was measured on a discrete scale?
2. Next we explore different solutions from hierarchical clustering of the data. For comparison we will keep the number of clusters fixed to 6 and we will perform the hierarchical clustering with different combinations of distance functions (Manhattan distance and Euclidean distance) and linkage (single, complete and Ward linkage). Which combinations make sense based on what we know about the method and the data?
3. For each of the hierarchical clustering solutions draw the dendrogram in 2D and also in the data space. You can also map the grouping into 6 clusters to different colors. How would you describe the different solutions?
4. Using the method introduced in this chapter, compare the solution using Manhattan distance and complete linkage to one using Euclidean distance and Ward linkage. What do you notice?
5. Selecting your preferred solution from hierarchical clustering, we will now compare it to what is found using $k$-means clustering with $k=6$. Use a tour to show the cluster means together with the data points (make sure to pick an appropriate symbol for the data points to avoid too much overplotting). What can you say about the variation within the clusters? Can you identify clear boundaries?
6. Use a projection pursuit guided tour to best separate the clusters identified with $k$-means clustering. How are the clusters related to the different types of risk?
7. Use the approaches from this chapter to summarize and compare the $k$-means solution to one of the hierarchical clustering results. Draw the convex hulls for the two solutions in a tour and compare the shapes.
8. XXX also model based?
9. XXX SOM here

```{r}
#| eval: false
#| echo: false
#| message: false
#| warning: false
# install.packages("https://homepage.boku.ac.at/leisch/MSA/packages/MSA_0.3-1.tar.gz", repos = NULL, type = "source")
library(MSA)
library(tidyverse)
data("risk", package = "MSA")
# looking at the data
library(tourr)
animate_xy(risk)
# hierarchical clustering solutions
risk_h_mc <- hclust(dist(risk, method = "manhattan"),
                  method = "complete")
risk_h_ms <- hclust(dist(risk, method = "manhattan"),
                  method = "single")
risk_h_ew <- hclust(dist(risk, method = "euclidean"),
                  method = "ward.D2")
# adding the clustering information to the data
risk_clw <- risk %>% 
  as_tibble() %>%
  mutate(cl_h_mc = factor(cutree(risk_h_mc, 6))) %>%
  mutate(cl_h_ms = factor(cutree(risk_h_ms, 6))) %>%
  mutate(cl_h_ew = factor(cutree(risk_h_ew, 6)))
# drawing 2D dendrograms
library(ggdendro)
ggplot() +
  geom_segment(data=dendro_data(risk_h_mc)$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  theme_dendro()
ggplot() +
  geom_segment(data=dendro_data(risk_h_ms)$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  theme_dendro()
ggplot() +
  geom_segment(data=dendro_data(risk_h_ew)$segments, 
               aes(x = x, y = y, 
                   xend = xend, yend = yend)) + 
  theme_dendro()
# dendrogram in the data space
library(mulgar)
risk_hfly_mc <- hierfly(risk_clw, risk_h_mc, scale=FALSE)
risk_hfly_ms <- hierfly(risk_clw, risk_h_ms, scale=FALSE)
risk_hfly_ew <- hierfly(risk_clw, risk_h_ew, scale=FALSE)
glyphs <- c(16, 46)
pchw_mc <- glyphs[risk_hfly_mc$data$node+1]
pchw_ms <- glyphs[risk_hfly_ms$data$node+1]
pchw_ew <- glyphs[risk_hfly_ew$data$node+1]
# manhattan + complete
# we can see hierarchical structure, small groups at the
# edges that get connected first and then combined into
# larger cluster, looks like some of the clusters are
# really spread out across the data space
animate_xy(risk_hfly_mc$data[,1:6], 
           col=risk_clw$cl_h_mc, 
           tour_path = grand_tour(),
           pch = pchw_mc,
           edges=risk_hfly_mc$edges, 
           axes="bottomleft",
           rescale=FALSE)
# Manhattan + single
# pretty much all the edges point in towards the center!
animate_xy(risk_hfly_ms$data[,1:6], 
           col=risk_clw$cl_h_ms, 
           tour_path = grand_tour(),
           pch = pchw_ms,
           edges=risk_hfly_ms$edges, 
           axes="bottomleft",
           rescale=FALSE)
# euclidean + ward
# at this stage looks mostly similar to mc case
animate_xy(risk_hfly_ew$data[,1:6], 
           col=risk_clw$cl_h_ew, 
           tour_path = grand_tour(),
           pch = pchw_ew,
           edges=risk_hfly_ew$edges, 
           axes="bottomleft",
           rescale=FALSE)
# comparison with liminal

# k-means + tour with cluster means
r_km <- kmeans(risk, centers=6, 
                     iter.max = 500, nstart = 5)
r_km_means <- data.frame(r_km$centers) %>%
  mutate(cl = factor(rownames(r_km$centers)))
r_km_d <- as.tibble(risk) %>% 
  mutate(cl = factor(r_km$cluster))
r_km_means <- r_km_means %>%
  mutate(type = "mean")
r_km_d <- r_km_d %>%
  mutate(type = "data")
r_km_all <- bind_rows(r_km_means, r_km_d)
r_km_all$type <- factor(r_km_all$type, levels=c("mean", "data"))
r_pch <- c(3, 46)[as.numeric(r_km_all$type)]
r_cex <- c(3, 1)[as.numeric(r_km_all$type)]
animate_xy(r_km_all[,1:6], col=r_km_all$cl, 
           pch=r_pch, cex=r_cex, axes="bottomleft")

# guided tour + interpretation in terms of variables
set.seed(543)
animate_xy(r_km_all[,1:6],
           tour_path = guided_tour(lda_pp(r_km_all$cl)) ,
           col=r_km_all$cl, 
           pch=r_pch, cex=r_cex, axes="bottomleft")
# from this solution it seems there is an overall
# risk behavior captured in the clustering, i.e. most
# variable contribute to separating the clusters along the
# x direction
# health risks seem to be a bit different, with one cluster
# containing average risk scores apart from high risk 
# scores for the health variable

# comparison k-means vs hierarchical clustering solution
# using convex hull in the data space
# is this a good idea? can we have facets to compare?

# SOM
library(kohonen)
library(aweSOM)
set.seed(947)
r_grid <- somgrid(xdim = 5, ydim = 5,
                           topo = 'rectangular')
r_init <- somInit(risk, 5, 5)
r_som <- som(risk, 
             rlen=500,
             grid = r_grid,
             init = r_init)
r_som_df_net <- som_model(r_som)
r_som_map <- r_som_df_net$net %>%
  mutate(type="net")
r_som_data <- mutate(as.tibble(risk), type = "data")
r_som_map_data <- bind_rows(r_som_map, r_som_data)
r_som_map_data$type <- factor(r_som_map_data$type,
  levels=c("net", "data"))
animate_xy(r_som_map_data[,1:6],
           edges=as.matrix(r_som_df_net$edges),
           pch = 46,
           edges.col = "black",
           axes="bottomleft")
# what do we learn from this?
# compare with
plot(r_som, main = "")
# can we link this with the tour somehow?
```


<!--
To compare the results of two methods we commonly compute a confusion table. For example, @tab-confusion is the confusion table for five-cluster solutions for the \Data{Music} data from $k$-means and
Ward's linkage hierarchical clustering, generated by:

```
> d.music.dist <- dist(subset(d.music.std,
   select=c(lvar:lfreq)))
> d.music.hc <- hclust(d.music.dist, method="ward")
> cl5 <- cutree(d.music.hc,5)
> d.music.km <- kmeans(subset(d.music.std,
   select=c(lvar:lfreq)), 5)
> table(d.music.km$cluster, cl5)
> d.music.clustcompare <- 
    cbind(d.music.std,cl5,d.music.km$cluster)
> names(d.music.clustcompare)[8] <- "Wards"
> names(d.music.clustcompare)[9] <- "km"
> gd <- ggobi(d.music.clustcompare)[1]
```

\noindent The numerical labels of clusters are arbitrary, so these can be rearranged to better digest the table. There is a lot of agreement
between the two methods: Both methods agree on the cluster for 48 tracks out of 62, or 77\% of the time.  We want to explore the data space to see where the agreement occurs and where the two methodsdisagree.

\begin{center}
\begin{table}[h]
\caption[Tables showing the agreement between two solutions for the
\Data{Music} data]{Tables showing the agreement between two
five-cluster solutions for the \Data{Music} data, showing a lot of
agreement between $k$-means and Ward's linkage hierarchical
clustering.  The rows have been rearranged to make the table more
readable.}
\begin{tabular}{cp{0.2in}p{1.3in}c}
\begin{tabular}{c@{\hspace{.1in}}|rrrrr}
& \multicolumn{5}{c}{Ward's} \\
\B $k$-means & ~1 & ~2 & ~3 & 4 & 5 \\\hline
\T 1 & 0 & 0 & 3 & 0 & 14 \\ 
2 & 0 & 0 & 1 & 0 & 0 \\
3 & 0 & 9 & 5 & 0 & 0 \\
4 & 8 & 2 & 1 & 0 & 0 \\
5 & 0 & 0 & 3 & 16 & 0 \\
\end{tabular}
& & Rearrange rows $\Rightarrow$  &
\begin{tabular}{c@{\hspace{.1in}}|rrrrr}
& \multicolumn{5}{c}{Ward's} \\
\B $k$-means & ~1 & ~2 & ~3 & 4 & 5 \\\hline
\T 4 & 8 & 2 & 1 & 0 & 0 \\
3 & 0 & 9 & 5 & 0 & 0 \\
2 & 0 & 0 & 1 & 0 & 0 \\
5 & 0 & 0 & 3 & 16 & 0 \\
1 & 0 & 0 & 3 & 0 & 14 \\ 
\end{tabular}
\end{tabular}
\label{confusion}
\vspace{.5em}
\end{table}
\end{center}

% Figure 11
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=2in]{chap-clust/music-hc1a.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-hc1b.pdf}}}
\smallskip
\centerline{{\includegraphics[width=2in]{chap-clust/music-hc2a.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-hc2b.pdf}}}
\caption[Comparing two five-cluster models of the \Data{Music} data
using confusion tables linked to tour plots]{Comparing two
five-cluster models of the \Data{Music} data using confusion tables
linked to tour plots.  In the confusion tables, $k$-means cluster
identifiers for each plot are plotted against Ward's linkage
hierarchical clustering ids.  (The values have been jittered.)  Two
different areas of agreement have been highlighted, and the tour
projections show the tightness of each cluster where the methods
agree. }
\label{clust-compare}
\end{figure*}

\index{jittering}

In @fig-clust-compare, we link jittered plots of the confusion table for the two clustering methods with 2D tour plots of the data. The first column contains two jittered plots of the confusion table.  In the top row of the figure, we have highlighted a group of 14 points that both methods agree form a cluster, painting them as orange triangles. From the plot at the right, we see that this cluster is a closely grouped set of points in the data space. From the tour axes we see that `lvar` has the largest axis pointing in the direction of the cluster separation, which suggests that music pieces
in this cluster are characterized by high values on `lvar` (variable 3 in the data); that is, they have large variance in frequency. By further investigating which tracks are in this cluster, we can learn that it consists of a mix of tracks by the Beatles
("Penny Lane", "Help", "Yellow Submarine", ...)  and the Eels ("Saturday Morning", "Love of the Loveless", ...).

In the bottom row of the figure, we have highlighted a second group of tracks that were clustered together by both methods, painting them again using orange triangles.  In the plot to the right, we see that this cluster is closely grouped in the data space.  Despite that, this cluster is a bit more difficult to characterize.  It is oriented mostly in the negative direction of `lave` (variable 4), so it would have smaller values on this variable. But this vertical direction in
the plot also has large contributions from variables 3 (`lvar`) and 7 (`lfreq`).  If you label these eight points on your own, you will see that they are all Abba songs ("Dancing Queen", "Waterloo", "Mamma Mia", ...).

We have explored two groups of tracks where the methods agree. In a similar fashion, we could also explore the tracks where the methods disagree.

\index{cluster analysis!cluster characterization}
\section{Characterizing clusters}

The final step in a cluster analysis is to characterize the clusters. Actually, we have engaged in cluster characterization throughout the examples, because it is an intrinsic part of assessing the results of any cluster analysis.  If we cannot detect any numerical or qualitative differences between clusters, then our analysis was not successful, and we start over with a different distance metric or algorithm.

However, once we are satisfied that we have found a set of clusters that can be differentiated from one another, we want to describe them more formally, both quantitatively and qualitatively.  We characterize
them quantitatively by computing such statistics as cluster means and standard deviations for each variable.  We can look at these results in tables and in plots, and we can refine the qualitative descriptions of the clusters we made during the assessment process.

\index{parallel coordinate plot}

The parallel coordinate plot is often used during this stage.
#fig-clust-char shows the parallel coordinate plot for the
first of the clusters of music pieces singled out for study in the
previous section. Ward's hierarchical linkage and $k$-means both
agreed that these music pieces form a cluster.  Since the matrix and
the number of clusters are both small, we plot the raw data; for
larger problems, we might plot cluster statistics as well [see,
for example, @DSP05].


% figure 12
\begin{figure*}[htbp]
\begin{center}
  {\includegraphics[width=4in]{chap-clust/music-clust1.pdf}}
\end{center}
\caption[Characterizing clusters in a parallel coordinate
plot]{Characterizing clusters in a parallel coordinate plot.  The
highlighted profiles correspond to one cluster for which
Ward's hierarchical linkage and $k$-means were in agreement.  }
\label{clust-char}
\end{figure*}

This cluster containing a mix of Beatles and Eels music has high
values on \Vbl{lvar}, medium values of \Vbl{lave}, high values of
\Vbl{lmax}, high values of \Vbl{lfener}, and varied \Vbl{lfreq}
values. That is, these pieces of music have a large variance in
frequency, high frequency, and high energy relative to the other
music pieces.

## Recap {#chap-clust-recap}


Graphics are invaluable for cluster analysis, whether they are used to find clusters or to interpret and evaluate the results of a cluster analysis arrived at by other means.

The spin and brush approach can be used to get an initial look at the data and to find clusters, and occasionally, it is sufficient.  When the clustering is the result of an algorithm, a very useful first step
is to paint the points by cluster membership and to look at the data to see whether the clustering seems sensible.  How many clusters are there, and how big are they?  What shape are they, and do they overlap
one another?  Which variables have contributed most to the clustering? Can the clusters be qualitatively described?  All the plots we have described can be useful: scatterplots, parallel coordinate plots, and
area plots, as well as static plots like dendrograms.

When the clusters have been generated by a model, we should also use graphics to help us assess the model.  If the model makes distributional assumptions, we can generate ellipses and compare them with the clusters to see whether the shapes are consistent.  For self-organizing maps the tour can assist in uncovering problems with the fit, such as when the map wraps in on itself through the data making it appear that some cases are far apart when they are truly
close together. A confusion table can come alive with linked brushing, so that mismatches and agreements between methods can be
explored. \index{brushing!linked}
-->
