## Comparing methods

\index{cluster analysis!confusion table}
\index{datasets!\Data{Music}}

To compare the results of two methods we commonly compute a confusion table. For example, @tab-confusion is the confusion table for five-cluster solutions for the \Data{Music} data from $k$-means and
Ward's linkage hierarchical clustering, generated by:

```
> d.music.dist <- dist(subset(d.music.std,
   select=c(lvar:lfreq)))
> d.music.hc <- hclust(d.music.dist, method="ward")
> cl5 <- cutree(d.music.hc,5)
> d.music.km <- kmeans(subset(d.music.std,
   select=c(lvar:lfreq)), 5)
> table(d.music.km$cluster, cl5)
> d.music.clustcompare <- 
    cbind(d.music.std,cl5,d.music.km$cluster)
> names(d.music.clustcompare)[8] <- "Wards"
> names(d.music.clustcompare)[9] <- "km"
> gd <- ggobi(d.music.clustcompare)[1]
```

\noindent The numerical labels of clusters are arbitrary, so these can be rearranged to better digest the table. There is a lot of agreement
between the two methods: Both methods agree on the cluster for 48 tracks out of 62, or 77\% of the time.  We want to explore the data space to see where the agreement occurs and where the two methodsdisagree.

<!--
\begin{center}
\begin{table}[h]
\caption[Tables showing the agreement between two solutions for the
\Data{Music} data]{Tables showing the agreement between two
five-cluster solutions for the \Data{Music} data, showing a lot of
agreement between $k$-means and Ward's linkage hierarchical
clustering.  The rows have been rearranged to make the table more
readable.}
\begin{tabular}{cp{0.2in}p{1.3in}c}
\begin{tabular}{c@{\hspace{.1in}}|rrrrr}
& \multicolumn{5}{c}{Ward's} \\
\B $k$-means & ~1 & ~2 & ~3 & 4 & 5 \\\hline
\T 1 & 0 & 0 & 3 & 0 & 14 \\ 
2 & 0 & 0 & 1 & 0 & 0 \\
3 & 0 & 9 & 5 & 0 & 0 \\
4 & 8 & 2 & 1 & 0 & 0 \\
5 & 0 & 0 & 3 & 16 & 0 \\
\end{tabular}
& & Rearrange rows $\Rightarrow$  &
\begin{tabular}{c@{\hspace{.1in}}|rrrrr}
& \multicolumn{5}{c}{Ward's} \\
\B $k$-means & ~1 & ~2 & ~3 & 4 & 5 \\\hline
\T 4 & 8 & 2 & 1 & 0 & 0 \\
3 & 0 & 9 & 5 & 0 & 0 \\
2 & 0 & 0 & 1 & 0 & 0 \\
5 & 0 & 0 & 3 & 16 & 0 \\
1 & 0 & 0 & 3 & 0 & 14 \\ 
\end{tabular}
\end{tabular}
\label{confusion}
\vspace{.5em}
\end{table}
\end{center}

% Figure 11
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=2in]{chap-clust/music-hc1a.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-hc1b.pdf}}}
\smallskip
\centerline{{\includegraphics[width=2in]{chap-clust/music-hc2a.pdf}}
 {\includegraphics[width=2in]{chap-clust/music-hc2b.pdf}}}
\caption[Comparing two five-cluster models of the \Data{Music} data
using confusion tables linked to tour plots]{Comparing two
five-cluster models of the \Data{Music} data using confusion tables
linked to tour plots.  In the confusion tables, $k$-means cluster
identifiers for each plot are plotted against Ward's linkage
hierarchical clustering ids.  (The values have been jittered.)  Two
different areas of agreement have been highlighted, and the tour
projections show the tightness of each cluster where the methods
agree. }
\label{clust-compare}
\end{figure*}
-->

\index{jittering}

In @fig-clust-compare, we link jittered plots of the confusion table for the two clustering methods with 2D tour plots of the data. The first column contains two jittered plots of the confusion table.  In the top row of the figure, we have highlighted a group of 14 points that both methods agree form a cluster, painting them as orange triangles. From the plot at the right, we see that this cluster is a closely grouped set of points in the data space. From the tour axes we see that `lvar` has the largest axis pointing in the direction of the cluster separation, which suggests that music pieces
in this cluster are characterized by high values on `lvar` (variable 3 in the data); that is, they have large variance in frequency. By further investigating which tracks are in this cluster, we can learn that it consists of a mix of tracks by the Beatles
("Penny Lane", "Help", "Yellow Submarine", ...)  and the Eels ("Saturday Morning", "Love of the Loveless", ...).

In the bottom row of the figure, we have highlighted a second group of tracks that were clustered together by both methods, painting them again using orange triangles.  In the plot to the right, we see that this cluster is closely grouped in the data space.  Despite that, this cluster is a bit more difficult to characterize.  It is oriented mostly in the negative direction of `lave` (variable 4), so it would have smaller values on this variable. But this vertical direction in
the plot also has large contributions from variables 3 (`lvar`) and 7 (`lfreq`).  If you label these eight points on your own, you will see that they are all Abba songs ("Dancing Queen", "Waterloo", "Mamma Mia", ...).

We have explored two groups of tracks where the methods agree. In a similar fashion, we could also explore the tracks where the methods disagree.

\index{cluster analysis!cluster characterization}
\section{Characterizing clusters}

The final step in a cluster analysis is to characterize the clusters. Actually, we have engaged in cluster characterization throughout the examples, because it is an intrinsic part of assessing the results of any cluster analysis.  If we cannot detect any numerical or qualitative differences between clusters, then our analysis was not successful, and we start over with a different distance metric or algorithm.

However, once we are satisfied that we have found a set of clusters that can be differentiated from one another, we want to describe them more formally, both quantitatively and qualitatively.  We characterize
them quantitatively by computing such statistics as cluster means and standard deviations for each variable.  We can look at these results in tables and in plots, and we can refine the qualitative descriptions of the clusters we made during the assessment process.

\index{parallel coordinate plot}

The parallel coordinate plot is often used during this stage.
#fig-clust-char shows the parallel coordinate plot for the
first of the clusters of music pieces singled out for study in the
previous section. Ward's hierarchical linkage and $k$-means both
agreed that these music pieces form a cluster.  Since the matrix and
the number of clusters are both small, we plot the raw data; for
larger problems, we might plot cluster statistics as well [see,
for example, @DSP05].

<!--
% figure 12
\begin{figure*}[htbp]
\begin{center}
  {\includegraphics[width=4in]{chap-clust/music-clust1.pdf}}
\end{center}
\caption[Characterizing clusters in a parallel coordinate
plot]{Characterizing clusters in a parallel coordinate plot.  The
highlighted profiles correspond to one cluster for which
Ward's hierarchical linkage and $k$-means were in agreement.  }
\label{clust-char}
\end{figure*}
-->

This cluster containing a mix of Beatles and Eels music has high
values on \Vbl{lvar}, medium values of \Vbl{lave}, high values of
\Vbl{lmax}, high values of \Vbl{lfener}, and varied \Vbl{lfreq}
values. That is, these pieces of music have a large variance in
frequency, high frequency, and high energy relative to the other
music pieces.

## Recap {#chap-clust-recap}


Graphics are invaluable for cluster analysis, whether they are used to find clusters or to interpret and evaluate the results of a cluster analysis arrived at by other means.

The spin and brush approach can be used to get an initial look at the data and to find clusters, and occasionally, it is sufficient.  When the clustering is the result of an algorithm, a very useful first step
is to paint the points by cluster membership and to look at the data to see whether the clustering seems sensible.  How many clusters are there, and how big are they?  What shape are they, and do they overlap
one another?  Which variables have contributed most to the clustering? Can the clusters be qualitatively described?  All the plots we have described can be useful: scatterplots, parallel coordinate plots, and
area plots, as well as static plots like dendrograms.

When the clusters have been generated by a model, we should also use graphics to help us assess the model.  If the model makes distributional assumptions, we can generate ellipses and compare them with the clusters to see whether the shapes are consistent.  For self-organizing maps the tour can assist in uncovering problems with the fit, such as when the map wraps in on itself through the data making it appear that some cases are far apart when they are truly
close together. A confusion table can come alive with linked brushing, so that mismatches and agreements between methods can be
explored. \index{brushing!linked}

