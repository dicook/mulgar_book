{
  "hash": "47e09c461b31eb53ff57d81517f53e40",
  "result": {
    "engine": "knitr",
    "markdown": "# $k$-means clustering {#sec-kmeans}\n\\index{cluster analysis!k-means}\n\\index{cluster analysis!algorithms}\n\nOne of the simplest and efficient techniques for clustering data is the $k$-means algorithm. The algorithm begins with a choice for $k$, the number of clusters to divide the data into. It is seeded with $k$ initial means, and sequentially iterates through the observations, assigning them to the nearest mean, and re-calculating the $k$ means. It stops at a given number of iterations or when points no longer change clusters. The algorithm will tend to segment the data into roughly equal sized, or spherical clusters, and thus will work well if the clusters are separated and equally spherical in shape. \n\nA good place to learn ore about the $k$-means algorithm is Chapter 20 of @HOML. The algorithm has been in use for a long time! It was named $k$-means by @MacQueen1967, but developed by Lloyd in 1957 (as described in @Lloyd1982) and separately by @forgy65, and perhaps others as it is a very simple procedure.\n\n::: {.content-visible when-format=\"html\"}\n::: info\nThe key elements to examine in a k-means clustering algorithm result  are:\n\n- means \n- boundaries\n\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\infobox{The key elements to examine in a k-means clustering algorithm result  are:\n\\begin{itemize}\n\\item means \n\\item boundaries \n\\end{itemize}}\n:::\n\n## Examining results in 2D\n\n@fig-km-2D shows the results of $k$-means clustering on the 2D `simple_clusters` data and two variables of the penguins data. We can see that it works well when the clusters are spherical, but for the penguins data it fails because the shape of the clusters is elliptical. It actually makes a mistake that would not be made if we simply visually clustered: cluster 3 has grouped points across a gap, a divide that visually we would all agree should form a separation.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(mulgar)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(colorspace)\nlibrary(patchwork)\ndata(\"simple_clusters\")\nload(\"data/penguins_sub.rda\")\n\nset.seed(202305)\nsc_bl_bd_km <- kmeans(simple_clusters[,1:2], centers=2, \n                     iter.max = 50, nstart = 5)\nsc_bl_bd_km_means <- data.frame(sc_bl_bd_km$centers) %>%\n  mutate(cl = factor(rownames(sc_bl_bd_km$centers)))\nsc_bl_bd_km_d <- simple_clusters[,1:2] %>% \n  mutate(cl = factor(sc_bl_bd_km$cluster))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to make plots\"}\nsc_bl_bd_km_p <- ggplot() +\n  geom_point(data=sc_bl_bd_km_d, \n             aes(x=x1, y=x2, colour=cl), \n             shape=16, alpha=0.4) +\n  geom_point(data=sc_bl_bd_km_means, \n             aes(x=x1, y=x2, colour=cl), \n             shape=3, size=5) +\n    scale_color_discrete_divergingx(\"Zissou 1\") +\n  ggtitle(\"(a)\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1, \n        legend.position = \"bottom\",\n        legend.title = element_blank()) \n\np_bl_bd_km <- kmeans(penguins_sub[,1:2], centers=3, \n                     iter.max = 50, nstart = 5)\np_bl_bd_km_means <- data.frame(p_bl_bd_km$centers) %>%\n  mutate(cl = factor(rownames(p_bl_bd_km$centers)))\np_bl_bd_km_d <- penguins_sub[,1:2] %>% \n  mutate(cl = factor(p_bl_bd_km$cluster))\n\np_bl_bd_km_p <- ggplot() +\n  geom_point(data=p_bl_bd_km_d, \n             aes(x=bl, y=bd, colour=cl), \n             shape=16, alpha=0.4) +\n  geom_point(data=p_bl_bd_km_means, \n             aes(x=bl, y=bd, colour=cl), \n             shape=3, size=5) +\n    scale_color_discrete_divergingx(\"Zissou 1\") +\n  ggtitle(\"(b)\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1, \n        legend.position = \"bottom\",\n        legend.title = element_blank()) \n\nsc_bl_bd_km_p + p_bl_bd_km_p + plot_layout(ncol=2)\n```\n\n::: {.cell-output-display}\n![Examining $k$-means clustering results for simple clusters (a) and two variables of the penguins data (b). The means are indicated by a $+$. The results are perfect for the simple clusters but not for the penguins data. The penguin clusters are elliptically shaped which is not captured by $k$-means. Cluster 3 has observations grouped across a gap in the data.](9-kmeans_files/figure-html/fig-km-2D-1.png){#fig-km-2D width=576}\n:::\n:::\n\n\n\n\n\n\n## Examining results in high dimensions\n\nThis approach extends to high-dimensions. One colours observations by the cluster label, and overlays the final cluster means. If we see gaps in points in a single cluster it would mean that $k$-means fails to see important cluster structure. This is what happens with the 4D penguins data as shown in @fig-p-km-html.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\np_km <- kmeans(penguins_sub[,1:4], centers=3, \n                     iter.max = 50, nstart = 5)\np_km_means <- data.frame(p_km$centers) %>%\n  mutate(cl = factor(rownames(p_km$centers)))\np_km_d <- penguins_sub[,1:4] %>% \n  mutate(cl = factor(p_km$cluster))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to make animated gifs\"}\nlibrary(tourr)\np_km_means <- p_km_means %>%\n  mutate(type = \"mean\")\np_km_d <- p_km_d %>%\n  mutate(type = \"data\")\np_km_all <- bind_rows(p_km_means, p_km_d)\np_km_all$type <- factor(p_km_all$type, levels=c(\"mean\", \"data\"))\np_pch <- c(3, 20)[as.numeric(p_km_all$type)]\np_cex <- c(3, 1)[as.numeric(p_km_all$type)]\nanimate_xy(p_km_all[,1:4], col=p_km_all$cl, \n           pch=p_pch, cex=p_cex, axes=\"bottomleft\")\nrender_gif(p_km_all[,1:4],\n           grand_tour(),\n           display_xy(col=p_km_all$cl, \n                      pch=p_pch, \n                      cex=p_cex, \n                      axes=\"bottomleft\"),\n           gif_file=\"gifs/p_km.gif\",\n           width=400,\n           height=400,\n           frames=500)\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n![Exploring the k-means clustering result for the 4D penguins data. You can see cluster 2 clearly separated from the other observations. Cluster 3, like in the 2D example, is a mix of observations across a gap. Even the mean of the cluster is almost in the gap. ](gifs/p_km.gif){#fig-p-km-html fig-alt=\"FIX ME\" width=400}\n\nGenerally, there is no need to choose $k$ ahead of time. One would re-fit $k$-means with various choices of $k$, and compare the `tot.withinss` and examine the clusters visually, to decide on the optimal final value of $k$. This can be assessed in a similar way to the scree plot for PCA. \n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n![Exploring the k-means clustering result for the 4D penguins data. You can see cluster 2 clearly separated from the other observations. Cluster 3, like in the 2D example, is a mix of observations across a gap. Even the mean of the cluster is almost in the gap. ](images/p_km_71.png){#fig-p-km-pdf fig-alt=\"FIX ME\" width=400}\n\nGenerally, there is no need to choose $k$ ahead of time. One would re-fit $k$-means with various choices of $k$, and compare the `tot.withinss` and examine the clusters visually, to decide on the optimal final value of $k$. This can be assessed in a similar way to the scree plot for PCA. \n:::\n\n## Exercises {-}\n\n1. Compute a $k$-means clustering for the `fake_trees` data, varying $k$ to about 20. Choose your best $k$, and examine the solution using the first 10 PCs on the data. It should capture the data quite nicely, although it will break up each branch into multiple clusters.\n2. Compute a $k$-means clustering of the first four PCs of the `aflw` data. Examine the best solution (you choose which $k$), and describe how it divides the data. By examining the means, can you tell if it extracts clusters of offensive vs defensive vs midfield players? Or does it break the data into high skills vs low skills?\n3. Use $k$-means clustering on the challenge data sets, `c1`-`c7` from the `mulgar` package. Explain what choice of $k$ is best for each data set, and why or why not the cluster structure, as you have described it from earlier chapters, is detected or not.\n",
    "supporting": [
      "9-kmeans_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}