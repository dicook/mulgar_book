{
  "hash": "8eff4ce9a0108982c9f1dd1851bb49d3",
  "result": {
    "markdown": "# Summarising and comparing results {#sec-clust-compare}\n\n\\index{cluster analysis!confusion table}\n\n<!--\n- Adding cluster summary, eg means\n- Linking between confusion matrix and tour\n- Using same tour path to examine cluster colour\n-->\n\n## Summarising results\n\nThe key elements for summarising cluster results are the centres of the clusters and the within-cluster variability of the observations. Adding cluster means to any plot, including tour plots, is easy. You add the additional rows, or a new data set, and set the point shape to be distinct. \n\nSummarising the variability is difficult. For model-based clustering, the shape of the clusters is assumed to be elliptical, so $p$-dimensional ellipses can be used to show the solution, as done in @sec-mclust. Generally, it is common to plot a convex hull of the clusters, as in @fig-penguin-hull-2D. This can also be done in high-dimensions, using the R package `cxhull` to compute the $p$-D convex hull.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Load libraries\"}\nlibrary(mclust) \nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(gt)\nlibrary(cxhull)\nlibrary(ggplot2)\nlibrary(colorspace)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to do clustering\"}\nlibrary(mclust) \nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(gt)\nlibrary(cxhull)\nlibrary(ggplot2)\nlibrary(colorspace)\nload(\"data/penguins_sub.rda\")\np_dist <- dist(penguins_sub[,1:4])\np_hcw <- hclust(p_dist, method=\"ward.D2\")\n\np_cl <- data.frame(cl_w = cutree(p_hcw, 3))\n\npenguins_mc <- Mclust(penguins_sub[,1:4], \n                      G=3, \n                      modelNames = \"EEE\")\np_cl <- p_cl %>% \n  mutate(cl_mc = penguins_mc$classification)\n\np_cl <- p_cl %>% \n  mutate(cl_w_j = jitter(cl_w),\n         cl_mc_j = jitter(cl_mc))\n\n# Arranging by cluster id is important to define edges \npenguins_cl <- penguins_sub %>%\n  mutate(cl_w = p_cl$cl_w,\n         cl_mc = p_cl$cl_mc) %>%\n  arrange(cl_w)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code for convex hulls in 2D\"}\n# Penguins in 2D\n# Duplicate observations need to be removed fo convex hull calculation\npsub <- penguins_cl %>%\n  select(bl, bd) \ndup <- duplicated(psub)\npsub <- penguins_cl %>%\n  select(bl, bd, cl_w) %>%\n  filter(!dup) %>%\n  arrange(cl_w)\n\nncl <- psub %>%\n  count(cl_w) %>%\n  arrange(cl_w) %>%\n  mutate(cumn = cumsum(n))\nphull <- NULL\nfor (i in unique(psub$cl_w)) {\n  x <- psub %>%\n    dplyr::filter(cl_w == i) %>%\n    select(bl, bd) \n  ph <- cxhull(as.matrix(x))$edges\n  if (i > 1) {\n    ph <- ph + ncl$cumn[i-1]\n  }\n  ph <- cbind(ph, rep(i, nrow(ph)))\n  phull <- rbind(phull, ph)\n}\nphull <- as.data.frame(phull)\ncolnames(phull) <- c(\"from\", \"to\", \"cl_w\") \nphull_segs <- data.frame(x = psub$bl[phull$from],\n                         y = psub$bd[phull$from],\n                         xend = psub$bl[phull$to],\n                         yend = psub$bd[phull$to],\n                         cl_w = phull$cl_w)\nphull_segs$cl_w <- factor(phull$cl_w) \npsub$cl_w <- factor(psub$cl_w)\np_chull2D <- ggplot() +\n  geom_point(data=psub, aes(x=bl, y=bd, \n                            colour=cl_w)) + \n  geom_segment(data=phull_segs, aes(x=x, xend=xend,\n                                    y=y, yend=yend,\n                                    colour=cl_w)) +\n  scale_colour_discrete_divergingx(palette = \"Zissou 1\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to generate pD convex hull and view in tour\"}\nncl <- penguins_cl %>%\n  count(cl_w) %>%\n  arrange(cl_w) %>%\n  mutate(cumn = cumsum(n))\nphull <- NULL\nfor (i in unique(penguins_cl$cl_w)) {\n  x <- penguins_cl %>%\n    dplyr::filter(cl_w == i) \n  ph <- cxhull(as.matrix(x[,1:4]))$edges\n  if (i > 1) {\n    ph <- ph + ncl$cumn[i-1]\n  }\n  ph <- cbind(ph, rep(i, nrow(ph)))\n  phull <- rbind(phull, ph)\n}\nphull <- as.data.frame(phull)\ncolnames(phull) <- c(\"from\", \"to\", \"cl_w\") \nphull$cl_w <- factor(phull$cl_w)\npenguins_cl$cl_w <- factor(penguins_cl$cl_w)\n\nanimate_xy(penguins_cl[,1:4], col=penguins_cl$cl_w,\n           edges=as.matrix(phull[,1:2]), edges.col=phull$cl_w)\nrender_gif(penguins_cl[,1:4], \n           tour_path = grand_tour(),\n           display = display_xy(col=penguins_cl$cl_w,\n                                edges=as.matrix(phull[,1:2]),\n                                edges.col=phull$cl_w),\n           gif_file = \"gifs/penguins_chull.gif\",\n           frames = 500, \n           width = 400,\n           height = 400)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {#fig-penguins-chull layout-ncol=2}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![2D](unsupervised-summary_files/figure-pdf/fig-penguin-hull-2D-1.pdf){#fig-penguin-hull-2D}\n:::\n:::\n\n\n::: {.content-hidden when-format=\"pdf\"}\n\n![4D](gifs/penguins_chull.gif){#fig-penguins-chull-pD}\n:::\n\nConvex hulls summarising the extent of Wards linkage clustering in 2D and 4D.\n:::\n\n## Comparing two clusterings\n\nEach cluster analysis will result in a vector of class labels for the data. To compare two results we would tabulate and plot the pair of integer variables. The labels given to each cluster will likely differ. If the two methods agree, there will be just a few cells with large counts among mostly empty cells. \n\nBelow is a comparison between the three cluster results of Wards linkage hierarchical clustering (rows) and model-based clustering (columns). The two methods mostly agree, as seen from the three cells with large counts, and most cells with zeros. They disagree only on eight penguins. These eight penguins would be considered to be part of cluster 1 by Wards, but model-based considers them to be members of cluster 2.\n\nThe two methods label them clusters differently: what Wards labels as cluster 3, model-based labels as cluster 2. The labels given by any algorithm are arbitrary, and can easily be changed to coordinate between methods. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code for confusion table\"}\np_cl %>% \n  count(cl_w, cl_mc) %>% \n  pivot_wider(names_from = cl_mc, \n              values_from = n, \n              values_fill = 0) %>%\n  gt() %>%\n  tab_spanner(label = \"cl_mc\", columns=c(`2`, `3`, `1`)) %>%\n  cols_width(everything() ~ px(60))\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{rrrr}\n\\toprule\n & \\multicolumn{3}{c}{cl\\_mc} \\\\ \n\\cmidrule(lr){2-4}\ncl\\_w & 2 & 3 & 1 \\\\ \n\\midrule\n1 & 8 & 0 & 149 \\\\ \n2 & 0 & 119 & 0 \\\\ \n3 & 57 & 0 & 0 \\\\ \n\\bottomrule\n\\end{longtable}\n:::\n:::\n\n\nWe can examine the disagreement by linking a plot of the table, with a tour plot. Here is how to do this with `liminal`. @fig-compare-clusters1 and @fig-compare-clusters2 show screenshots of the exploration of the eight penguins on which the methods disagree. It makes sense that there is some confusion. These penguins are part of the large clump of observations that don't separate cleanly into two clusters. The eight penguins are in the middle of this clump. Realistically, both methods result in a plausible clustering, and it is not clear how these penguins should be grouped.  \n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to do linked brushing with liminal\"}\nlibrary(liminal)\nlimn_tour_link(\n  p_cl[,3:4],\n  penguins_cl,\n  cols = bl:bm,\n  color = cl_w\n)\n```\n:::\n\n\n![Linking the confusion table with a tour using liminal. Points are coloured according to Wards linkage. The disagreement on eight penguins is with cluster 1 from Wards and cluster 2 from model-based.](images/compare-clusters1.png){#fig-compare-clusters1}\n\n![Highlighting the penguins where the methods disagree so we can see where these observations are located relative to the two clusters.](images/compare-clusters2.png){#fig-compare-clusters2}\n\n## Exercises {-}\n\n1. Compare the results of the four cluster model-based clustering with that of the four cluster Wards linkage clustering of the penguins data.\n\n## Project {-}\n\nMost of the time your data will not neatly separate into clusters, but partitioning it into groups of similar observations can still be useful. In this case our toolbox will be useful in comparing and contrasting different methods, understanding to what extend a cluster mean can describe the observations in the cluster, and also how the boundaries between clusters have been drawn. To explore this we will use survey data that examines the risk taking behavior of tourists. The data was collected in Australia in 2015 [@risk-survey] and includes six types of risks (recreational, health, career, financial, safety and social) with responses on a scale from 1 (never) to 5 (very often). You can download the data from XXX\n\n1. We first examine the data in a grand tour. Do you notice that each variable was measured on a discrete scale?\n2. Next we explore different solutions from hierarchical clustering of the data. For comparison we will keep the number of clusters fixed to 6 and we will perform the hierarchical clustering with different combinations of distance functions (Manhattan distance and Euclidean distance) and linkage (single, complete and Ward linkage). Which combinations make sense based on what we know about the method and the data?\n3. For each of the hierarchical clustering solutions draw the dendrogram in 2D and also in the data space. You can also map the grouping into 6 clusters to different colors. How would you describe the different solutions?\n4. Using the method introduced in this chapter, compare the solution using Manhattan distance and complete linkage to one using Euclidean distance and Ward linkage. What do you notice?\n5. Selecting your preferred solution from hierarchical clustering, we will now compare it to what is found using $k$-means clustering with $k=6$. Use a tour to show the cluster means together with the data points. What can you say about the variation within the clusters? Can you identify clear boundaries?\n6. Use a projection pursuit guided tour to best separate the clusters identified with $k$-means clustering. How are the clusters related to the different types of risk?\n7. Use the approaches from this chapter to summarize and compare the $k$-means solution to one of the hierarchical clustering results. Draw the convex hulls for the two solutions in a tour and compare the shapes.\n8. XXX SOM here\n\n\n::: {.cell}\n\n:::\n\n\n\n<!--\nTo compare the results of two methods we commonly compute a confusion table. For example, @tab-confusion is the confusion table for five-cluster solutions for the \\Data{Music} data from $k$-means and\nWard's linkage hierarchical clustering, generated by:\n\n```\n> d.music.dist <- dist(subset(d.music.std,\n   select=c(lvar:lfreq)))\n> d.music.hc <- hclust(d.music.dist, method=\"ward\")\n> cl5 <- cutree(d.music.hc,5)\n> d.music.km <- kmeans(subset(d.music.std,\n   select=c(lvar:lfreq)), 5)\n> table(d.music.km$cluster, cl5)\n> d.music.clustcompare <- \n    cbind(d.music.std,cl5,d.music.km$cluster)\n> names(d.music.clustcompare)[8] <- \"Wards\"\n> names(d.music.clustcompare)[9] <- \"km\"\n> gd <- ggobi(d.music.clustcompare)[1]\n```\n\n\\noindent The numerical labels of clusters are arbitrary, so these can be rearranged to better digest the table. There is a lot of agreement\nbetween the two methods: Both methods agree on the cluster for 48 tracks out of 62, or 77\\% of the time.  We want to explore the data space to see where the agreement occurs and where the two methodsdisagree.\n\n\\begin{center}\n\\begin{table}[h]\n\\caption[Tables showing the agreement between two solutions for the\n\\Data{Music} data]{Tables showing the agreement between two\nfive-cluster solutions for the \\Data{Music} data, showing a lot of\nagreement between $k$-means and Ward's linkage hierarchical\nclustering.  The rows have been rearranged to make the table more\nreadable.}\n\\begin{tabular}{cp{0.2in}p{1.3in}c}\n\\begin{tabular}{c@{\\hspace{.1in}}|rrrrr}\n& \\multicolumn{5}{c}{Ward's} \\\\\n\\B $k$-means & ~1 & ~2 & ~3 & 4 & 5 \\\\\\hline\n\\T 1 & 0 & 0 & 3 & 0 & 14 \\\\ \n2 & 0 & 0 & 1 & 0 & 0 \\\\\n3 & 0 & 9 & 5 & 0 & 0 \\\\\n4 & 8 & 2 & 1 & 0 & 0 \\\\\n5 & 0 & 0 & 3 & 16 & 0 \\\\\n\\end{tabular}\n& & Rearrange rows $\\Rightarrow$  &\n\\begin{tabular}{c@{\\hspace{.1in}}|rrrrr}\n& \\multicolumn{5}{c}{Ward's} \\\\\n\\B $k$-means & ~1 & ~2 & ~3 & 4 & 5 \\\\\\hline\n\\T 4 & 8 & 2 & 1 & 0 & 0 \\\\\n3 & 0 & 9 & 5 & 0 & 0 \\\\\n2 & 0 & 0 & 1 & 0 & 0 \\\\\n5 & 0 & 0 & 3 & 16 & 0 \\\\\n1 & 0 & 0 & 3 & 0 & 14 \\\\ \n\\end{tabular}\n\\end{tabular}\n\\label{confusion}\n\\vspace{.5em}\n\\end{table}\n\\end{center}\n\n% Figure 11\n\\begin{figure*}[htbp]\n\\centerline{{\\includegraphics[width=2in]{chap-clust/music-hc1a.pdf}}\n {\\includegraphics[width=2in]{chap-clust/music-hc1b.pdf}}}\n\\smallskip\n\\centerline{{\\includegraphics[width=2in]{chap-clust/music-hc2a.pdf}}\n {\\includegraphics[width=2in]{chap-clust/music-hc2b.pdf}}}\n\\caption[Comparing two five-cluster models of the \\Data{Music} data\nusing confusion tables linked to tour plots]{Comparing two\nfive-cluster models of the \\Data{Music} data using confusion tables\nlinked to tour plots.  In the confusion tables, $k$-means cluster\nidentifiers for each plot are plotted against Ward's linkage\nhierarchical clustering ids.  (The values have been jittered.)  Two\ndifferent areas of agreement have been highlighted, and the tour\nprojections show the tightness of each cluster where the methods\nagree. }\n\\label{clust-compare}\n\\end{figure*}\n\n\\index{jittering}\n\nIn @fig-clust-compare, we link jittered plots of the confusion table for the two clustering methods with 2D tour plots of the data. The first column contains two jittered plots of the confusion table.  In the top row of the figure, we have highlighted a group of 14 points that both methods agree form a cluster, painting them as orange triangles. From the plot at the right, we see that this cluster is a closely grouped set of points in the data space. From the tour axes we see that `lvar` has the largest axis pointing in the direction of the cluster separation, which suggests that music pieces\nin this cluster are characterized by high values on `lvar` (variable 3 in the data); that is, they have large variance in frequency. By further investigating which tracks are in this cluster, we can learn that it consists of a mix of tracks by the Beatles\n(\"Penny Lane\", \"Help\", \"Yellow Submarine\", ...)  and the Eels (\"Saturday Morning\", \"Love of the Loveless\", ...).\n\nIn the bottom row of the figure, we have highlighted a second group of tracks that were clustered together by both methods, painting them again using orange triangles.  In the plot to the right, we see that this cluster is closely grouped in the data space.  Despite that, this cluster is a bit more difficult to characterize.  It is oriented mostly in the negative direction of `lave` (variable 4), so it would have smaller values on this variable. But this vertical direction in\nthe plot also has large contributions from variables 3 (`lvar`) and 7 (`lfreq`).  If you label these eight points on your own, you will see that they are all Abba songs (\"Dancing Queen\", \"Waterloo\", \"Mamma Mia\", ...).\n\nWe have explored two groups of tracks where the methods agree. In a similar fashion, we could also explore the tracks where the methods disagree.\n\n\\index{cluster analysis!cluster characterization}\n\\section{Characterizing clusters}\n\nThe final step in a cluster analysis is to characterize the clusters. Actually, we have engaged in cluster characterization throughout the examples, because it is an intrinsic part of assessing the results of any cluster analysis.  If we cannot detect any numerical or qualitative differences between clusters, then our analysis was not successful, and we start over with a different distance metric or algorithm.\n\nHowever, once we are satisfied that we have found a set of clusters that can be differentiated from one another, we want to describe them more formally, both quantitatively and qualitatively.  We characterize\nthem quantitatively by computing such statistics as cluster means and standard deviations for each variable.  We can look at these results in tables and in plots, and we can refine the qualitative descriptions of the clusters we made during the assessment process.\n\n\\index{parallel coordinate plot}\n\nThe parallel coordinate plot is often used during this stage.\n#fig-clust-char shows the parallel coordinate plot for the\nfirst of the clusters of music pieces singled out for study in the\nprevious section. Ward's hierarchical linkage and $k$-means both\nagreed that these music pieces form a cluster.  Since the matrix and\nthe number of clusters are both small, we plot the raw data; for\nlarger problems, we might plot cluster statistics as well [see,\nfor example, @DSP05].\n\n\n% figure 12\n\\begin{figure*}[htbp]\n\\begin{center}\n  {\\includegraphics[width=4in]{chap-clust/music-clust1.pdf}}\n\\end{center}\n\\caption[Characterizing clusters in a parallel coordinate\nplot]{Characterizing clusters in a parallel coordinate plot.  The\nhighlighted profiles correspond to one cluster for which\nWard's hierarchical linkage and $k$-means were in agreement.  }\n\\label{clust-char}\n\\end{figure*}\n\nThis cluster containing a mix of Beatles and Eels music has high\nvalues on \\Vbl{lvar}, medium values of \\Vbl{lave}, high values of\n\\Vbl{lmax}, high values of \\Vbl{lfener}, and varied \\Vbl{lfreq}\nvalues. That is, these pieces of music have a large variance in\nfrequency, high frequency, and high energy relative to the other\nmusic pieces.\n\n## Recap {#chap-clust-recap}\n\n\nGraphics are invaluable for cluster analysis, whether they are used to find clusters or to interpret and evaluate the results of a cluster analysis arrived at by other means.\n\nThe spin and brush approach can be used to get an initial look at the data and to find clusters, and occasionally, it is sufficient.  When the clustering is the result of an algorithm, a very useful first step\nis to paint the points by cluster membership and to look at the data to see whether the clustering seems sensible.  How many clusters are there, and how big are they?  What shape are they, and do they overlap\none another?  Which variables have contributed most to the clustering? Can the clusters be qualitatively described?  All the plots we have described can be useful: scatterplots, parallel coordinate plots, and\narea plots, as well as static plots like dendrograms.\n\nWhen the clusters have been generated by a model, we should also use graphics to help us assess the model.  If the model makes distributional assumptions, we can generate ellipses and compare them with the clusters to see whether the shapes are consistent.  For self-organizing maps the tour can assist in uncovering problems with the fit, such as when the map wraps in on itself through the data making it appear that some cases are far apart when they are truly\nclose together. A confusion table can come alive with linked brushing, so that mismatches and agreements between methods can be\nexplored. \\index{brushing!linked}\n-->\n",
    "supporting": [
      "unsupervised-summary_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"unnamed-chunk-7\",\"unnamed-chunk-7\",\"unnamed-chunk-7\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"caption\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}