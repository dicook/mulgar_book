{
  "hash": "8824d86b98206c00fbd77aadb7d7fda3",
  "result": {
    "markdown": "# Neural networks and deep learning\n\nüõ†Ô∏è  **UNDER DEVELOPMENT**\n\nThis chapter will likely include:\n\n- Models at nodes or epochs \n- Predictions (like vote matrix) for training and test. It provides a visual guide to overfitting.\n- Classification boundaries comparison with other methods\n- Explainability and interpretability\n\n(This paper https://distill.pub/2020/grand-tour/ has good examples)\n\n\\index{classification methods!neural network}\n\n\n\n::: {.cell}\n\n:::\n\n\n\n<!--\nNeural networks for classification can be thought of as additive\nmodels where explanatory variables are transformed, usually through a\nlogistic function, added to other explanatory variables, transformed\nagain, and added again to yield class predictions. Aside from the data\nmining literature, mentioned earlier, a good comprehensive and\naccessible description for statisticians can be found in\n\\citeasnoun{CT94}. The model can be formulated as:\n\n\\[\n\\hat{y} = f(x) = \\phi(\\alpha+\\sum_{h=1}^{s}\nw_{h}\\phi(\\alpha_h+\\sum_{i=1}^{p} w_{ih}x_i))\n\\]\n\n\\index{R package!\\RPackage{nnet}}\n\n\\noindent where $x$ is the vector of explanatory variable values, $y$\nis the target value, $p$ is the number of variables, $s$ is the number\nof nodes in the single hidden layer, and $\\phi$ is a fixed function,\nusually a linear or logistic function. This model has a single hidden\nlayer and univariate output values.  The model is fit by minimizing\nthe sum of squared differences between observed values and fitted\nvalues, and the minimization does not always converge. A neural\nnetwork is a black box that accepts inputs, computes, and spits out\npredictions.  With graphics, some insight into the black box can be\ngained. We use the feed-forward neural network provided in the {\\tt\nnnet} package of R \\cite{VR02} to illustrate.\n\nWe continue to work with \\Data{Olive Oils}, and we look at the\nperformance of the neural network in classifying the oils in the four\nareas of the South, a difficult challenge. Because the software\ndoes not include a method for computing the predictive error, we break\nthe data into training and test samples so we can better estimate the\npredictive error.  (We could tweak the neural network to perfectly fit\nall the data, but then we could not estimate how well it would perform\nwith new data.)\n\n\\begin{verbatim}\n> indx.tst <- c(1,7,12,15,16,22,27,32,34,35,36,41,50,54,61,\n 68,70,75,76,80,95,101,102,105,106,110,116,118,119,122,134,\n 137,140,147,148,150,151,156,165,175,177,182,183,185,186,\n 187,190,192,194,201,202,211,213,217,218,219,225,227,241,\n 242,246,257,259,263,266,274,280,284,289,291,292,297,305,\n 310,313,314,323,330,333,338,341,342,347,351,352,356,358,\n 359,369,374,375,376,386,392,405,406,415,416,418,420,421,\n 423,426,428,435,440,451,458,460,462,466,468,470,474,476,\n 480,481,482,487,492,493,500,501,509,519,522,530,532,541,\n 543,545,546,551,559,567,570)\n> d.olive.train <- d.olive[-indx.tst,]\n> d.olive.test <- d.olive[indx.tst,]\n> d.olive.sth.train <- subset(d.olive.train, region==1, \n   select=area:eicosenoic)\n> d.olive.sth.test <- subset(d.olive.test, region==1, \n   select=area:eicosenoic)\n\\end{verbatim}\n\n% this may need a few more words\n\nAfter trying several values for $s$, the number of nodes in the hidden\nlayer, we chose $s=4$; we also chose a linear $\\phi$, $decay=0.005$,\nand $range=0.06$. We fit the model using many different random\nstarting values, rejecting the results until it eventually converged\nto a solution with a reasonably low error:\n\n\\begin{verbatim}\n> library(nnet)\n> olive.nn <- nnet(as.factor(area)~., d.olive.sth.train, \n  size=4, linout=T, decay=0.005, range=0.06, maxit=1000)\n> targetr <- class.ind(d.olive.sth.train[,1])\n> targets <- class.ind(d.olive.sth.test[,1])\n> test.cl <- function(true, pred){\n    true <- max.col(true)\n    cres <- max.col(pred)\n    table(true, cres)\n  }\n> test.cl(targetr, predict(olive.nn, \n  d.olive.sth.train[,-1]))\n    cres\ntrue   1   2   3   4\n   1  16   0   1   2\n   2   0  42   0   0\n   3   0   1 155   2\n   4   1   1   1  24\n\\end{verbatim}\n\\newpage  % Insert page break to avoid breaking the R output.\n\\begin{verbatim}\n> test.cl(targets, predict(olive.nn, d.olive.sth.test[,-1]))\n    cres\ntrue  1  2  3  4\n   1  3  2  0  1\n   2  0 12  2  0\n   3  0  2 45  1\n   4  1  2  1  5\n> parea <- c(max.col(predict(olive.nn, \n   d.olive.sth.train[,-1])),\n   max.col(predict(olive.nn, d.olive.sth.test[,-1])))\n> d.olive.nn <- cbind(rbind(d.olive.sth.train, \n    d.olive.sth.test), parea)\n> gd <- ggobi(d.olive.nn)[1]\n> glyph_color(gd) <- c(6,3,2,9)[d.olive.nn$area]\n\\end{verbatim}\n\nBelow are the misclassification tables for the training and test\nsamples.\n\n\\bigskip\n\\emph{Training:}\n\\begin{center}\n\\begin{tabular}{l@{\\hspace{.3in}}r@{\\hspace{1em}}r@{\\hspace{1em}}r@{\\hspace{1em}}r@{\\hspace{1.5em}}r@{\\hspace{2em}}r}\n\\B & & \\multicolumn{4}{c}{Predicted \\Vbl{area}} & Error \\\\\n\n\\T & & \\multicolumn{1}{c}{North}  & \\multicolumn{1}{c}{Calabria} & \\multicolumn{1}{c}{South}  & \\multicolumn{1}{c}{Sicily} &  \\\\\n\\B & & \\multicolumn{1}{c}{Apulia} &  & \\multicolumn{1}{c}{Apulia} & & \\\\ \\cline{3-7}\n\n\\T         & North Apulia & 16 & 0 & {\\bf 1} & {\\bf 2} & 0.158\\\\\n\\Vbl{area} & Calabria & 0 & 42 & 0 &  0 & 0.000\\\\\n           & South Apulia & 0 & {\\bf 1} & 155 & {\\bf 2} & 0.019\\\\\n\\B         & Sicily & {\\bf 1} & {\\bf 1} & {\\bf 1} & 24 & 0.111\\\\ \\cline{3-7}\n\\T         &        &         &         &         &    & 0.037\n\\end{tabular}\n\\end{center}\n\n\\bigskip\n\n\\emph{Test:}\n\\begin{center}\n\\begin{tabular}{l@{\\hspace{.3in}}r@{\\hspace{1em}}r@{\\hspace{1em}}r@{\\hspace{1em}}r@{\\hspace{1.5em}}r@{\\hspace{2em}}r}\n\\B & & \\multicolumn{4}{c}{Predicted \\Vbl{area}} & Error \\\\\n\n\\T & & \\multicolumn{1}{c}{North}  & \\multicolumn{1}{c}{Calabria} & \\multicolumn{1}{c}{South}  & \\multicolumn{1}{c}{Sicily} &  \\\\\n\\B & & \\multicolumn{1}{c}{Apulia} &  & \\multicolumn{1}{c}{Apulia} & & \\\\  \\cline{3-7}\n\n\\T         & North Apulia & 3 & {\\bf 2} & 0 & {\\bf 1} & 0.333\\\\\n\\Vbl{area} & Calabria & 0 & 12 & {\\bf 2} &  0 & 0.143\\\\\n           & South Apulia & 0 & {\\bf 2} & 45 & {\\bf 1} & 0.063\\\\\n\\B         & Sicily & {\\bf 1} & {\\bf 2} & {\\bf 1} & 5 & 0.444\\\\  \\cline{3-7}\n\\T         &        &         &         &         &    & 0.156\n\\end{tabular}\n\\end{center}\n\n\\bigskip\n\n\\noindent The training error is $9/246=0.037$, and the test error is\n$12/77=0.156$.  The overall errors, as in the random forest model, are\nnot uniform across classes.  This is particularly obvious in the test\nerror table: The error in classifying North Apulian oils is close to a\nthird, and it is even worse for Sicilian oils, which have an almost\neven chance of being misclassified.\n\nOur exploration of the misclassifications is shown in\nFig.~\\ref{olive-nn}.  (The troublesome Sicilian oils have been\nexcluded from all plots in this figure.)  Consider first the plots in\nthe top row.  The left-hand plot shows the misclassification table.\nTwo samples of oils from North Apulia (orange $+$) have been\nincorrectly classified as South Apulian (pink $\\times$), and these two\npoints have been brushed as filled orange circles.  Note where these\npoints fall in the next two plots, which are linked 2D tour\nprojections. \\index{brushing!linked}\\index{tour!grand} One of the two\nmisclassified points is on the edge of the cluster of North Apulian\npoints, close to the Calabrian cluster. It is understandable that\nthere might be some confusion about this case. The other sample is on\nthe outer edge of the North Apulian cluster, but it is far from the\nCalabrian cluster ~---~ this should not have been confused.\n\n% Figure 13\n\\begin{figure*}[htbp]\n\\centerline{\n {\\includegraphics[width=1.5in]{chap-class/olive-nn1.pdf}}\n {\\includegraphics[width=1.5in]{chap-class/olive-nn2.pdf}}\n {\\includegraphics[width=1.5in]{chap-class/olive-nn8.pdf}}}\n\\smallskip\n\\centerline{\n {\\includegraphics[width=1.5in]{chap-class/olive-nn3.pdf}}\n {\\includegraphics[width=1.5in]{chap-class/olive-nn4.pdf}}\n {\\includegraphics[width=1.5in]{chap-class/olive-nn6.pdf}}}\n\\caption[Misclassifications of a feed-forward neural network\nclassifying the oils from the South]{Misclassifications of a\nfeed-forward neural network classifying the oils from the South by\n\\Vbl{area}. A representation of the misclassification table {\\bf (left\ncolumn)} is linked to projections viewed in a 2D tour. Different\nmisclassifications are examined in the top and bottom rows. (The\nSicilian oils, which would have appeared in the top row of the\nmisclassification tables, have been removed from all plots.)  }\n\\label{olive-nn}\n\\end{figure*}\n\nIn the bottom row of plots, we follow the same procedure to examine\nthe single North Apulian sample misclassified as South Apulian. It is\npainted as a filled orange circle in the misclassification plot and\nviewed in a \\index{tour!grand} tour. This point is on the outer edge\nof the North Apulian cluster, but it is closer to the Calabrian cluster\nthan the South Apulian cluster. It would be understandable for it to\nbe misclassified as Calabrian, so it is puzzling that it is\nmisclassified as South Apulian.\n\nIn summary, a neural network is a black box method for tackling tough\nclassification problems. It will generate different solutions each\ntime the net is fit, some much better than others. When numerical\nmeasures suggest that a reasonable model has been found, graphics can\nbe used to inspect the model in more detail.\n-->\n",
    "supporting": [
      "17-nn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}