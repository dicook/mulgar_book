{
  "hash": "d96ae207794e85506c5dc722ffbd44c9",
  "result": {
    "markdown": "# Neural networks and deep learning\n\\index{classification!neural networks}\n\nNeural networks can be considered to be nested additive (or even ensemble) models where explanatory variables are combined, and transformed through an activation function like a logistic, added to other combinations of explanatory variables recursively to yield class predictions. They are considered to be the supreme black box models. Although as their complexity grows with difficult classification tasks which make it unappealing to understand, there is a growing demand for interpretability. In the simplest form, we might write the equation for a neural network as\n\n$$\n\\hat{y} = f(x) = \\phi(a_0+\\sum_{h=1}^{s}\nw_{0h}\\phi(a_h+\\sum_{i=1}^{p} w_{ih}x_i))\n$$\nwhere $s$ indicates the number of nodes in the hidden (middle layer), and $\\phi$ is a choice of activation function. In a simple situation where $p=3$, $s=2$ and $\\phi$ is linear, in theory, the model could be written as:\n\n$$\n\\hat{y} = a_0+w_{01}(a_1+w_{11}x_1+w_{21}x_2+w_{31}x_3) +\\\\\nw_{02}(a_2+w_{12}x_1+w_{22}x_2+w_{32}x_3)\n$$\nIn practice, there are often many nodes, and several hidden layers, a variety of activation functions, and regularisation modifications. One of the dangers with applying neural networks, is using overly complex construction which is over-parameterised. Although, the fitting procedures have vastly improved, providing more stable solutions, it is important to keep in mind that choices made in the model definition are important.\n\nFor these examples we use the software `keras` [@keras] following the installation and tutorial details at [https://tensorflow.rstudio.com/tutorials/](https://tensorflow.rstudio.com/tutorials/). Because it is an interface to python it can be tricky to install. If this is a problem, the example code should be possible to convert to use `nnet` [@VR02] or `neuralnet` [@neuralnet]. We will use the penguins data to illustrate the fitting, because it makes it easier to understand the procedures and the fit. However, a neural network is like using a jackhammer instead of a trowel on a pot plat, more complicated than necessary to build a good classification model.\n\n## Setting up the model \n\nA first step is to decide how many nodes the neural network architecture should have, and and what activation function should be used. To make these decisions, ideally you already has some knowledge of the shapes of class clusters. For the \npenguins classification, we have seen that it contains three elliptically shaped clusters of roughly the same size. This suggests two nodes in the hidden layer would be sufficient to separate three clusters. Because the shapes of the clusters are convex, using linear activation (\"relu\") will also be sufficient.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Define model\np_nn_model <- keras_model_sequential()\np_nn_model %>% \n  layer_dense(units = 2, activation = 'relu', \n              input_shape = 4) %>% \n  layer_dense(units = 3, activation = 'softmax')\np_nn_model %>% summary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential\"\n______________________________________________________________\n Layer (type)              Output Shape             Param #   \n==============================================================\n dense_1 (Dense)           (None, 2)                10        \n dense (Dense)             (None, 3)                9         \n==============================================================\nTotal params: 19 (76.00 Byte)\nTrainable params: 19 (76.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n______________________________________________________________\n```\n:::\n\n```{.r .cell-code}\nloss_fn <- loss_sparse_categorical_crossentropy(\n  from_logits = TRUE)\n\np_nn_model %>% compile(\n  optimizer = \"adam\",\n  loss      = loss_fn,\n  metrics   = c('accuracy')\n)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Checking the training/test split\n\nSplitting the data into training and test is an essential way to protect against overfitting, for most classifiers, but especially so for the copiously parameterised neural networks. The model specified for the penguins data with only two nodes is unlikely to be overfitted, but it is nevertheless good practice to use a training set for building and a test set for evaluation. \n\n@fig-p-split-pdf shows the tour being used to examine the split into training and test samples for the penguins data. Using random sampling, particularly stratified by group, should result in similar samples, as can be seen here. It does happen that several observations in the test set are on the extremes of their class cluster, so it could be that the model makes errors here.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the data intro training and testing\nlibrary(dplyr)\nlibrary(rsample)\nload(\"data/penguins_sub.rda\") # from mulgar book\n\nset.seed(821)\np_split <- penguins_sub %>% \n  select(bl:species) %>%\n  initial_split(prop = 2/3, \n                strata=species)\np_train <- training(p_split)\np_test <- testing(p_split)\n\n# Check training and test split\np_split_check <- bind_rows(\n  bind_cols(p_train, type = \"train\"), \n  bind_cols(p_test, type = \"test\")) %>%\n  mutate(type = factor(type))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {#fig-p-split-html layout-ncol=2}\n\n![Grand tour](gifs/p_split.gif){#fig-split-grand fig-alt=\"FIX ME\" width=300}\n\n![Guided tour](gifs/p_split_guided.gif){#fig-split-guided fig-alt=\"FIX ME\" width=300}\n\nEvaluating the training/test split. The two samples should roughly match, like they do here. There are a few observations in the test set that are on the outer edges of the clusters, which will likely result in the model making an error for them.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n::: {#fig-p-split-pdf layout-ncol=2}\n\n![Grand tour](images/p_split.png){#fig-split-grand fig-alt=\"FIX ME\" width=220}\n\n![Guided tour](images/p_split_guided.png){#fig-split-guided fig-alt=\"FIX ME\" width=220}\n\nEvaluating the training/test split. The two samples should roughly match, like they do here. There are a few observations in the test set that are on the outer edges of the clusters, which will likely result in the model making an error for them.\n:::\n:::\n\n## Fit the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data needs to be matrix, and response needs to be numeric\np_train_x <- p_train %>%\n  select(bl:bm) %>%\n  as.matrix()\np_train_y <- p_train %>% pull(species) %>% as.numeric() \np_train_y <- p_train_y-1 # Needs to be 0, 1, 2\np_test_x <- p_test %>%\n  select(bl:bm) %>%\n  as.matrix()\np_test_y <- p_test %>% pull(species) %>% as.numeric() \np_test_y <- p_test_y-1 # Needs to be 0, 1, 2\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit model\nset.seed(211)\np_nn_fit <- p_nn_model %>% keras::fit(\n  x = p_train_x, \n  y = p_train_y,\n  epochs = 200,\n  verbose = 0\n)\n\n# Check\np_nn_model %>% evaluate(p_test_x, p_test_y, verbose = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     loss  accuracy \n0.2109162 0.9375000 \n```\n:::\n\n```{.r .cell-code}\nplot(p_nn_fit)\n```\n\n::: {.cell-output-display}\n![](17-nn_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H' width=80%}\n:::\n\n```{.r .cell-code}\n# Save model: does not work!\n# save_model_tf(p_nn_model, \"data/penguins_cnn\")\n```\n:::\n\n\n\n\n\n## Extracting model components\n\n<!-- Weights, and plotting nodes: reference Removing the blindfold-->\n\nBecause nodes in the hidden layers of neural networks are themselves (relatively simple regression) models, it can be helpful to examine them to understand the overall model. Most software will allow the coefficients for the models at these nodes to be extracted. For the penguins example there are two nodes, so we can extract the coefficients and plot the resulting two linear combinations to examine the separation between classes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(colorspace)\n\n# Extract hidden layer model weights\np_nn_wgts <- keras::get_weights(p_nn_model, trainable=TRUE)\np_nn_wgts \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n           [,1]       [,2]\n[1,]  0.2706964  0.9599816\n[2,] -1.1342078  0.5203390\n[3,]  0.2714159 -0.4919693\n[4,]  0.7671224  0.1536695\n\n[[2]]\n[1]  0.5250872 -0.0965992\n\n[[3]]\n          [,1]       [,2]      [,3]\n[1,] -1.243448 0.08659787 1.6951888\n[2,] -1.157857 0.56848991 0.1315699\n\n[[4]]\n[1]  0.2671503  0.1900631 -0.6711022\n```\n:::\n\n```{.r .cell-code}\n# Hidden layer\np_train_m <- p_train %>%\n  mutate(nn1 = as.matrix(p_train[,1:4]) %*%\n           as.matrix(p_nn_wgts[[1]][,1], ncol=1),\n         nn2 = as.matrix(p_train[,1:4]) %*%\n           matrix(p_nn_wgts[[1]][,2], ncol=1))\n\n# Now add the test points on.\np_test_m <- p_test %>%\n  mutate(nn1 = as.matrix(p_test[,1:4]) %*%\n           as.matrix(p_nn_wgts[[1]][,1], ncol=1),\n         nn2 = as.matrix(p_test[,1:4]) %*%\n           matrix(p_nn_wgts[[1]][,2], ncol=1))\np_train_m <- p_train_m %>%\n  mutate(set = \"train\")\np_test_m <- p_test_m %>%\n  mutate(set = \"test\")\np_all_m <- bind_rows(p_train_m, p_test_m)\nggplot(p_all_m, aes(x=nn1, y=nn2, \n                     colour=species, shape=set)) + \n  geom_point() +\n  scale_colour_discrete_divergingx(palette=\"Zissou 1\") +\n  scale_shape_manual(values=c(16, 1)) +\n  theme_minimal() +\n  theme(aspect.ratio=1)\n```\n\n::: {.cell-output-display}\n![](17-nn_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H' width=80%}\n:::\n:::\n\n\n\n## Examining predictive probabilities\n\\index{classification!predictive probabilities}\n\n## Building a diagnostic data set\n\n<!--\nThis chapter will likely include:\n\n- Models at nodes or epochs \n- Predictions (like vote matrix) for training and test. It provides a visual guide to overfitting.\n- Classification boundaries comparison with other methods\n- Explainability and interpretability\n\n(This paper https://distill.pub/2020/grand-tour/ has good examples)\n\nNOTE: Results might vary with different knits \n\nReferences keras/tensorflow book, tidymodels, interpretable machine learning, and removing the blindfold\n-->\n\nOutline for chapter:\n\n- Penguins data\n    - Setting up with the CNN with keras\n    - Splitting training and test, checking\n    - Specifying model, choices of layers\n    - Making predictions\n    - Extracting weights\n    - Building your diagnostic data set\n    - Examine predictive probabilities with simplex\n    - Examining the nodes - like the discriminant space when its 2\n    - Misclassifications\n    - \n\nBushfires\n    - Model fitting when regularisation needed\n    - Overfitting\n    \nSketches as an example  \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-penguins-lda-tree-html layout-ncol=2}\n\n![LDA model](gifs/penguins_lda_boundaries.gif){#fig-lda-boundary fig-alt=\"FIX ME\" width=300}\n\n![NN model](gifs/penguins_nn_boundaries.gif){#fig-tree-boundary fig-alt=\"FIX ME\" width=300}\n\nComparison of the boundaries produced by the LDA (a) and the CNN (b) model, using a slice tour. \n:::\n:::\n\n\\index{tour!slice} \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Explore probabilistic predictions like vote matrix\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now look at interpretability metrics\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n<!--\nNeural networks for classification can be thought of as additive\nmodels where explanatory variables are transformed, usually through a\nlogistic function, added to other explanatory variables, transformed\nagain, and added again to yield class predictions. Aside from the data\nmining literature, mentioned earlier, a good comprehensive and\naccessible description for statisticians can be found in\n\\citeasnoun{CT94}. The model can be formulated as:\n\n\\[\n\\hat{y} = f(x) = \\phi(\\alpha+\\sum_{h=1}^{s}\nw_{h}\\phi(\\alpha_h+\\sum_{i=1}^{p} w_{ih}x_i))\n\\]\n\n\\index{R package!\\RPackage{nnet}}\n\n\\noindent where $x$ is the vector of explanatory variable values, $y$\nis the target value, $p$ is the number of variables, $s$ is the number\nof nodes in the single hidden layer, and $\\phi$ is a fixed function,\nusually a linear or logistic function. This model has a single hidden\nlayer and univariate output values.  The model is fit by minimizing\nthe sum of squared differences between observed values and fitted\nvalues, and the minimization does not always converge. A neural\nnetwork is a black box that accepts inputs, computes, and spits out\npredictions.  With graphics, some insight into the black box can be\ngained. We use the feed-forward neural network provided in the {\\tt\nnnet} package of R \\cite{VR02} to illustrate.\n\nWe continue to work with \\Data{Olive Oils}, and we look at the\nperformance of the neural network in classifying the oils in the four\nareas of the South, a difficult challenge. Because the software\ndoes not include a method for computing the predictive error, we break\nthe data into training and test samples so we can better estimate the\npredictive error.  (We could tweak the neural network to perfectly fit\nall the data, but then we could not estimate how well it would perform\nwith new data.)\n\n\\begin{verbatim}\n> indx.tst <- c(1,7,12,15,16,22,27,32,34,35,36,41,50,54,61,\n 68,70,75,76,80,95,101,102,105,106,110,116,118,119,122,134,\n 137,140,147,148,150,151,156,165,175,177,182,183,185,186,\n 187,190,192,194,201,202,211,213,217,218,219,225,227,241,\n 242,246,257,259,263,266,274,280,284,289,291,292,297,305,\n 310,313,314,323,330,333,338,341,342,347,351,352,356,358,\n 359,369,374,375,376,386,392,405,406,415,416,418,420,421,\n 423,426,428,435,440,451,458,460,462,466,468,470,474,476,\n 480,481,482,487,492,493,500,501,509,519,522,530,532,541,\n 543,545,546,551,559,567,570)\n> d.olive.train <- d.olive[-indx.tst,]\n> d.olive.test <- d.olive[indx.tst,]\n> d.olive.sth.train <- subset(d.olive.train, region==1, \n   select=area:eicosenoic)\n> d.olive.sth.test <- subset(d.olive.test, region==1, \n   select=area:eicosenoic)\n\\end{verbatim}\n\n% this may need a few more words\n\nAfter trying several values for $s$, the number of nodes in the hidden\nlayer, we chose $s=4$; we also chose a linear $\\phi$, $decay=0.005$,\nand $range=0.06$. We fit the model using many different random\nstarting values, rejecting the results until it eventually converged\nto a solution with a reasonably low error:\n\n\\begin{verbatim}\n> library(nnet)\n> olive.nn <- nnet(as.factor(area)~., d.olive.sth.train, \n  size=4, linout=T, decay=0.005, range=0.06, maxit=1000)\n> targetr <- class.ind(d.olive.sth.train[,1])\n> targets <- class.ind(d.olive.sth.test[,1])\n> test.cl <- function(true, pred){\n    true <- max.col(true)\n    cres <- max.col(pred)\n    table(true, cres)\n  }\n> test.cl(targetr, predict(olive.nn, \n  d.olive.sth.train[,-1]))\n    cres\ntrue   1   2   3   4\n   1  16   0   1   2\n   2   0  42   0   0\n   3   0   1 155   2\n   4   1   1   1  24\n\\end{verbatim}\n\\newpage  % Insert page break to avoid breaking the R output.\n\\begin{verbatim}\n> test.cl(targets, predict(olive.nn, d.olive.sth.test[,-1]))\n    cres\ntrue  1  2  3  4\n   1  3  2  0  1\n   2  0 12  2  0\n   3  0  2 45  1\n   4  1  2  1  5\n> parea <- c(max.col(predict(olive.nn, \n   d.olive.sth.train[,-1])),\n   max.col(predict(olive.nn, d.olive.sth.test[,-1])))\n> d.olive.nn <- cbind(rbind(d.olive.sth.train, \n    d.olive.sth.test), parea)\n> gd <- ggobi(d.olive.nn)[1]\n> glyph_color(gd) <- c(6,3,2,9)[d.olive.nn$area]\n\\end{verbatim}\n\nBelow are the misclassification tables for the training and test\nsamples.\n\n\\bigskip\n\\emph{Training:}\n\\begin{center}\n\\begin{tabular}{l@{\\hspace{.3in}}r@{\\hspace{1em}}r@{\\hspace{1em}}r@{\\hspace{1em}}r@{\\hspace{1.5em}}r@{\\hspace{2em}}r}\n\\B & & \\multicolumn{4}{c}{Predicted \\Vbl{area}} & Error \\\\\n\n\\T & & \\multicolumn{1}{c}{North}  & \\multicolumn{1}{c}{Calabria} & \\multicolumn{1}{c}{South}  & \\multicolumn{1}{c}{Sicily} &  \\\\\n\\B & & \\multicolumn{1}{c}{Apulia} &  & \\multicolumn{1}{c}{Apulia} & & \\\\ \\cline{3-7}\n\n\\T         & North Apulia & 16 & 0 & {\\bf 1} & {\\bf 2} & 0.158\\\\\n\\Vbl{area} & Calabria & 0 & 42 & 0 &  0 & 0.000\\\\\n           & South Apulia & 0 & {\\bf 1} & 155 & {\\bf 2} & 0.019\\\\\n\\B         & Sicily & {\\bf 1} & {\\bf 1} & {\\bf 1} & 24 & 0.111\\\\ \\cline{3-7}\n\\T         &        &         &         &         &    & 0.037\n\\end{tabular}\n\\end{center}\n\n\\bigskip\n\n\\emph{Test:}\n\\begin{center}\n\\begin{tabular}{l@{\\hspace{.3in}}r@{\\hspace{1em}}r@{\\hspace{1em}}r@{\\hspace{1em}}r@{\\hspace{1.5em}}r@{\\hspace{2em}}r}\n\\B & & \\multicolumn{4}{c}{Predicted \\Vbl{area}} & Error \\\\\n\n\\T & & \\multicolumn{1}{c}{North}  & \\multicolumn{1}{c}{Calabria} & \\multicolumn{1}{c}{South}  & \\multicolumn{1}{c}{Sicily} &  \\\\\n\\B & & \\multicolumn{1}{c}{Apulia} &  & \\multicolumn{1}{c}{Apulia} & & \\\\  \\cline{3-7}\n\n\\T         & North Apulia & 3 & {\\bf 2} & 0 & {\\bf 1} & 0.333\\\\\n\\Vbl{area} & Calabria & 0 & 12 & {\\bf 2} &  0 & 0.143\\\\\n           & South Apulia & 0 & {\\bf 2} & 45 & {\\bf 1} & 0.063\\\\\n\\B         & Sicily & {\\bf 1} & {\\bf 2} & {\\bf 1} & 5 & 0.444\\\\  \\cline{3-7}\n\\T         &        &         &         &         &    & 0.156\n\\end{tabular}\n\\end{center}\n\n\\bigskip\n\n\\noindent The training error is $9/246=0.037$, and the test error is\n$12/77=0.156$.  The overall errors, as in the random forest model, are\nnot uniform across classes.  This is particularly obvious in the test\nerror table: The error in classifying North Apulian oils is close to a\nthird, and it is even worse for Sicilian oils, which have an almost\neven chance of being misclassified.\n\nOur exploration of the misclassifications is shown in\nFig.~\\ref{olive-nn}.  (The troublesome Sicilian oils have been\nexcluded from all plots in this figure.)  Consider first the plots in\nthe top row.  The left-hand plot shows the misclassification table.\nTwo samples of oils from North Apulia (orange $+$) have been\nincorrectly classified as South Apulian (pink $\\times$), and these two\npoints have been brushed as filled orange circles.  Note where these\npoints fall in the next two plots, which are linked 2D tour\nprojections. \\index{brushing!linked}\\index{tour!grand} One of the two\nmisclassified points is on the edge of the cluster of North Apulian\npoints, close to the Calabrian cluster. It is understandable that\nthere might be some confusion about this case. The other sample is on\nthe outer edge of the North Apulian cluster, but it is far from the\nCalabrian cluster ~---~ this should not have been confused.\n\n% Figure 13\n\\begin{figure*}[htbp]\n\\centerline{\n {\\includegraphics[width=1.5in]{chap-class/olive-nn1.pdf}}\n {\\includegraphics[width=1.5in]{chap-class/olive-nn2.pdf}}\n {\\includegraphics[width=1.5in]{chap-class/olive-nn8.pdf}}}\n\\smallskip\n\\centerline{\n {\\includegraphics[width=1.5in]{chap-class/olive-nn3.pdf}}\n {\\includegraphics[width=1.5in]{chap-class/olive-nn4.pdf}}\n {\\includegraphics[width=1.5in]{chap-class/olive-nn6.pdf}}}\n\\caption[Misclassifications of a feed-forward neural network\nclassifying the oils from the South]{Misclassifications of a\nfeed-forward neural network classifying the oils from the South by\n\\Vbl{area}. A representation of the misclassification table {\\bf (left\ncolumn)} is linked to projections viewed in a 2D tour. Different\nmisclassifications are examined in the top and bottom rows. (The\nSicilian oils, which would have appeared in the top row of the\nmisclassification tables, have been removed from all plots.)  }\n\\label{olive-nn}\n\\end{figure*}\n\nIn the bottom row of plots, we follow the same procedure to examine\nthe single North Apulian sample misclassified as South Apulian. It is\npainted as a filled orange circle in the misclassification plot and\nviewed in a \\index{tour!grand} tour. This point is on the outer edge\nof the North Apulian cluster, but it is closer to the Calabrian cluster\nthan the South Apulian cluster. It would be understandable for it to\nbe misclassified as Calabrian, so it is puzzling that it is\nmisclassified as South Apulian.\n\nIn summary, a neural network is a black box method for tackling tough\nclassification problems. It will generate different solutions each\ntime the net is fit, some much better than others. When numerical\nmeasures suggest that a reasonable model has been found, graphics can\nbe used to inspect the model in more detail.\n-->\n",
    "supporting": [
      "17-nn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}