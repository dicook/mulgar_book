{
  "hash": "1c5e7aac9ad845498cb63b30eddecc8b",
  "result": {
    "markdown": "# Trees and forests {#sec-trees-forests}\n\n## Trees {#sec-trees}\n\\index{classification!trees}\n\nThe tree algorithm [@BFOS84] is a simple and versatile algorithmic method for supervised classification. The basic tree algorithm generates a classification rule by sequentially splitting the data into two buckets. Splits are made between sorted data values of individual variables, with the goal of obtaining pure classes on each side of the split. The inputs for a simple tree classifier commonly include (1) an impurity measure, an indication of the relative diversity among the cases in the terminal nodes; (2) a parameter that sets the minimum number of cases in a node, or the minimum number of observations in a terminal node of the tree; and (3) a complexity measure that controls the growth of a tree, balancing the use of a simple generalizable tree against a more accurate tree\ntailored to the sample.  When applying tree methods, exploring the effects of the input parameters on the tree is instructive; for example, it helps us to assess the stability of the tree model.\n\nAlthough algorithmic models do not depend on distributional assumptions, that does not mean that every algorithm is suitable for all data.  For example, the tree model works best when all variables are independent within each class, because it does not take such dependencies into account.  Visualization can help us to determine whether a particular model should be applied.  In classification problems, it is useful to explore the cluster structure, comparing the clusters with the classes and looking for evidence of correlation within each class. \nThe plots in @fig-lda-assumptions1 and @fig-penguins-lda-ellipses-pdf shows a strong correlation between the variables within each species, which indicates that the tree model may not give good results for the penguins data. We'll show how this is the case with two variables initially, and then extend to the four variables.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The association between variables in the penguins data causes problems for fitting a tree model. Although the model, computed using only bl and bd, is simple (left), the fit is poor (right) because it doesn't adequately utilise combinations of variables.](15-forests_files/figure-pdf/fig-p-bl-bd-tree-1.pdf){#fig-p-bl-bd-tree fig-alt='Tree diagram with top split bl<0.3004, leading to Adelie branch, second split at bd >= -0.4138, leading to Gentoo branch, and final split at bl< 0.1476, leading to Adelie and Chinstrap branches. The scatterplot at right shows bd vs bl, with three predictive region partitions, and the data is overplotted. The elliptical spreads of data points crosses the rectangular partitions in places.' width=100%}\n:::\n:::\n\n\n\nThe plots in @fig-p-bl-bd-tree show the inadequacies of the tree fit. The background color indicates the class predictions, and thus boundaries produced by the tree fit. They can be seen to be boxy, and missing the elliptical nature of the penguin clusters. This produces errors in the classification of observations which are indefensible. One could always force the tree to fit the data more closely by adjusting the parameters, but the main problem persists: that one is trying to fit elliptical shapes using boxes.\n\n::: {.content-visible when-format=\"html\"}\n::: info\nThere are less strict assumptions for a non-parametric model but it is still important to understand the model fit relative to the data. \n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\infobox{There are less strict assumptions for a non-parametric model but it is still important to understand the model fit relative to the data. \n}\n:::\n\nThe boundaries for the tree model on all four variables of the penguins data can be viewed similarly, by predicting a set of points randomly generated in the 4D domain of observed values. @fig-penguins-lda-tree-pdf shows the prediction regions for LDA and a default tree in a slice tour. The slice tour is used to help see into the middle of the 4D cube. It slices the cube through the centre of the data, where the boundaries of the regions should meet. \n\nThe prediction regions of the default fitted tree are shown in comparison to those from the LDA model. We don't show the tree diagram here, but it makes only six splits of the tree model, which is delightfully simple. However, just like the model fitted to two variables, the result is not adequate for the penguins data. The tree model generates boxy boundaries, whereas the LDA model splits the 4D cube obliquely. The boxy regions don't capture the differences between the elliptically-shaped clusters. Overlaying the observed data on this display would make this clearer, but the boundaries are easier to examine without them.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\\index{tour!slice} \n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-penguins-lda-tree-html layout-ncol=2}\n\n![LDA model](gifs/penguins_lda_boundaries.gif){#fig-lda-boundary fig-alt=\"FIX ME\" width=300}\n\n![Tree model](gifs/penguins_tree_boundaries.gif){#fig-tree-boundary fig-alt=\"FIX ME\" width=300}\n\nComparison of the boundaries produced by the LDA (a) and the tree (b) model, using a slice tour. The tree boundaries are more box-shaped than the LDA boundaries, which does not adequately capture the differences between the elliptically-shaped clusters of the penguins data.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n::: {#fig-penguins-lda-tree-pdf layout-ncol=2}\n\n![LDA model](images/penguins_lda_boundaries.png){#fig-lda-boundary fig-alt=\"FIX ME\"}\n\n![Tree model](images/penguins_tree_boundaries.png){#fig-tree-boundary fig-alt=\"FIX ME\"}\n\nComparison of the boundaries produced by the LDA (a) and the tree (b) model, using a slice tour. (Here only a single frame is shown.) The tree boundaries are more box-shaped than the LDA boundaries, which does not adequately capture the differences between the elliptically-shaped clusters of the penguins data.\n:::\n:::\n\n\n::: {.content-visible when-format=\"html\"}\n::: insight\nWith the penguins data, a tree model may not be a good choice due to the strong correlation between variables. The best separation is in combinations of variables, not the single variable tree splits. \n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\insightbox{With the penguins data, a tree model may not be a good choice due to the strong correlation between variables. The best separation is in combinations of variables, not the single variable tree splits.}\n:::\n\n\n\n## Random forests \n\\index{classification!random forest}\n\nA random forest [@Br01] is a classifier that is built from multiple trees generated by randomly sampling the cases and the variables.  The random sampling (with replacement) of cases has the fortunate effect of creating a training (\"in-bag\") and a test (\"out-of-bag\") sample for each tree computed.  The class of each case in the out-of-bag sample for each tree is predicted, and the predictions for all trees are combined into a vote for the class identity.  \n\nA random forest is a computationally intensive method, a \"black box\" classifier, but it produces several diagnostics that make the outcome less mysterious.  Some diagnostics that help us to assess the model are the votes, the measure of variable importance, and the proximity matrix.\n\n### Examining the votes matrix {#sec-votes}\n\nHere we show how to use the `randomForest` [@randomForest2002] votes matrix for the penguins data to investigate confusion between classes, and observations which are problematic to classify. The votes matrix can be considered to be predictive probability distribution, where the values for each observation sum to 1. With only three classes the votes matrix is only a 2D object, and thus easy to examine. With four or more classes the votes matrix needs to be examined in a tour. \n\\index{classification!vote matrix}\n\\index{classification!predictive probability distribution}\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(randomForest)\nlibrary(dplyr)\npenguins_rf <- randomForest(species~.,\n                             data=penguins_sub[,1:5],\n                             importance=TRUE)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\nTo examine the votes matrix, we extract the `votes` element from the random forest model object. The first five rows are:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(penguins_rf$votes, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Adelie Chinstrap Gentoo\n1 1.0000   0.00000      0\n2 0.9392   0.06077      0\n3 0.9822   0.01775      0\n4 1.0000   0.00000      0\n5 1.0000   0.00000      0\n```\n:::\n:::\n\n\n\nThis has three columns corresponding to the three species, but because each row is a set of proportions it is only a 2D object. To reduce the dimension from 3D to the 2D we use a Helmert matrix [@helmert]. A Helmert matrix has a first row of all 1's. The remaining components of the matrix are 1's in the lower triangle, and 0's in the upper triangle and the diagonal elements are the negative row sum. The rows are usually normalised to have length 1. They are used to create contrasts to test combinations of factor levels for post-testing after Analysis of Variance (ANOVA). For compositional data, like the votes matrix, when the first row is removed a Helmert matrix can be used to reduce the dimension appropriately. For three classes, this will generate the common 2D ternary diagram, but for higher dimensions it will reduce to a $(g-1)$-dimensional simplex. For the penguins data, the Helmert matrix for 3D is \n\\index{ternary diagram}\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to compute Helmert matrix\"}\ngeozoo::f_helmert(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]    [,2]    [,3]\nhelmert 0.5774  0.5774  0.5774\nx       0.7071 -0.7071  0.0000\nx       0.4082  0.4082 -0.8165\n```\n:::\n:::\n\n\n\nWe drop the first row, transpose it, and use matrix multiplication with the votes matrix to get the ternary diagram.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Project 4D into 3D\nlibrary(geozoo)\nproj <- t(geozoo::f_helmert(3)[-1,])\np_rf_v_p <- as.matrix(penguins_rf$votes) %*% proj\ncolnames(p_rf_v_p) <- c(\"x1\", \"x2\")\np_rf_v_p <- p_rf_v_p %>%\n  as.data.frame() %>%\n  mutate(species = penguins_sub$species)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {#fig-penguins-votes-html layout-ncol=2}\n\n![3D](gifs/penguins_rf_votes.gif){#fig-p-votes-tour fig-alt=\"FIX ME\" width=300}\n\n\n\n::: {.cell}\n\n:::\n\n\n\nExamining the votes matrix from a random forest fit to the penguins: (a) from a tour of the 3D, (b) projected into 2D, to make a ternary diagram. In 3D the points can be seen to lie along a 2D plane, which is due to the constraint that the values sum to 1. From the ternary diagram, the classification can be seen to be reasonably well distinguished because points mostly lie at the vertex. There are a few penguins that are confused with a different species, as seen from the few points spread between vertices.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n::: {#fig-penguins-votes-pdf layout-ncol=2}\n\n![3D](images/penguins_rf_votes.png){#fig-p-votes-tour fig-alt=\"FIX ME\"}\n\n![2D ternary diagram](images/fig-p-votes-ggplot-1.png){#fig-p-votes-ggplot-pdf fig-alt=\"FIX ME\"}\n\nExamining the votes matrix from a random forest fit to the penguins: (a) in a frame from a tour of the 3D, (b) projected into 2D, to make a ternary diagram. In 3D the points can be seen to lie along a 2D plane, which is due to the constraint that the values sum to 1. From the ternary diagram, the classification can be seen to be reasonably well distinguished because points mostly lie at the vertex. There are a few penguins that are confused with a different species, as seen from the few points spread between vertices.\n:::\n:::\n\n\nWe can use the `geozoo` package to generate the surrounding simplex, which for 2D is a triangle.\n\nThe votes matrix reports the proportion of trees each observation is classified as each class. From the tour of the votes matrix, as in @fig-penguins-votes-pdf(a), it can be seen to be 2D in 3D space. This is due to the constraint that the three proportions for each observation sum to 1. Using a Helmert matrix, this data can be projected into the 2D space, or more generally the $(g-1)$-dimensional space where it resides, shown in @fig-penguins-votes-pdf(b). In 2D this is called a ternary diagram, and in higher dimensions the bounding shapes might be considered to be a simplex. The vertices of this shape correspond to $(1,0,0), (0,1,0), (0,0,1)$ (and analogously for higher dimensions), which represent perfect confidence, that an observation is classified into that group all the time.\n\nWhat we can see here is a concentration of points in the corners of the triangle indicates that most of the penguins are confidently classified into their correct class. Then there is more separation between the Gentoo and the others, than between Chinstrap and Adelie. That means that as a group Gentoo are more distinguishable. Only one of the Gentoo penguins has substantial confusion, mostly confused as a Chinstrap, but occasionally confused as an Adelie -- if it was only ever confused as a Chinstrap it would fall on the edge between Gentoo and Chinstrap. There are quite a few Chinstrap and Adelie penguins confused as each other, with a couple of each more confidently predicted to be the other class. This can be seen because there are points of the wrong colour close to those vertices. \n\nThe votes matrix is useful for investigating the fit, but one should remember that there are some structural elements of the penguins data that don't lend themselves to tree models. Although a forest has the capacity to generate non-linear boundaries by combining predictions from multiple trees, it is still based on the boxy boundaries of trees. This makes it less suitable for the penguins data with elliptical classes. You could use the techniques from the previous section to explore the boundaries produced by the forest, and you will find that the are more boxy than the LDA models.\n\\index{classification!vote matrix}\n\n\n::: {.content-visible when-format=\"pdf\"}\n\\infobox{By visualising the votes matrix we can understand which observations are harder to classify, which of the classes are more easily confused with each other.}\n:::\n\n::: {.content-visible when-format=\"html\"}\n::: info\nBy visualising the votes matrix we can understand which observations are harder to classify, which of the classes are more easily confused with each other.\n:::\n:::\n\nTo examine a vote matrix for a problem with more classes, we will examine the 10 class fake_trees data example. The full data has 100 variables, and we have seen from @sec-clust-graphics that reducing to 10 principal components allows the linear branching structure in the data to be seen. Given that the branches correspond to the classes, it will be interesting to see how well the random forest model performs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(mulgar)\nlibrary(dplyr)\nlibrary(liminal)\nft_pca <- prcomp(fake_trees[,1:100], \n                 scale=TRUE, retx=TRUE)\nft_pc <- as.data.frame(ft_pca$x[,1:10])\nft_pc$branches <- fake_trees$branches\nlibrary(randomForest)\nft_rf <- randomForest(branches~., data=ft_pc, \n                            importance=TRUE)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(ft_rf$votes, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    0 1    2     3     4    5 6 7     8    9\n1 0.9 0 0.03 0.000 0.011 0.09 0 0 0.000 0.00\n2 0.7 0 0.02 0.000 0.012 0.29 0 0 0.000 0.00\n3 0.8 0 0.04 0.000 0.107 0.02 0 0 0.000 0.04\n4 0.9 0 0.03 0.000 0.005 0.08 0 0 0.000 0.00\n5 0.7 0 0.05 0.005 0.027 0.25 0 0 0.005 0.00\n```\n:::\n:::\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nft_rf_votes <- ft_rf$votes %>%\n  as_tibble() %>%\n  mutate(branches = fake_trees$branches)\n\nproj <- t(geozoo::f_helmert(10)[-1,])\nf_rf_v_p <- as.matrix(ft_rf_votes[,1:10]) %*% proj\ncolnames(f_rf_v_p) <- c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\")\nf_rf_v_p <- f_rf_v_p %>%\n  as.data.frame() %>%\n  mutate(branches = fake_trees$branches)\n\nsimp <- geozoo::simplex(p=9)\nsp <- data.frame(simp$points)\ncolnames(sp) <- c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\")\nsp$branches = \"\"\nf_rf_v_p_s <- bind_rows(sp, f_rf_v_p) %>%\n  mutate(branches = factor(branches))\nlabels <- c(\"0\" , \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n                rep(\"\", 3000))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to make animated gifs\"}\nanimate_xy(f_rf_v_p_s[,1:9], col = f_rf_v_p_s$branches, \n           axes = \"off\", half_range = 0.8,\n           edges = as.matrix(simp$edges),\n           obs_labels = labels, palette = \"Viridis\")\n\nrender_gif(f_rf_v_p_s[,1:9],\n           grand_tour(),\n           display_xy(col = f_rf_v_p_s$branches, \n           axes = \"off\", half_range = 0.8,\n           edges = as.matrix(simp$edges),\n           obs_labels = labels, palette=\"Viridis\"),\n           gif_file=\"gifs/ft_votes.gif\",\n           frames=500) \n```\n:::\n\n\n:::\n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-ft-votes layout-ncol=2}\n\n![The 9D votes matrix for the 10 class fake_trees data in a tour.](gifs/ft_votes.gif){#fig-ft-votes-tour fig-alt=\"FIX ME\" width=300}\n\n![Several static views from the tour revealing how clusters connect.](images/ft-votes.png){#fig-ft-votes-prj fig-alt=\"FIX ME\" width=300}\n\nA tour and several static views of the votes matrix. Lines are the edges of the 8D simplex, which bounds the shape. Points mostly concentrate in the vertices, or spread along one of the edges, which means that most observations are clearly belonging to one group, or confused with a single other group. The exception to this is class 0, which spreads in many directions.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n::: {#fig-ft-votes-pdf layout-ncol=2}\n\n![](images/ft-votes.png){fig-alt=\"FIX ME\" width=400}\n\nSeveral static views from the tour of the votes matrix. Lines are the edges of the 8D simplex, which bounds the shape. Points mostly concentrate in the vertices, or spread along one of the edges, which means that most observations are clearly belonging to one group, or confused with a single other group. The exception to this is class 0, which spreads in many directions.\n:::\n:::\n\n\\index{data!fake trees}\nThe votes matrix is 9D, due to the 9 groups. With this many dimensions, if the cluster structure is weak, it will look messy in a tour. However, what we can see in @fig-ft-votes-pdf is that the structure is relatively simple, and very interesting in that it suggests a strong clustering of classes. Points are coloured by their true class. The lines represent the 8D simplex that bounds the observations, akin to the triangle in the ternary diagram.\n\nPoints concentrate at the vertices, which means that most are confidently predicted to be their true class. The most spread of points is along single edges, between pairs of vertices. This means that when there is confusion it is mostly with just one other group. One vertex (0) which has connections to all other vertexes. That is, there are points stretching from this vertex to every other. It means that some observations in every other class can be confused with class 0, and class 0 observations can be confused with every other class. This information suggests that cluster 0 is central to all the other clusters. \n\n\nSome of this information could also be inferred from the confusion matrix for the model. However visualising the votes matrix provides more intricate details. Here we have seen that the points spread out from a vertex, with fewer and fewer the further one gets. It allows us to see the distribution of points, which is not possible from the confusion matrix alone. The same misclaassification rate could be due to a variety of distributions. The visual pattern in the votes matrix is striking, and gives additional information about how the clustering distribution, and shapes of clusters, matches the class labels. It reinforces the clusters are linear extending into different dimensions in the 100D space, but really only into about 8D (as we'll see from the variable importance explanation below). We also see that nine of the clusters are all connected to a single cluster.\n\n::: {.content-visible when-format=\"html\"}\n::: insight\nThe votes matrix for the fake trees has a striking geometric structure, with one central cluster connected to all other clusters, each of which is distinct from each other.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\insightbox{The votes matrix for the fake trees has a striking geometric structure, with one central cluster connected to all other clusters, each of which is distinct from each other.}\n:::\n\n### Using variable importance {#sec-forest-var-imp}\n\\index{classification!variable importance}\n\nThe variable importance score across all classes, and for each class is useful for choosing variables to enter into a tour, to explore class differences. This is particularly so when there are many variables, as in the fake_trees data. We would also expect that this data will have a difference between importance for some classes.\n\n\n\n\n::: {#tbl-ft-importance .cell tbl-cap='Variable importance from the random forest fit to the fake_trees data, for each of the 9 classes, and using the accuracy and Gini metrics.'}\n::: {.cell-output-display}\n\\begin{longtable}{lrrrrrrrrrrrr}\n\\toprule\nVar & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & Acc & Gini \\\\ \n\\midrule\nPC1 & $0.1$ & $0.4$ & $0.5$ & $0.3$ & $0.2$ & $0.5$ & $0.4$ & $0.2$ & $0.3$ & $0.3$ & $0.31$ & $483$ \\\\ \nPC2 & $0.1$ & $0.2$ & $0.2$ & $0.5$ & $0.3$ & $0.3$ & $0.2$ & $0.4$ & $0.2$ & $0.3$ & $0.28$ & $376$ \\\\ \nPC3 & $0.1$ & $0.1$ & $0.1$ & $0.1$ & $0.5$ & $0.1$ & $0.1$ & $0.1$ & $0.2$ & $0.2$ & $0.16$ & $313$ \\\\ \nPC4 & $0.1$ & $0.5$ & $0.1$ & $0.0$ & $0.1$ & $0.0$ & $0.4$ & $0.2$ & $0.1$ & $0.1$ & $0.14$ & $341$ \\\\ \nPC5 & $0.1$ & $0.1$ & $0.4$ & $0.1$ & $0.2$ & $0.2$ & $0.1$ & $0.1$ & $0.3$ & $0.2$ & $0.18$ & $338$ \\\\ \nPC6 & $0.1$ & $0.2$ & $0.2$ & $0.2$ & $0.0$ & $0.1$ & $0.0$ & $0.3$ & $0.1$ & $0.2$ & $0.15$ & $284$ \\\\ \nPC7 & $0.1$ & $0.0$ & $0.1$ & $0.0$ & $0.0$ & $0.1$ & $0.1$ & $0.3$ & $0.1$ & $0.1$ & $0.11$ & $245$ \\\\ \nPC8 & $0.0$ & $0.1$ & $0.0$ & $0.2$ & $0.1$ & $0.1$ & $0.0$ & $0.0$ & $0.1$ & $0.3$ & $0.09$ & $214$ \\\\ \nPC9 & $0.1$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.1$ & $0.0$ & $0.02$ & $61$ \\\\ \nPC10 & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.0$ & $0.01$ & $45$ \\\\ \n\\bottomrule\n\\end{longtable}\n:::\n:::\n\n\n\nFrom the variable importance (@tbl-ft-importance), we can see that PC9 and PC10 do not substantially contribute. That means the 100D data can be reduced to 8D without losing the information about the cluster structure. PC1 is most important overall, and the order matches the PC order, as might be expected because highest variance corresponds to the most spread clusters. Each cluster has a different set of variables that are important. For example, the variables important for distinguishing cluster 1 are PC1 and PC4, and for cluster 2 they are PC1 and PC5. \n\n\n::: {.content-visible when-format=\"html\"}\n::: info\nClass-wise variable importance helps to find a subspace on which to tour to examine how this class cluster differs from the others.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\infobox{Class-wise variable importance helps to find a subspace on which to tour to examine how this class cluster differs from the others.}\n:::\n\nWe can use the accuracy information to choose variables to provide to the tour. Overall, one would sequentially add the variables into a tour based on their accuracy or Gini value. Here it is simply starting with the first three PCs, and then sequentially adding the PCs to examine how distinct the clusters are with ot without the extra variable. It can be helpful to focus on a single class against all the others. To do this create a new binary class variable, indicating that the observation belongs to class $k$ or not, as follows: \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nft_pc <- ft_pc %>%\n  mutate(cl1 = factor(case_when(\n                 branches == \"0\" ~ \"0\",\n                 branches == \"1\" ~ \"1\",\n                 .default = \"other\"\n  )))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nFrom @fig-ft-cl-pdf we can see how cluster 1 is distinct from all of the other observations, albeit with a close connection to the trunk of the tree (cluster 0). The distinction is visible whenever PC4 contributes to the projection, but can be seen clearly with only PC1 and PC4.\n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-ft-cl-html layout-ncol=2}\n\n![Tour of most important variables for class 1.](gifs/ft_cl1.gif){#fig-ft-cl1 fig-alt=\"FIX ME\" width=300}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![PC1 and PC4 together reveal cluster 1.](15-forests_files/figure-pdf/fig-ft-cl1-pc-1.pdf){#fig-ft-cl1-pc width=100%}\n:::\n:::\n\n\n\nFocusing on class 1 in the fake_trees data. The most important variables were PC1 and PC4. A combination of PC2 and PC4 reveals the difference between cluster 1 and all the other clusters.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n::: {#fig-ft-cl-pdf layout-ncol=2}\n\n![](images/ft_cl1.png){fig-alt=\"FIX ME\"}\n\n\n![](images/fig-ft-cl1-pc-1.png){fig-alt=\"FIX ME\"} \n\nFocusing on class 1 in the fake_trees data. The most important variables were PC1 and PC4. A combination of PC2 and PC4 reveals the difference between cluster 1 and all the other clusters.\n:::\n:::\n\nFor a problem like this, it can be useful to several classes together. We've chosen to start with class 8 (light green), because from @fig-ft-votes-pdf it appears to have less connection with class 0, and closer connection with another class. This is class 6 (medium green). A good guess because it has one observation confused with class 8 according to the confusion matrix (printed below). \n\\index{classification!confusion matrix}\n\nWhen we examine these two clusters in association with class 0, we can see that there is a third cluster that is connected with clusters 6 and 8. It turns out to be cluster 1. It's confusing, because the confusion matrix would suggest that the overlap from all is with cluster 0, but not each other. \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nft_rf$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    0   1   2   3   4   5   6   7   8   9 class.error\n0 269   6   2   2   3   3   3   3   6   3       0.103\n1  12 288   0   0   0   0   0   0   0   0       0.040\n2   9   0 289   0   2   0   0   0   0   0       0.037\n3   7   0   0 290   0   0   0   3   0   0       0.033\n4  14   0   0   0 286   0   0   0   0   0       0.047\n5  11   0   0   0   0 289   0   0   0   0       0.037\n6  12   0   0   0   0   0 287   0   1   0       0.043\n7   6   0   0   5   0   0   0 288   0   1       0.040\n8   6   0   0   0   0   0   0   0 294   0       0.020\n9   5   0   0   0   0   0   0   1   0 294       0.020\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\nFrom the tour in @fig-ft-cl2 we can see that clusters 1, 6, and 8 share one end of the trunk (cluster 0). Cluster 8 is almost more closely connected with cluster 6, though, than cluster 0. PC1 and PC5 mostly show the distinction between cluster 8 and the rest of the points, but it is clearer if more variables are used.\n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-ft-cl2 layout-ncol=2}\n\n![Tour of most important variables for class 1.](gifs/ft_cl8.gif){#fig-ft-cl8 fig-alt=\"FIX ME\" width=300}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![PC1 and PC5 together mostly reveal cluster 8.](15-forests_files/figure-pdf/fig-ft-cl8-pc-1.pdf){#fig-ft-cl8-pc width=100%}\n:::\n:::\n\n\n\nFocusing on class 8 in the fake_trees data, relative to nearby clusters 1 and 6. The most important variables for cluster 8 are  PC1, PC2, PC5, but to explore in association with clusters 1 and 6, we include PC4 and PC6. A combination of PC1 and PC5 reveals the difference between cluster 8, 6, 1 and 0.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n::: {#fig-ft-cl2 layout-ncol=2}\n\n![](images/ft_cl8.png){fig-alt=\"FIX ME\"}\n\n![](images/fig-ft-cl8-pc-1.png){fig-alt=\"FIX ME\"}\n\nFocusing on class 8 in the fake_trees data using a tour (left) reveals that it shares an end of cluster 0 with clusters 1 and 6. A combination of PC1 and PC5 reveals that there is a difference between the observations in class 8 relative to 6, 1 and 0 is largely due to PC5 (right).\n:::\n:::\n\n::: {.content-visible when-format=\"html\"}\n::: insight\nAlthough the confusion matrix suggests that class clusters are separated except for class 0, focusing on a few classes and using the variable importance to examine smaller subspaces, reveals they are connected in groups of three to class 0.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\insightbox{Although the confusion matrix suggests that class clusters are separated except for class 0, focusing on a few classes and using the variable importance to examine smaller subspaces, reveals they are connected in groups of three to class 0.}\n:::\n\n## Exercises {-}\n\n1. Using a grand tour compare the boundaries from the random forest model on the `penguins` data to that of (a) a default tree model, (b) an LDA model. Is it less boxy than the tree model, but still more boxy than that of the LDA model?\n2. Tinker with the parameters of the tree model to force it to fit a tree more closely to the data. Compare the boundaries from this with the default tree, and with the forest model. Is it less boxy than the default tree, but more boxy than the forest model?\n3. Fit a random forest model to the `bushfires` data using the `cause` variable as the class. It is a highly imbalanced classification problem. What is the out-of-bag error rate for the forest? Are there some classes that have lower error rate than others? Examine the 4D votes matrix with a tour, and describe the confusion between classes. This is interesting because it is difficult to accurately classify the fire ignition cause, and only some groups are often confused with each other. You should be able to see this from the 3D votes matrix. \n4. Fit a forest model to the first 21 PCs of the `sketches` data. Explore the 5D votes matrix. Why does it look star-shaped?\n5. Choose a cluster (or group of clusters) from the fake_trees data (2, 3, 4, 5, 7, 9) to explore in detail like done in @sec-forest-var-imp. Be sure to choose which PCs are the most useful using a tour, and follow-up by making a scatterplot showing the best distinction between your chosen cluster and the other observations. \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "15-forests_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"tbl-ft-importance\",\"tbl-ft-importance\",\"tbl-ft-importance\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"caption\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}