{
  "hash": "97953c94ba5f9ffcceeba00544cea9c9",
  "result": {
    "markdown": "# Trees and forests {#sec-trees-forests}\n\n## Trees {#sec-trees}\n\\index{classification methods!trees}\n\nThe tree algorithm [@BFOS84] is a simple and versatile algorithmic method for supervised classification. The basic tree algorithm generates a classification rule by sequentially splitting the data into two buckets. Splits are made between sorted data values of individual variables, with the goal of obtaining pure classes on each side of the split. The inputs for a simple tree classifier commonly include (1) an impurity measure, an indication of the relative diversity among the cases in the terminal nodes; (2) a parameter that sets the minimum number of cases in a node, or the minimum number of observations in a terminal node of the tree; and (3) a complexity measure that controls the growth of a tree, balancing the use of a simple generalizable tree against a more accurate tree\ntailored to the sample.  When applying tree methods, exploring the effects of the input parameters on the tree is instructive; for example, it helps us to assess the stability of the tree model.\n\nAlthough algorithmic models do not depend on distributional assumptions, that does not mean that every algorithm is suitable for all data.  For example, the tree model works best when all variables are independent within each class, because it does not take such dependencies into account.  Visualization can help us to determine whether a particular model should be applied.  In classification problems, it is useful to explore the cluster structure, comparing the clusters with the classes and looking for evidence of correlation within each class. \nThe plots in @fig-lda-assumptions1 and @fig-penguins-lda-ellipses-pdf shows a strong correlation between the variables within each species, which indicates that the tree model may not give good results for the penguins data. We'll show how this is the case with two variables initially, and then extend to the four variables.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The association between variables in the penguins data causes problems for fitting a tree model. Although the model, computed using only bl and bd, is simple (left), the fit is poor (right) because it doesn't adequately utilise combinations of variables.](15-forests_files/figure-pdf/fig-p-bl-bd-tree-1.pdf){#fig-p-bl-bd-tree fig-alt='Tree diagram with top split bl<0.3004, leading to Adelie branch, second split at bd >= -0.4138, leading to Gentoo branch, and final split at bl< 0.1476, leading to Adelie and Chinstrap branches. The scatterplot at right shows bd vs bl, with three predictive region partitions, and the data is overplotted. The elliptical spreads of data points crosses the rectangular partitions in places.' width=100%}\n:::\n:::\n\n\n\nThe plots in @fig-p-bl-bd-tree show the inadequacies of the tree fit. The background color indicates the class predictions, and thus boundaries produced by the tree fit. They can be seen to be boxy, and missing the elliptical nature of the penguin clusters. This produces errors in the classification of observations which are indefensible. One could always force the tree to fit the data more closely by adjusting the parameters, but the main problem persists: that one is trying to fit elliptical shapes using boxes.\n\n::: {.content-visible when-format=\"html\"}\n::: info\nThere are less strict assumptions for a non-parametric model but it is still important to understand the model fit relative to the data. \n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\infobox{There are less strict assumptions for a non-parametric model but it is still important to understand the model fit relative to the data. \n}\n:::\n\nThe boundaries for the tree model on all four variables of the penguins data can be viewed similarly, by predicting a set of points randomly generated in the 4D domain of observed values. @fig-penguins-lda-tree-pdf shows the prediction regions for LDA and a default tree in a slice tour. The slice tour is used to help see into the middle of the 4D cube. It slices the cube through the centre of the data, where the boundaries of the regions should meet. \n\nThe prediction regions of the default fitted tree are shown in comparison to those from the LDA model. We don't show the tree diagram here, but it makes only six splits of the tree model, which is delightfully simple. However, just like the model fitted to two variables, the result is not adequate for the penguins data. The tree model generates boxy boundaries, whereas the LDA model splits the 4D cube obliquely. The boxy regions don't capture the differences between the elliptically-shaped clusters. Overlaying the observed data on this display would make this clearer, but the boundaries are easier to examine without them.\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\\index{tour!slice} \n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-penguins-lda-tree-html layout-ncol=2}\n\n![LDA model](gifs/penguins_lda_boundaries.gif){#fig-lda-boundary fig-alt=\"FIX ME\" width=300}\n\n![Tree model](gifs/penguins_tree_boundaries.gif){#fig-tree-boundary fig-alt=\"FIX ME\" width=300}\n\nComparison of the boundaries produced by the LDA (a) and the tree (b) model, using a slice tour. The tree boundaries are more box-shaped than the LDA boundaries, which does not adequately capture the differences between the elliptically-shaped clusters of the penguins data.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n::: {#fig-penguins-lda-tree-pdf layout-ncol=2}\n\n![LDA model](images/penguins_lda_boundaries.png){#fig-lda-boundary fig-alt=\"FIX ME\"}\n\n![Tree model](images/penguins_tree_boundaries.png){#fig-tree-boundary fig-alt=\"FIX ME\"}\n\nComparison of the boundaries produced by the LDA (a) and the tree (b) model, using a slice tour. (Here only a single frame is shown.) The tree boundaries are more box-shaped than the LDA boundaries, which does not adequately capture the differences between the elliptically-shaped clusters of the penguins data.\n:::\n:::\n\n\n::: {.content-visible when-format=\"html\"}\n::: insight\nWith the penguins data, a tree model may not be a good choice due to the strong correlation between variables. The best separation is in combinations of variables, not the single variable tree splits. \n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\insightbox{With the penguins data, a tree model may not be a good choice due to the strong correlation between variables. The best separation is in combinations of variables, not the single variable tree splits.}\n:::\n\n\n\n## Random forests \n\\index{classification methods!random forest}\n\nA random forest [@Br01] is a classifier that is built from multiple trees generated by randomly sampling the cases and the variables.  The random sampling (with replacement) of cases has the fortunate effect of creating a training (\"in-bag\") and a test (\"out-of-bag\") sample for each tree computed.  The class of each case in the out-of-bag sample for each tree is predicted, and the predictions for all trees are combined into a vote for the class identity.  \n\nA random forest is a computationally intensive method, a \"black box\" classifier, but it produces several diagnostics that make the outcome less mysterious.  Some diagnostics that help us to assess the model are the votes, the measure of variable importance, and the proximity matrix.\n\n### Examining the votes matrix\n\nHere we show how to use the `randomForest` [@randomForest2002] votes matrix for the penguins data to investigate confusion between classes, and observations which are problematic to classify. With only three classes the votes matrix is only a 2D object, and thus easy to examine. With four or more classes the votes matrix needs to be examined in a tour. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(randomForest)\nlibrary(dplyr)\npenguins_rf <- randomForest(species~.,\n                             data=penguins_sub[,1:5],\n                             importance=TRUE)\npenguins_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(formula = species ~ ., data = penguins_sub[, 1:5],      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 2.4%\nConfusion matrix:\n          Adelie Chinstrap Gentoo class.error\nAdelie       143         3      0 0.020547945\nChinstrap      4        64      0 0.058823529\nGentoo         0         1    118 0.008403361\n```\n:::\n:::\n\n\n\n\nTo examine the votes matrix, we extract the `votes` element from the random forest model object. This will have three columns corresponding to the three species, but because each row is a set of proportions it is only a 2D object, as seen in @fig-p-votes-tour. To reduce the dimension from 3D to the 2D we use a Helmert matrix [@helmert]. A Helmert matrix has a first row of all 1's. The remaining components of the matrix are 1's in the lower triangle, and 0's in the upper triangle and the diagonal elements are the negative row sum. The rows are usually normalised to have length 1. They are used to create contrasts to test combinations of factor levels for post-testing after Analysis of Variance (ANOVA). For compositional data, like the votes matrix, when the first row is removed a Helmert matrix can be used to reduce the dimension appropriately. For three classes, this will generate the common 2D ternary diagram, but for higher dimensions it will reduce to a $(g-1)$-dimensional simplex. For the penguins data, the Helmert matrix for 3D is \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to compute Helmert matrix\"}\ngeozoo::f_helmert(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             [,1]       [,2]       [,3]\nhelmert 0.5773503  0.5773503  0.5773503\nx       0.7071068 -0.7071068  0.0000000\nx       0.4082483  0.4082483 -0.8164966\n```\n:::\n:::\n\n\n\nWe drop the first row, transpose it, and use matrix multiplication with the votes matrix to get the ternary diagram.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Project 4D into 3D\nlibrary(geozoo)\nproj <- t(geozoo::f_helmert(3)[-1,])\np_rf_v_p <- as.matrix(penguins_rf$votes) %*% proj\ncolnames(p_rf_v_p) <- c(\"x1\", \"x2\")\np_rf_v_p <- p_rf_v_p %>%\n  as.data.frame() %>%\n  mutate(species = penguins_sub$species)\n```\n:::\n\n\n\nWe can use the `geozoo` package to generate the surrounding simplex, which is a triangle for 2D.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Add simplex\nsimp <- simplex(p=2)\nsp <- data.frame(cbind(simp$points), simp$points[c(2,3,1),])\ncolnames(sp) <- c(\"x1\", \"x2\", \"x3\", \"x4\")\nsp$species = sort(unique(penguins_sub$species))\np_ternary <- ggplot() +\n  geom_segment(data=sp, aes(x=x1, y=x2, xend=x3, yend=x4)) +\n  geom_text(data=sp, aes(x=x1, y=x2, label=species),\n            nudge_x=c(-0.06, 0.07, 0),\n            nudge_y=c(0.05, 0.05, -0.05)) +\n  geom_point(data=p_rf_v_p, aes(x=x1, y=x2, colour=species), size=2, alpha=0.5) +\n  scale_color_discrete_divergingx(palette=\"Zissou 1\") +\n  theme_map() +\n  theme(aspect.ratio=1, legend.position=\"none\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to generate animated gifs\"}\n# Look at the votes matrix, in its 3D space\nanimate_xy(penguins_rf$votes, col=penguins_sub$species)\n\n# Save an animated gif\nrender_gif(penguins_rf$votes,\n           grand_tour(),\n           display_xy(v_rel=0.02, \n             col=penguins_sub$species, \n             axes=\"bottomleft\"), \n           gif_file=\"gifs/penguins_rf_votes.gif\",\n           frames=500,\n           loop=FALSE\n)\n```\n:::\n\n\n\n::: {.content-visible when-format=\"html\"}\n::: {#fig-penguins-votes layout-ncol=2}\n\n![Votes matrix in a tour.](gifs/penguins_rf_votes.gif){#fig-p-votes-tour fig-alt=\"FIX ME\" width=300}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Votes matrix in its 2D space, a ternary diagram.](15-forests_files/figure-pdf/fig-p-votes-ggplot-1.pdf){#fig-p-votes-ggplot width=80%}\n:::\n:::\n\n\n\nExamining the votes matrix from a random forest fit to the penguins.\n:::\n:::\n\nThe votes matrix reports the proportion of trees each observation is classified as each class. From the tour of the votes matrix, it can be seen to be 2D in 3D space. This is due to the constraint that the three proportions for each observation sum to 1. Using a Helmert matrix, this data can be projected into the 2D space, or more generally the $(g-1)$-dimensional space where it resides. In 2D this is called a ternary diagram, and in higher dimensions the bounding shapes might be considered to be a simplex. The vertices of this shape correspond to $(1,0,0), (0,1,0), (0,0,1)$ (and analogously for higher dimensions), which represent perfect confidence, that an observation is classified into that group all the time.\n\nWhat we can see here is a concentration of points in the corners of the triangle indicates that most of the penguins are confidently classified into their correct class. Then there is more separation between the Gentoo and the others, than between Chinstrap and Adelie. That means that as a group Gentoo are more distinguishable. Only one of the Gentoo penguins has substantial confusion, mostly confused as a Chinstrap, but occasionally confused as an Adelie -- if it was only ever confused as a Chinstrap it would fall on the edge between Gentoo and Chinstrap. There are quite a few Chinstrap and Adelie penguins confused as each other, with a couple of each more confidently predicted to be the other class. This can be seen because there are points of the wrong colour close to those vertices. \n\nThe votes matrix is useful for investigating the fit, but one should remember that there are some structural elements of this data that don't lend themselves to tree models. Although a forest has the capacity to generate non-linear boundaries by combining predictions from multiple trees, it is still based on the boxy boundaries of trees. This makes it less suitable for the penguins data with elliptical classes. You could use the techniques from the previous section to explore the boundaries produced by the forest, and you will find that the are more boxy than the LDA models.\n\nTo examine a vote matrix for a problem with more classes, we will examine the 10 class fake_trees data example. The full data has 100 variables, and we have seen from @sec-clust-graphics that reducing to 10 principal components allows the linear branching structure in the data to be seen. Given that the branches correspond to the classes, it will be interesting to see how well the random forest model performs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(mulgar)\nlibrary(dplyr)\nlibrary(liminal)\nft_pca <- prcomp(fake_trees[,1:100], \n                 scale=TRUE, retx=TRUE)\nft_pc <- as.data.frame(ft_pca$x[,1:10])\nft_pc$branches <- fake_trees$branches\nlibrary(randomForest)\nft_rf <- randomForest(branches~., data=ft_pc, \n                            importance=TRUE)\nft_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(formula = branches ~ ., data = ft_pc, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n\n        OOB estimate of  error rate: 4.2%\nConfusion matrix:\n    0   1   2   3   4   5   6   7   8   9 class.error\n0 266   5   2   3   3   5   5   1   6   4  0.11333333\n1  13 287   0   0   0   0   0   0   0   0  0.04333333\n2   9   0 289   0   2   0   0   0   0   0  0.03666667\n3   4   0   0 290   0   0   0   3   0   3  0.03333333\n4  11   0   0   0 289   0   0   0   0   0  0.03666667\n5  12   0   0   0   0 288   0   0   0   0  0.04000000\n6  10   0   0   0   0   0 289   0   1   0  0.03666667\n7   6   0   0   4   0   0   0 290   0   0  0.03333333\n8   7   0   0   0   0   0   0   0 293   0  0.02333333\n9   6   0   0   0   0   0   0   1   0 293  0.02333333\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nft_rf_votes <- ft_rf$votes %>%\n  as_tibble() %>%\n  mutate(branches = fake_trees$branches)\n\nproj <- t(geozoo::f_helmert(10)[-1,])\nf_rf_v_p <- as.matrix(ft_rf_votes[,1:10]) %*% proj\ncolnames(f_rf_v_p) <- c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\")\nf_rf_v_p <- f_rf_v_p %>%\n  as.data.frame() %>%\n  mutate(branches = fake_trees$branches)\n\nsimp <- geozoo::simplex(p=9)\nsp <- data.frame(simp$points)\ncolnames(sp) <- c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\")\nsp$branches = \"\"\nf_rf_v_p_s <- bind_rows(sp, f_rf_v_p) %>%\n  mutate(branches = factor(branches))\nlabels <- c(\"0\" , \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n                rep(\"\", 3000))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to make animated gifs\"}\nanimate_xy(f_rf_v_p_s[,1:9], col = f_rf_v_p_s$branches, \n           axes = \"off\", half_range = 0.8,\n           edges = as.matrix(simp$edges),\n           obs_labels = labels, palette = \"Viridis\")\n\nrender_gif(f_rf_v_p_s[,1:9],\n           grand_tour(),\n           display_xy(col = f_rf_v_p_s$branches, \n           axes = \"off\", half_range = 0.8,\n           edges = as.matrix(simp$edges),\n           obs_labels = labels, palette=\"Viridis\"),\n           gif_file=\"gifs/ft_votes.gif\",\n           frames=500) \n```\n:::\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-ft-votes layout-ncol=2}\n\n![The 9D votes matrix for the 10 class fake_trees data in a tour.](gifs/ft_votes.gif){#fig-ft-votes-tour fig-alt=\"FIX ME\" width=300}\n\n![Several static views from the tour revealing how clusters connect.](images/ft-votes.png){#fig-ft-votes-prj fig-alt=\"FIX ME\" width=300}\n\nThe votes matrix for the fake_trees data has a very striking geometric shape. The branching nature of the clusters is very clear. Most classes are distinct except for a connection with class 0.\n:::\n:::\n\nThe votes matrix is 9D, but the structure of it is easy to read, and very interesting. The observations are coloured by class. There is one vertex (0) which has connections to all other vertexes. That is, there are points stretching without big breaks from this vertex to every other. It means that some observations in every other class can be confused with class 0, and class 0 observations can be confused with every other class. All of the other vertexes have a string of points almost entirely along one edge, the edge leading to vertex 0. This shows the lack of confusion with any other class, except 0. Cluster 0 could be considered the trunk of the tree, from which the other clusters grow. \n\nThis pattern is what can be inferred from the confusion matrix, but we can't determine whether the clusters are mostly separated except for a few observations, or some other clustering shape. The visual pattern in the votes matrix is so striking, and gives additional information about the clustering distribution, and shapes of clusters. It reinforces the clusters are linear extending into different dimensions in the 100D space, but really only into about 8D (as we'll see from the variable importance explanation below). We also see that 9 of the clusters are all connected to one cluster.\n\n::: {.content-visible when-format=\"html\"}\n::: info\nBy visualizing the votes matrix we can understand which observations are harder to classify, which of the classes are more easily confused with each other and if there are groups with similar patterns in the data.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\infobox{By visualizing the votes matrix we can understand which observations are harder to classify, which of the classes are more easily confused with each other and if there are groups with similar patterns in the data.}\n:::\n\n### Using variable importance {#sec-forest-var-imp}\n\nThe variable importance score across all classes, and for each class is useful for choosing variables to enter into a tour, to explore class differences. This is particularly so when there are many variables, as in the fake_trees data. We would also expect that this data will have a difference between importance for some classes.\n\n\n\n::: {#tbl-ft-importance .cell tbl-cap='Variable importance from the random forest fit to the fake_trees data.'}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(gt)\nft_rf$importance %>% \n  as_tibble(rownames=\"Variable\") %>% \n  rename(Accuracy=MeanDecreaseAccuracy,\n         Gini=MeanDecreaseGini) %>%\n  #arrange(desc(Gini)) %>%\n  gt() %>%\n  fmt_number(columns = c(`0`,`1`,`2`,`3`,`4`,`5`,`6`,`7`,`8`,`9`, Accuracy),\n             decimals = 2) %>%\n  fmt_number(columns = Gini,\n             decimals = 0)\n```\n\n::: {.cell-output-display}\n\\begin{longtable}{lrrrrrrrrrrrr}\n\\toprule\nVariable & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & Accuracy & Gini \\\\ \n\\midrule\nPC1 & $0.10$ & $0.36$ & $0.45$ & $0.29$ & $0.21$ & $0.49$ & $0.40$ & $0.19$ & $0.31$ & $0.31$ & $0.31$ & $487$ \\\\ \nPC2 & $0.13$ & $0.24$ & $0.22$ & $0.50$ & $0.30$ & $0.30$ & $0.17$ & $0.39$ & $0.23$ & $0.29$ & $0.28$ & $377$ \\\\ \nPC3 & $0.09$ & $0.06$ & $0.09$ & $0.13$ & $0.54$ & $0.15$ & $0.09$ & $0.13$ & $0.18$ & $0.16$ & $0.16$ & $308$ \\\\ \nPC4 & $0.09$ & $0.46$ & $0.06$ & $0.04$ & $0.09$ & $0.03$ & $0.37$ & $0.15$ & $0.08$ & $0.09$ & $0.15$ & $336$ \\\\ \nPC5 & $0.13$ & $0.10$ & $0.35$ & $0.08$ & $0.16$ & $0.23$ & $0.09$ & $0.10$ & $0.29$ & $0.22$ & $0.17$ & $332$ \\\\ \nPC6 & $0.10$ & $0.23$ & $0.25$ & $0.18$ & $0.04$ & $0.13$ & $0.04$ & $0.29$ & $0.15$ & $0.17$ & $0.16$ & $290$ \\\\ \nPC7 & $0.07$ & $0.03$ & $0.15$ & $0.04$ & $0.05$ & $0.07$ & $0.12$ & $0.34$ & $0.12$ & $0.15$ & $0.11$ & $249$ \\\\ \nPC8 & $0.04$ & $0.06$ & $0.02$ & $0.25$ & $0.06$ & $0.08$ & $0.02$ & $0.04$ & $0.08$ & $0.26$ & $0.09$ & $214$ \\\\ \nPC9 & $0.07$ & $0.01$ & $0.01$ & $0.02$ & $0.01$ & $0.01$ & $0.01$ & $0.03$ & $0.05$ & $0.02$ & $0.02$ & $59$ \\\\ \nPC10 & $0.04$ & $0.01$ & $0.01$ & $0.01$ & $0.00$ & $0.01$ & $0.01$ & $0.00$ & $0.00$ & $0.02$ & $0.01$ & $45$ \\\\ \n\\bottomrule\n\\end{longtable}\n:::\n:::\n\n\n\nFrom the variable importance, we can see that PC9 and PC10 do not substantially contribute. That means the 100D data can be reduced to 8 PCs while maintaining the information about the clustering. PC1 is most important overall, but each cluster has a different set of variables that are important. For example, the variables important for distinguishing cluster 1 are PC1, PC2, PC4 and PC6, and for cluster 7 they are PC2, PC6, PC7. We can use this information to choose variables to provide to the tour. It can be helpful to reduce the class variable to focus on a particular class, by creating a new class variable, as follows. \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nft_pc <- ft_pc %>%\n  mutate(cl1 = factor(case_when(\n                 branches == \"0\" ~ \"0\",\n                 branches == \"1\" ~ \"1\",\n                 .default = \"other\"\n  )))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to make animated gifs\"}\nanimate_xy(ft_pc[,c(\"PC1\", \"PC2\", \"PC4\", \"PC6\")], col=ft_pc$cl1, palette=\"Viridis\")\nrender_gif(ft_pc[,c(\"PC1\", \"PC2\", \"PC4\", \"PC6\")],\n           grand_tour(),\n           display_xy(col=ft_pc$cl1, palette=\"Viridis\"),\n           gif_file=\"gifs/ft_cl1.gif\",\n           frames=500)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to make plot\"}\nft_pc_cl1 <- ggplot(ft_pc, aes(x=PC4, y=PC2, col=cl1)) +\n  geom_point(alpha=0.7, size=1) +\n  scale_color_discrete_sequential(palette=\"Viridis\", rev=FALSE) +\n  theme_minimal() +\n  theme(aspect.ratio = 1)\n```\n:::\n\n\n\nFrom @fig-ft-cl we can see how cluster 1 is distinct from all of the other observations, albeit with a close connection to the trunk of the tree (cluster 0). The distinction is visible in PC1, PC2, PC4, PC6, but can be seen clearly with just two of these.\n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-ft-cl layout-ncol=2}\n\n![Tour of most important variables for class 1.](gifs/ft_cl1.gif){#fig-ft-cl1 fig-alt=\"FIX ME\" width=300}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![PC2 and PC4 together reveal cluster 1.](15-forests_files/figure-pdf/fig-ft-cl1-pc-1.pdf){#fig-ft-cl1-pc width=100%}\n:::\n:::\n\n\n\nFocusing on class 1 in the fake_trees data. The most important variables were PC1, PC2, PC4, PC6. A combination of PC2 and PC4 reveals the difference between cluster 1 and all the other clusters.\n:::\n:::\n\nFor a problem with this many classes it can be useful to focus on several groups together. We've chosen cluster 8, because it appears to have less of a connection with cluster 0. See the light green cluster in bottom right static plot of @fig-ft-votes-prj is connected more to a different vertex, which is cluster 6 when carefully viewed. This is also suggested by the confusion matrix, where there is one observation from cluster 8 confused with cluster 6, although somewhat contradictory information, most are confused with cluster 0. We have also added cluster 1 to the investigation because it is closely connected to 6 and 8.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nft_pc <- ft_pc %>%\n  mutate(cl8 = factor(case_when(\n                 branches == \"0\" ~ \"0\",\n                 branches == \"6\" ~ \"6\",\n                 branches == \"1\" ~ \"1\",\n                 branches == \"8\" ~ \"8\",\n                 .default = \"other\"\n  )))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to make animated gif\"}\nanimate_xy(ft_pc[,c(\"PC1\", \"PC2\", \"PC4\", \"PC5\", \"PC6\")], col=ft_pc$cl8, palette=\"Viridis\")\nrender_gif(ft_pc[,c(\"PC1\", \"PC2\", \"PC4\", \"PC5\", \"PC6\")],\n           grand_tour(),\n           display_xy(col=ft_pc$cl8, palette=\"Viridis\"),\n           gif_file=\"gifs/ft_cl8.gif\",\n           frames=500)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code to make plot\"}\nft_pc_cl8 <- ggplot(ft_pc, aes(x=PC1, y=PC5, col=cl8)) +\n  geom_point(alpha=0.7, size=1) +\n  scale_color_discrete_sequential(palette=\"Viridis\", rev=FALSE) +\n  theme_minimal() +\n  theme(aspect.ratio = 1)\n```\n:::\n\n\n\nFrom @fig-ft-cl2 we can see that clusters 1, 6, and 8 share one end of the trunk (cluster 0). Cluster 8 is almost more closely connected with cluster 6, though, than cluster 0. PC1 and PC5 mostly show the distinction between cluster 8 and the rest of the points, but it is clearer if more variables are used.\n\n::: {.content-visible when-format=\"html\"}\n\n::: {#fig-ft-cl2 layout-ncol=2}\n\n![Tour of most important variables for class 1.](gifs/ft_cl8.gif){#fig-ft-cl8 fig-alt=\"FIX ME\" width=300}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![PC1 and PC5 together mostly reveal cluster 8.](15-forests_files/figure-pdf/fig-ft-cl8-pc-1.pdf){#fig-ft-cl8-pc width=100%}\n:::\n:::\n\n\n\nFocusing on class 8 in the fake_trees data, relative to nearby clusters 1 and 6. The most important variables for cluster 8 are  PC1, PC2, PC5, but to explore in association with clusters 1 and 6, we include PC4 and PC6. A combination of PC1 and PC5 reveals the difference between cluster 8, 6, 1 and 0.\n:::\n:::\n\n::: {.content-visible when-format=\"html\"}\n::: info\nVariable importance can be used in feature selection. Looking at class-wise variable importance we can select a subspace that can separate a selected class and with a tour we can understand how this separation in multiple variables looks like. This works best when focusing on a small subset of classes.\n:::\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\\infobox{Variable importance can be used in feature selection. Looking at class-wise variable importance we can select a subspace that can separate a selected class and with a tour we can understand how this separation in multiple variables looks like. This works best when focusing on a small subset of classes.}\n:::\n\n## Exercises {-}\n\n1. Using a grand tour compare the boundaries from the random forest model on the `penguins` data to that of (a) a default tree model, (b) an LDA model. Is it less boxy than the tree model, but still more boxy than that of the LDA model?\n2. Tinker with the parameters of the tree model to force it to fit a tree more closely to the data. Compare the boundaries from this with the default tree, and with the forest model. Is it less boxy than the default tree, but more boxy than the forest model?\n3. Fit a random forest model to the `bushfires` data using the `cause` variable as the class. It is a highly imbalanced classification problem. What is the out-of-bag error rate for the forest? Are there some classes that have lower error rate than others? Examine the 4D votes matrix with a tour, and describe the confusion between classes. This is interesting because it is difficult to accurately classify the fire ignition cause, and only some groups are often confused with each other. You should be able to see this from the 3D votes matrix. \n4. Fit a forest model to the first 21 PCs of the `sketches` data. Explore the 5D votes matrix. Why does it look star-shaped?\n5. Choose a cluster (or group of clusters) from the fake_trees data (2, 3, 4, 5, 7, 9) to explore in detail like done in @sec-forest-var-imp. Be sure to choose which PCs are the most useful using a tour, and follow-up by making a scatterplot showing the best distinction between your chosen cluster and the other observations. \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "15-forests_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"tbl-ft-importance\",\"tbl-ft-importance\",\"tbl-ft-importance\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"caption\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}