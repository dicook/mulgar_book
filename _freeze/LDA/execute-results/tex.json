{
  "hash": "f94791fc52d14f168ac5c3670eefdf41",
  "result": {
    "markdown": "# Linear discriminant analysis\n\nLinear discriminant analysis (LDA) dates to the early 1900s. It's one of the most elegant and simple techniques for both modeling separation between groups, and as an added bonus, producing a low-dimensional representation of the differences between groups. LDA has two strong assumptions: the groups are samples from multivariate normal distributions, and each have the same variance-covariance. If the latter assumption is relaxed, a slightly less elegant solution results from quadratic discriminant analysis.\n\nUseful explanations can be found in @VR02 and @Ri96. A good general treatment of parametric methods for supervised classification can be found in @JW02 or another similar multivariate analysis textbook. It's also useful to know that hypothesis testing for the difference in multivariate means using multivariate analysis of variance (MANOVA) has similar assumptions to LDA. Also model-based clustering assumes that each cluster arises from a multivariate normal distribution, and is related to LDA. The methods described here can be used to check these assumptions when applying these methods, too.\\index{classification methods!multivariate analysis of variance (MANOVA)}. \\index{cluster analysis!model-based} \n\n\n::: info\nBecause LDA is a parametric model it is important to check that the assumptions are reasonable:\n\n- shape of clusters are elliptical\n- cluster sizes are the same.\n\n:::\n\n<!-- Algorithmic methods have overtaken parametric methods in the practice of supervised classification.  A parametric method such as linear discriminant analysis (LDA) yields a set of interpretable output parameters, so it leaves a clear trail helping us to understand what was done to produce the results.  An algorithmic method, on the other hand, is more or less a black box, with various input parameters that are adjusted to tune the algorithm.  The algorithm's input and output parameters do not always correspond in any obvious way to the interpretation of the results.  Because if it's simplicity, if the assumptions (approximately) hold, LDA can provide an elegant classification solution.-->\n\n## Extracting the key elements of the model\n\nLDA builds the model on the between-group sum-of-square matrix\n\n$$B=\\sum_{k=1}^g n_k(\\bar{X}_k-\\bar{X})(\\bar{X}_k-\\bar{X})^\\top$$\nwhich measures the differences between the class means, \ncompared with the overall data mean $\\bar{X}$ and the within-group sum-of-squares matrix,\n\n$$W =\n\\sum_{k=1}^g\\sum_{i=1}^{n_k}\n(X_{ki}-\\bar{X}_k)(X_{ki}-\\bar{X}_k)^\\top$$\n\nwhich measures the variation of values around each class mean. The linear discriminant space is generated by computing the eigenvectors (canonical coordinates) of $W^{-1}B$, and this is the $(g-1)$-D space where the group means are most separated with respect to the\npooled variance-covariance.\n\n$$\n\\delta_k(x) = (x-\\mu_k)^\\top W^{-1}\\mu_k + \\log \\pi_k\n$$\n\nwhere $\\pi_k$ is a prior probability for class $k$ that might be based on unequal sample sizes, or cost of misclassification. The LDA classifier rule is to *assign a new observation to the class with the largest value*.\n\nWe can fit an LDA model using the `lda()` function from the `MASS` package. Here we have used the `penguins` data, assuming equal prior probability, to illustrate. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(dplyr)\nlibrary(mulgar)\nlibrary(MASS)\nload(\"data/penguins_sub.rda\")\n\np_lda <- lda(species~bl+bd+fl+bm, data=penguins_sub, prior=c(1/3, 1/3, 1/3))\noptions(digits=2)\np_lda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nlda(species ~ bl + bd + fl + bm, data = penguins_sub, prior = c(1/3, \n    1/3, 1/3))\n\nPrior probabilities of groups:\n   Adelie Chinstrap    Gentoo \n     0.33      0.33      0.33 \n\nGroup means:\n             bl    bd    fl    bm\nAdelie    -0.95  0.60 -0.78 -0.62\nChinstrap  0.89  0.64 -0.37 -0.59\nGentoo     0.65 -1.10  1.16  1.10\n\nCoefficients of linear discriminants:\n     LD1   LD2\nbl -0.24 -2.31\nbd  2.04  0.19\nfl -1.20  0.08\nbm -1.22  1.24\n\nProportion of trace:\n LD1  LD2 \n0.83 0.17 \n```\n:::\n:::\n\n\nBecause there are three classes the dimension of the discriminant space is 2D. We can easily extract the group means from the model. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\np_lda$means\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             bl    bd    fl    bm\nAdelie    -0.95  0.60 -0.78 -0.62\nChinstrap  0.89  0.64 -0.37 -0.59\nGentoo     0.65 -1.10  1.16  1.10\n```\n:::\n:::\n\n\nThe coefficients to project the data into the discriminant space, that is the eigenvectors of $W^{-1}B$ are: \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\np_lda$scaling\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     LD1   LD2\nbl -0.24 -2.31\nbd  2.04  0.19\nfl -1.20  0.08\nbm -1.22  1.24\n```\n:::\n:::\n\n\nand the predicted values, which include class predictions, and coordinates in the discriminant space are generated as:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\np_lda_pred <- predict(p_lda, penguins_sub)\n```\n:::\n\n\nThe best separation between classes can be viewed from this object, which can be shown to match the original data projected using the `scaling` component of the model object.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(colorspace)\nlibrary(ggplot2)\nlibrary(ggpubr)\np_lda_pred_x1 <- data.frame(p_lda_pred$x)\np_lda_pred_x1$species <- penguins_sub$species\np_lda1 <- ggplot(p_lda_pred_x1, \n                 aes(x=LD1, y=LD2, \n                     colour=species)) + \n  geom_point() +\n  xlim(-6, 8) + ylim(-6.5, 5.5) +\n  scale_color_discrete_divergingx(\"Zissou 1\") +\n  ggtitle(\"(a)\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1, legend.title = element_blank()) \n\np_lda_pred_x2 <- data.frame(as.matrix(penguins_sub[,1:4]) %*%\n                              p_lda$scaling)\np_lda_pred_x2$species <- penguins_sub$species\np_lda2 <- ggplot(p_lda_pred_x2, \n                 aes(x=LD1, y=LD2, \n                     colour=species)) + \n  geom_point() +\n  xlim(-6, 8) + ylim(-7, 5.5) +\n  scale_color_discrete_divergingx(\"Zissou 1\") +\n  ggtitle(\"(b)\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1, legend.title = element_blank()) \nggarrange(p_lda1, p_lda2, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 20 rows containing missing values (`geom_point()`).\nRemoved 20 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 17 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![Penguins projected into the 2D discriminant space, done two ways: (a) using the predicted values, (b) directly projecting using the model component. The scale is not quite the same but the projected data is identical in shape.](LDA_files/figure-pdf/fig-p-lda-1.pdf){#fig-p-lda fig-pos='H'}\n:::\n:::\n\n\nThe $W$ and $B$ matrices cannot be extracted from the model object, so we need to compute these separately. We only need $W$ actually. It is useful to think of this as the pooled variance-covariance matrix. Because the assumption for LDA is that the population group variance-covariances are identical, we estimate this by computing them for each class and then averaging them to get the pooled variance-covariance matrix. It's laborious, but easy.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\np_vc_pool <- mulgar::pooled_vc(penguins_sub[,1:4],\n                               penguins_sub$species)\np_vc_pool\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     bl   bd   fl   bm\nbl 0.31 0.18 0.13 0.18\nbd 0.18 0.32 0.14 0.20\nfl 0.13 0.14 0.23 0.16\nbm 0.18 0.20 0.16 0.31\n```\n:::\n:::\n\n\nThis can be used to draw an ellipse corresponding to the pooled variance-covariance that is used by the LDA model.\n\n\n::: {.cell}\n\n:::\n\n\n## Checking assumptions\n\nThis LDA approach is widely applicable, but it is useful\nto check the underlying assumptions on which it depends: (1) that the cluster structure corresponding to each class forms an ellipse, showing that the class is consistent with a sample from a multivariate normal distribution, and (2) that the variance of values around each mean is nearly the same. @fig-lda-assumptions1 and @fig-lda-assumptions2 illustrates two datasets, of which only one is consistent with these assumptions. Other parametric models, such as quadratic discriminant analysis or logistic regression, also depend on assumptions about the data which should be validated.  \\index{classification methods!quadratic discriminant analysis (QDA)} \\index{classification methods!logistic regression}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlda1 <- ggplot(penguins_sub, aes(x=bl, \n                         y=bd, \n                         colour=species)) +\n  geom_point() +\n  scale_color_discrete_divergingx(\"Zissou 1\") +\n  xlim(-2.5, 3) + ylim(-2.5, 2.5) +\n  ggtitle(\"(a)\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1) \np_ell <- NULL\nfor (i in unique(penguins_sub$species)) {\n  x <- penguins_sub %>% dplyr::filter(species == i)\n  e <- gen_xvar_ellipse(x[,1:2], n=150, nstd=1.5)\n  e$species <- i\n  p_ell <- bind_rows(p_ell, e)\n}\nlda2 <- ggplot(p_ell, aes(x=bl, \n                         y=bd, \n                         colour=species)) +\n  geom_point() +\n  scale_color_discrete_divergingx(\"Zissou 1\") +\n  xlim(-2.5, 3) + ylim(-2.5, 2.5) +\n  ggtitle(\"(b)\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1)\nggarrange(lda1, lda2, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Scatterplot of flipper length by bill length of the penguins data, and corresponding variance-covariance ellipses. There is a small amount of difference between the ellipses, but they are similar enough to be confident in assuming the population variance-covariances are equal.](LDA_files/figure-pdf/fig-lda-assumptions1-1.pdf){#fig-lda-assumptions1 fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now repeat for a data set that violates assumptions\ndata(bushfires)\nlda3 <- ggplot(bushfires, aes(x=log_dist_cfa, \n                         y=log_dist_road, \n                         colour=cause)) +\n  geom_point() +\n  scale_color_discrete_divergingx(\"Zissou 1\") +\n  xlim(6, 11) + ylim(-1, 10.5) +\n  ggtitle(\"(a)\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1)\nb_ell <- NULL\nfor (i in unique(bushfires$cause)) {\n  x <- bushfires %>% dplyr::filter(cause == i)\n  e <- gen_xvar_ellipse(x[,c(57, 59)], n=150, nstd=2)\n  e$cause <- i\n  b_ell <- bind_rows(b_ell, e)\n}\nlda4 <- ggplot(b_ell, aes(x=log_dist_cfa, \n                         y=log_dist_road, \n                         colour=cause)) +\n  geom_point() +\n  scale_color_discrete_divergingx(\"Zissou 1\") +\n  xlim(6, 11) + ylim(-1, 10.5) +\n  ggtitle(\"(b)\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1)\nggarrange(lda3, lda4, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![Scatterplot of distance to cfa and road for the bushfires data, and corresponding variance-covariance ellipses. There is a lot of difference between the ellipses, so it cannot be assumed that the population variance-covariances are equal.](LDA_files/figure-pdf/fig-lda-assumptions2-1.pdf){#fig-lda-assumptions2 fig-pos='H'}\n:::\n:::\n\n\nThis approach extends to any dimension. We would use the same projection sequence to view both the data and the variance-covariance ellipses, as in @fig-penguins-lda-ellipses. It can be seen that there is some difference in the shape and size of the ellipses in some projections, as there is with the spread of points in the projected data. However, \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tourr)\np_ell <- NULL\nfor (i in unique(penguins_sub$species)) {\n  x <- penguins_sub %>% dplyr::filter(species == i)\n  e <- gen_xvar_ellipse(x[,1:4], n=150, nstd=1.5)\n  e$species <- i\n  p_ell <- bind_rows(p_ell, e)\n}\np_ell$species <- factor(p_ell$species)\nload(\"data/penguins_tour_path.rda\")\nanimate_xy(p_ell[,1:4], col=factor(p_ell$species))\nrender_gif(penguins_sub[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\", col=penguins_sub$species),\n           gif_file=\"gifs/penguins_lda1.gif\",\n           frames=500,\n           loop=FALSE)\nrender_gif(p_ell[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\", col=p_ell$species),\n           gif_file=\"gifs/penguins_lda2.gif\",\n           frames=500,\n           loop=FALSE)\n```\n:::\n\n\n::: {#fig-penguins-lda-ellipses layout-ncol=2}\n\n::: {.content-hidden when-format=\"pdf\"}\n\n![Data](gifs/penguins_lda1.gif){#fig-lda-4D-assumptions1 fig-alt=\"Tour of penguins data, with colour indicating species\" width=300}\n\n![Variance-covariance ellipses](gifs/penguins_lda2.gif){#fig-lda-4D-assumptions2 fig-alt=\"Tour of ellipses corresponding to variance-covariance matrices for each species\" width=300}\n:::\n\nChecking the assumption of equal variance-covariance matrices for the 4D penguins data. \n:::\n\nAs a further check, we could generate three ellipses corresponding to the pooled variance-covariance matrix, as would be used in the model, centered at each of the means. Overlay this with the data. Now you will compare the spread of the observations in the data, with the elliptical shape of the pooled variance-covariance. If it matches reasonably we can safely use LDA. This can also be done group by group when multiple groups make it difficult to view all together. \n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tourr)\n# Create an ellipse corresponding to pooled vc\npool_ell <- gen_vc_ellipse(p_vc_pool, xm=rep(0, ncol(p_vc_pool)))\n\n# Add means to produce ellipses for each species\np_lda_pool <- data.frame(rbind(pool_ell +\n                                 matrix(rep(p_lda$means[1,],\n                                            each=nrow(pool_ell)),\n                                        ncol=4),\n                        pool_ell +\n                                 matrix(rep(p_lda$means[2,],\n                                            each=nrow(pool_ell)),\n                                        ncol=4),\n                        pool_ell +\n                                 matrix(rep(p_lda$means[3,],\n                                            each=nrow(pool_ell)),\n                                        ncol=4)))\n# Create one data set with means, data, ellipses\np_lda_pool$species <- factor(rep(levels(penguins_sub$species),\n                          rep(nrow(pool_ell), 3)))\np_lda_pool$type <- \"ellipse\"\np_lda_means <- data.frame(p_lda$means,\n                          species=factor(rownames(p_lda$means)),\n                          type=\"mean\")\np_data <- data.frame(penguins_sub[,1:5], type=\"data\")\np_lda_all <- bind_rows(p_lda_means,\n                       p_data,\n                       p_lda_pool)\np_lda_all$type <- factor(p_lda_all$type, \n                         levels=c(\"mean\", \"data\", \"ellipse\"))\nshapes <- c(3, 4, 20)\np_pch <- shapes[p_lda_all$type]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code to run the tour\nanimate_xy(p_lda_all[,1:4], col=p_lda_all$species, pch=p_pch)\nload(\"data/penguins_tour_path.rda\")\nrender_gif(p_lda_all[,1:4], \n           planned_tour(pt1), \n           display_xy(col=p_lda_all$species, pch=p_pch, \n                      axes=\"off\", half_range = 0.7),\n           gif_file=\"gifs/penguins_lda_pooled1.gif\",\n           frames=500,\n           loop=FALSE)\n\n# Focus on one species\nrender_gif(p_lda_all[p_lda_all$species == \"Gentoo\",1:4], \n           planned_tour(pt1), \n           display_xy(col=\"#F5191C\", \n                      pch=p_pch[p_lda_all$species == \"Gentoo\"], \n                      axes=\"off\", half_range = 0.7),\n           gif_file=\"gifs/penguins_lda_pooled2.gif\",\n           frames=500,\n           loop=FALSE)\n```\n:::\n\n\n::: {#fig-penguins-lda-ellipses layout-ncol=2}\n\n::: {.content-hidden when-format=\"pdf\"}\n\n![All species](gifs/penguins_lda_pooled1.gif){#fig-lda-pooled1 fig-alt=\"FIX ME\" width=300}\n\n![Gentoo](gifs/penguins_lda_pooled2.gif){#fig-lda-pooled1 fig-alt=\"FIX ME\" width=300}\n:::\n\nTour of penguins data, pooled variance-covariance ellipse overlaid. We can see that the pooled variance-covariance is a reasonable estimate of the spread of the data. \n:::\n\n## Examining results\n\nThe boundaries for a classification model can be examined by: \n\n1. generating a large number of observations in the domain of the data\n2. predicting the class for each\n\nWe'll look at this for 2D using the LDA model fitted to `bl`, and `bd` of the `penguins` data.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(classifly)\n\nload(\"data/penguins_sub.rda\")\np_bl_bd_lda <- lda(species~bl+bd, data=penguins_sub, \n                                  prior = c(1/3, 1/3, 1/3))\n```\n:::\n\n\nThe fitted model parameters are the means: \n$\\bar{x}_{Adelie} = ($ -0.95, 0.6$)^\\top$, $\\bar{x}_{Chinstrap} = ($ 0.89, 0.64$)^\\top$, and $\\bar{x}_{Gentoo} = ($ 0.65, -1.1$)^\\top$. \n\nThe boundaries can be examined using the `explore()` function from the `classifly` package, which generates observations in the range of all values of`bl` and `bd` and predicts their class. We can overlay the sample means and an ellipse corresponding to the variance-covariance also, by extracting these from the model object.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\np_bl_bd_lda_boundaries <- explore(p_bl_bd_lda, penguins_sub)\np_bl_bd_lda_m1 <- ggplot(p_bl_bd_lda_boundaries) +\n  geom_point(aes(x=bl, y=bd, colour=species, shape=.TYPE)) + \n  scale_color_discrete_divergingx(\"Zissou 1\") +\n  scale_shape_manual(values=c(46, 16)) +\n  theme_minimal() +\n  theme(aspect.ratio = 1, legend.position = \"none\")\n\np_bl_bd_lda_means <- data.frame(p_bl_bd_lda$means, species=rownames(p_bl_bd_lda$means))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\np_lda <- lda(species ~ ., penguins_sub[,1:5], prior = c(1/3, 1/3, 1/3))\np_lda_boundaries <- explore(p_lda, penguins_sub)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Code to run the tour\np_lda_boundaries$species\nanimate_slice(p_lda_boundaries[p_lda_boundaries$.TYPE == \"simulated\",1:4], col=p_lda_boundaries$species[p_lda_boundaries$.TYPE == \"simulated\"], v_rel=0.02, axes=\"bottomleft\")\nrender_gif(p_lda_boundaries[p_lda_boundaries$.TYPE == \"simulated\",1:4],\n           planned_tour(pt1),\n           display_slice(v_rel=0.02, \n             col=p_lda_boundaries$species[p_lda_boundaries$.TYPE == \"simulated\"], \n             axes=\"bottomleft\"),                     gif_file=\"gifs/penguins_lda_boundaries.gif\",\n           frames=500,\n           loop=FALSE\n           )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Project the boundaries into the 2D discriminant space\np_lda_b_sub <- p_lda_boundaries[\n  p_lda_boundaries$.TYPE == \"simulated\", \n  c(1:4, 6)]\np_lda_b_sub_ds <- data.frame(as.matrix(p_lda_b_sub[,1:4]) %*%\n  p_lda$scaling)\np_lda_b_sub_ds$species <- p_lda_b_sub$species\np_lda_b_sub_ds_p <- ggplot(p_lda_b_sub_ds, \n       aes(x=LD1, y=LD2, \n           colour=species)) +\n  geom_point(alpha=0.2) +  \n  scale_color_discrete_divergingx(\"Zissou 1\") +\n  theme_minimal() +\n  theme(aspect.ratio = 1, \n        legend.position = \"bottom\",\n        legend.title = element_blank()) \n```\n:::\n\n\n::: {#fig-penguins-lda-boundaries layout-ncol=2}\n\n::: {.content-hidden when-format=\"pdf\"}\n\n![4D](gifs/penguins_lda_boundaries.gif){#fig-lda-4D-boundaries fig-alt=\"Sliced tour to explore the boundaries produced by the LDA classifier on the penguins data.\" width=300}\n\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Discriminant space](LDA_files/figure-pdf/fig-lda-2D-boundaries-1.pdf){#fig-lda-2D-boundaries}\n:::\n:::\n\n\nExamining the boundaries produced by the LDA model in the full 4D with a slice tour and in the discriminant space. \n:::\n\n::: insight\nThe LDA boundaries divide the classes in the discriminant space, and the remaining directions are irrelevant.\n:::\n\n\n## Exercises {-}\n\n1. For the `simple_clusters` compute the lda model, and make a plot of the data, with points coloured by the class. Overlay variance-covariance ellipses, and a $+$ indicating the sample mean for each class. Is it reasonable to assume that the two classes are sampled from populations with the same variance-covariance?\n2. Examine the clusters corresponding to the classes in the `clusters` data set, using a tour. Based on the shape of the data is the assumption of equal variance-coviance reasonable?\n3. Examine the pooled variance-covariance for the `clusters` data, overlaid on the data in a tour on the 5D. Does it fit the variance of each cluster nicely?\n4. Fit an LDA model to the `simple_clusters` data. Examine the boundaries produced by the model, in 2D. \n5. Fit an LDA model to the `clusters` data. Examine the boundaries produced by the model in 5D.\n6. Assess the LDA assumptions for the `multicluster` data. Is LDA an appropriate model?\n\n",
    "supporting": [
      "LDA_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}