# Linear discriminant analysis

Linear discriminant analysis (LDA) dates to the early 1900s. It's one of the most elegant and simple techniques for both modeling separation between groups, and as an added bonus, producing a low-dimensional representation of the differences between groups. LDA has two strong assumptions: the groups are samples from multivariate normal distributions, and each have the same variance-covariance. If the latter assumption is relaxed, a slightly less elegant solution results from quadratic discriminant analysis.

Useful explanations can be found in @VR02 and @Ri96. A good general treatment of parametric methods for supervised classification can be found in @JW02 or another similar multivariate analysis textbook. It's also useful to know that hypothesis testing for the difference in multivariate means using multivariate analysis of variance (MANOVA) has similar assumptions to LDA. Also model-based clustering assumes that each cluster arises from a multivariate normal distribution, and is related to LDA. The methods described here can be used to check these assumptions when applying these methods, too.\index{classification methods!multivariate analysis of variance (MANOVA)}. \index{cluster analysis!model-based} 

<!-- Algorithmic methods have overtaken parametric methods in the practice of supervised classification.  A parametric method such as linear discriminant analysis (LDA) yields a set of interpretable output parameters, so it leaves a clear trail helping us to understand what was done to produce the results.  An algorithmic method, on the other hand, is more or less a black box, with various input parameters that are adjusted to tune the algorithm.  The algorithm's input and output parameters do not always correspond in any obvious way to the interpretation of the results.  Because if it's simplicity, if the assumptions (approximately) hold, LDA can provide an elegant classification solution.-->

## Definition

Fisher's linear discriminant [@Fi36] computes a linear combination of the variables that separates two classes by comparing the differences between class means with the variance of values within each class. It makes no assumptions about the distribution of the data. 

Linear discriminant analysis (LDA), as proposed by @Rao48, formalizes Fisher's approach, by recognising that it arises from making the assumption that the data values for each class arise from a $p$-dimensional multivariate normal distribution, sharing a common variance-covariance matrix with data from other classes. When this assumption holds, Fisher's linear discriminant gives the optimal separation between the two groups.

For two equally weighted groups, where $Y$ is coded as $\{0, 1\}$, the LDA rule is:

*Allocate a new observation $X_0$ to group 1 if*

$$(\bar{X}_1-\bar{X}_2)'S^{-1}_{\rm pooled}X_0 \geq 
  \frac{1}{2}(\bar{X}_1-\bar{X}_2)'S^{-1}_{\rm pooled}
  (\bar{X}_1+\bar{X}_2)$$

*else allocate it to group 2,*

where $\bar{X}_k $ are the class mean vectors of an 
$n\times p$ data matrix $X_k ~~(k=1,2)$, 


$$S_{\rm pooled} = \frac{(n_1-1) S_1}{(n_1-1)+(n_2-1)} + \frac{(n_2-1) S_2}{(n_1-1)+(n_2-1)}$$

is the pooled variance-covariance matrix, and

$$S_k = \frac{1}{n-1}\sum_{i=1}^{n}
(X_{ki}-\bar{X}_k)(X_{ki}-\bar{X}_k)', ~~k=1,2$$

is the class variance--covariance matrix. The linear
discriminant part of this rule is
$(\bar{X}_1-\bar{X}_2)'S^{-1}_{\rm pooled}$, which defines the linear combination of variables that best separates the two groups.  To define a classification rule, we compute the value of the
new observation $X_0$ on this line and compare it with the value of the average of the two class means $(\bar{X}_1+\bar{X}_2)/2$ on the same line.

For multiple $(g)$ classes, the rule and the discriminant space are constructed using the between-group sum-of-squares matrix,

$$B=\sum_{k=1}^g n_k(\bar{X}_k-\bar{X})(\bar{X}_k-\bar{X})'$$

which measures the differences between the class means, 
compared with the overall data mean $\bar{X}$ and the within-group sum-of-squares matrix,

$$W =
\sum_{k=1}^g\sum_{i=1}^{n_k}
(X_{ki}-\bar{X}_k)(X_{ki}-\bar{X}_k)'$$

which measures the variation of values around each class mean. The linear discriminant space is generated by computing the eigenvectors (canonical coordinates) of $W^{-1}B$, and this is the space where the group means are most separated with respect to the
pooled variance--covariance. The resulting classification rule is to allocate a new observation to the class with the highest value of

$$\bar{X}_k'S^{-1}_{\rm pooled}X_0 - 
\frac{1}{2}\bar{X}_k'S^{-1}_{\rm pooled}\bar{X}_k ~~~k=1,...,g$$

which results in allocating the new observation into the
class with the closest mean.

## Checking assumptions

This LDA approach is widely applicable, but it is useful
to check the underlying assumptions on which it depends: (1) that the cluster structure corresponding to each class forms an ellipse, showing that the class is consistent with a sample from a multivariate normal distribution, and (2) that the variance of values
around each mean is nearly the same. @fig-lda-assumptions1 and @fig-lda-assumptions2
illustrates two datasets, of which only one is consistent with these assumptions. Other parametric models, such as quadratic discriminant analysis or logistic regression, also depend on assumptions about the data which should be validated.  \index{classification methods!quadratic discriminant analysis (QDA)} \index{classification methods!logistic regression}

```{r}
#| label: fig-lda-assumptions1
#| fig-cap: "Scatterplot of flipper length by bill length of the penguins data, and corresponding variance-covariance ellipses. There is a small amount of difference between the ellipses, but they are similar enough to be confident in assuming the population variance-covariances are equal."
#| message: false
#| fig-width: 8
#| fig-height: 4
library(dplyr)
library(colorspace)
library(ggplot2)
library(mulgar)
library(MASS)
library(ggpubr)
load("data/penguins_sub.rda")
lda1 <- ggplot(penguins_sub, aes(x=bl, 
                         y=bd, 
                         colour=species)) +
  geom_point() +
  scale_color_discrete_qualitative("Dark 3") +
  xlim(-2.5, 3) + ylim(-2.5, 2.5) +
  ggtitle("(a)") +
  theme(aspect.ratio = 1) 
p_ell <- NULL
for (i in unique(penguins_sub$species)) {
  x <- penguins_sub %>% dplyr::filter(species == i)
  e <- gen_xvar_ellipse(x[,1:2], n=150, nstd=1.5)
  e$species <- i
  p_ell <- bind_rows(p_ell, e)
}
lda2 <- ggplot(p_ell, aes(x=bl, 
                         y=bd, 
                         colour=species)) +
  geom_point() +
  scale_color_discrete_qualitative("Dark 3") +
  xlim(-2.5, 3) + ylim(-2.5, 2.5) +
  ggtitle("(b)") +
  theme(aspect.ratio = 1)
ggarrange(lda1, lda2, ncol=2, 
          common.legend = TRUE, legend = "bottom")
```

```{r}
#| label: fig-lda-assumptions2
#| fig-cap: "Scatterplot of distance to cfa and road for the bushfires data, and corresponding variance-covariance ellipses. There is a lot of difference between the ellipses, so it cannot be assumed that the population variance-covariances are equal."
#| message: false
#| fig-width: 8
#| fig-height: 4
# Now repeat for a data set that violates assumptions
data(bushfires)
lda3 <- ggplot(bushfires, aes(x=log_dist_cfa, 
                         y=log_dist_road, 
                         colour=cause)) +
  geom_point() +
  scale_color_discrete_qualitative("Dark 3") +
  xlim(6, 11) + ylim(-1, 10.5) +
  ggtitle("(a)") +
  theme(aspect.ratio = 1)
b_ell <- NULL
for (i in unique(bushfires$cause)) {
  x <- bushfires %>% dplyr::filter(cause == i)
  e <- gen_xvar_ellipse(x[,c(57, 59)], n=150, nstd=2)
  e$cause <- i
  b_ell <- bind_rows(b_ell, e)
}
lda4 <- ggplot(b_ell, aes(x=log_dist_cfa, 
                         y=log_dist_road, 
                         colour=cause)) +
  geom_point() +
  scale_color_discrete_qualitative("Dark 3") +
  xlim(6, 11) + ylim(-1, 10.5) +
  ggtitle("(b)") +
  theme(aspect.ratio = 1)
ggarrange(lda3, lda4, ncol=2, 
          common.legend = TRUE, legend = "bottom")

```

```{r}
#| label: fig-LDA-4D
#| fig-cap: "Examining the 4D normality assumption"
#| message: false
#| fig-width: 8
#| fig-height: 4
#| eval: false
library(tourr)
p_ell <- NULL
for (i in unique(penguins_sub$species)) {
  x <- penguins_sub %>% dplyr::filter(species == i)
  e <- gen_xvar_ellipse(x[,1:4], n=150, nstd=1.5)
  e$species <- i
  p_ell <- bind_rows(p_ell, e)
}
p_ell$species <- factor(p_ell$species)
load("data/penguins_tour_path.rda")
animate_xy(p_ell[,1:4], col=factor(p_ell$species))
render_gif(penguins_sub[,1:4], 
           planned_tour(pt1), 
           display_xy(half_range=0.9, axes="off", col=penguins_sub$species),
           gif_file="gifs/penguins_lda1.gif",
           frames=500,
           loop=FALSE)
render_gif(p_ell[,1:4], 
           planned_tour(pt1), 
           display_xy(half_range=0.9, axes="off", col=p_ell$species),
           gif_file="gifs/penguins_lda2.gif",
           frames=500,
           loop=FALSE)
```

::: {#fig-penguins-lda-ellipses layout-ncol=2}

![Data](gifs/penguins_lda1.gif){#fig-lda-4D-assumptions1 fig-alt="Tour of penguins data, with colour indicating species" width=300}

![Variance-covariance ellipses](gifs/penguins_lda2.gif){#fig-lda-4D-assumptions2 fig-alt="Tour of ellipses corresponding to variance-covariance matrices for each species" width=300}

Checking the assumption of equal variance-covariance matrices for the 4D penguins data. 
:::


## Examining results

```{r}
#| eval: false
library(classifly)
library(MASS)
penguins_lda <- lda(species ~ ., penguins_sub, prior = c(1/3, 1/3, 1/3))
p_lda_boundaries <- explore(penguins_lda, penguins_sub)
animate_slice(p_lda_boundaries[p_lda_boundaries$.TYPE == "simulated",1:4], col=p_lda_boundaries[p_lda_boundaries$.TYPE == "simulated",6], v_rel=0.02, axes="bottomleft")
render_gif(p_lda_boundaries[p_lda_boundaries$.TYPE == "simulated",1:4],
           planned_tour(pt1),
           display_slice(v_rel=0.02, 
             col=p_lda_boundaries[ p_lda_boundaries$.TYPE == "simulated",6], 
             axes="bottomleft"),                     gif_file="gifs/penguins_lda_boundaries.gif",
           frames=500,
           loop=FALSE
           )
```


![](gifs/penguins_lda_boundaries.gif){#fig-lda-4D-boundaries fig-alt="Sliced tour to explore the boundaries produced by the LDA classifier on the penguins data." width=300}