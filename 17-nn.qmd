# Neural networks and deep learning

üõ†Ô∏è  **UNDER DEVELOPMENT**

This chapter will likely include:

- Models at nodes or epochs 
- Predictions (like vote matrix) for training and test. It provides a visual guide to overfitting.
- Classification boundaries comparison with other methods
- Explainability and interpretability

(This paper https://distill.pub/2020/grand-tour/ has good examples)

\index{classification methods!neural network}

```{r}
#| eval: false
#| echo: false
# Penguins example
# You need to install python and tensorflow, following
# instructions at https://tensorflow.rstudio.com/install/
library(mulgar)
load("data/penguins_sub.rda") # from mulgar book

library(ggplot2)
library(dplyr)
library(keras)
library(tidymodels)
library(tourr)
library(classifly)

# Create training and test split - absolutely vital now
set.seed(1210)
p_split <- initial_split(penguins_sub[,1:5])
p_train <- training(p_split)
p_test <- testing(p_split)
p_train_x <- p_train[,1:4] %>% as.matrix()
p_train_y <- p_train %>% pull(species) %>% as.numeric() 
p_train_y <- p_train_y-1 # Needs to be 0, 1, 2
p_test_x <- p_test[,1:4] %>% as.matrix()
p_test_y <- p_test %>% pull(species) %>% as.numeric() 
p_test_y <- p_test_y-1

# Check training and test split
p_split_check <- bind_rows(
  bind_cols(p_train, type = "train"), 
  bind_cols(p_test, type = "test")) %>%
  mutate(type = factor(type))
animate_xy(p_split_check[,1:4], 
           col=p_split_check$species,
           pch=p_split_check$type)

# Define model
model <- keras_model_sequential()
model %>% 
  layer_dense(units = 4, activation = 'relu', input_shape = 4) %>% 
  layer_dense(units = 3, activation = 'softmax')
model %>% summary

loss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)

model %>% compile(
  optimizer = "adam",
  loss      = loss_fn,
  metrics   = c('accuracy')
)

# Fit model
fit <- model %>% fit(
  x = p_train_x, y = p_train_y,
  epochs           = 50
)
plot(fit)

# Check accuracy
score <- model %>% evaluate(p_test_x, p_test_y, verbose = 0)

# Predict
predictions <- model %>% predict(p_test_x)
predictions_cat <- levels(p_test$species)[apply(predictions, 1, which.max)]
predictions_cat <- factor(predictions_cat, levels=levels(p_test$species))
table(p_test$species, predictions_cat)

# Examine boundaries
# p_nn_boundaries <- explore(model, penguins_sub)
p_grid <- tibble(bl = runif(10000, -2.2, 2.9),
                 bd = runif(10000, -2.1, 2.2),
                 fl = runif(10000, -2.1, 2.15),
                 bm = runif(10000, -1.9, 2.6))
p_grid_x <- p_grid %>% as.matrix()
p_grid_pred <- model %>% predict(p_grid_x)
p_pred_cat <- levels(p_test$species)[apply(p_grid_pred, 1, which.max)]
p_pred_cat <- factor(p_pred_cat, levels=levels(p_test$species))

p_grid <- p_grid %>%
  mutate(species = p_pred_cat)

animate_slice(p_grid[,1:4], col=p_grid$species)

# Tour using same sequence as LDA boundaries
load("data/penguins_tour_path.rda")
render_gif(p_grid[,1:4],
           planned_tour(pt1),
           display_slice(v_rel=0.3, 
             col=p_grid$species, 
             axes="bottomleft"),                     gif_file="gifs/penguins_nn_boundaries.gif",
           frames=500,
           loop=FALSE
           )

```

::: {.content-visible when-format="html"}

::: {#fig-penguins-lda-tree-html layout-ncol=2}

![LDA model](gifs/penguins_lda_boundaries.gif){#fig-lda-boundary fig-alt="FIX ME" width=300}

![NN model](gifs/penguins_nn_boundaries.gif){#fig-tree-boundary fig-alt="FIX ME" width=300}

Comparison of the boundaries produced by the LDA (a) and the CNN (b) model, using a slice tour. 
:::
:::

\index{tour!slice} 

```{r}
#| eval: false
#| echo: false
# Explore misclassifications
# Predict
p_test <- p_test %>% 
  mutate(pred = predictions_cat)
pred_tr <- model %>% predict(p_train_x)
pred_tr_cat <- levels(p_train$species)[apply(pred_tr, 1, which.max)]
pred_tr_cat <- factor(pred_tr_cat, levels=levels(p_train$species))
p_train <- p_train %>% 
  mutate(pred = pred_tr_cat)
p_all <- bind_rows(p_train, p_test)
p_all <- p_all %>% 
  mutate(err = ifelse(p_all$pred !=
                        p_all$species, 1, 0))

symbols <- c(1, 16)
p_pch <- symbols[p_all$err+1]
p_cex <- rep(1, length(p_pch))
p_cex[p_all$err==1] <- 2
animate_xy(p_all[,1:4],
           col=p_all$species,
           pch=p_pch, cex=p_cex)
render_gif(p_all[,1:4],
           grand_tour(),
           display_xy(col=p_all$species,
                      pch=p_pch, cex=p_cex),
           gif_file="gifs/p_nn_errors.gif",
           frames=500,
           width=400,
           height=400)
animate_xy(p_all[,1:4], 
           guided_tour(lda_pp(p_all$species)),
           col=p_all$species,
           pch=p_pch, cex=p_cex)
render_gif(p_all[,1:4],
           guided_tour(lda_pp(p_all$species)),
           display_xy(col=p_all$species,
                      pch=p_pch, cex=p_cex),
           gif_file="gifs/p_nn_errors_guided.gif",
           frames=500,
           width=400,
           height=400)
```

```{r}
# Explore probabilistic predictions like vote matrix
```

```{r}
# Models at the nodes
layer1 <- get_layer(model, index=1)
layer2 <- get_layer(model, index=2)
layer1$weights
```

```{r}
#| eval: false
#| echo: false
# Following https://parsnip.tidymodels.org/articles/Examples.html
library(nnet)
library(mulgar)
library(tidymodels)
tidymodels_prefer()

data(sketches_train)
sketches_sub <- sketches_train %>%
  #filter(word %in% c("crab", "flip flops")) %>%
  #mutate(word = factor(word)) %>% 
  select(-id)

set.seed(423)
sketches_split <- initial_split(sketches_sub)
sk_tr <- training(sketches_split)
sk_ts <- testing(sketches_split)

mlp_cls_spec <- 
    mlp(hidden_units = 12, 
        penalty = 0.1, 
        epochs = 1000) %>% 
    set_mode("classification") %>% 
    set_engine("nnet", MaxNWts = 10000)
sk_wf <- workflow() %>%
  add_model(mlp_cls_spec) %>%
  add_formula(word ~ .)
sk_grid <- grid_regular(dials::hidden_units(),
                        dials::penalty(), 
                        levels=3)
  
  #expand_grid(hidden_units = c(6, 10, 12), epochs = c(500, 1000), penalty=c(0, 0.1))
sk_folds <- vfold_cv(sk_tr, v = 5)
sk_res <- 
  sk_wf %>% 
  tune_grid(
    resamples = sk_folds, 
    grid = sk_grid
    )
sk_res %>% 
  collect_metrics()
sk_res %>%
  show_best("accuracy")

mlp_cls_fit <- mlp_cls_spec %>% 
    fit(word ~ ., data = sk_tr)
#mlp_cls_fit
sk_pred <- bind_cols(sk_ts,
    predict(mlp_cls_fit, sk_ts),
    predict(mlp_cls_fit, sk_ts, type = "prob")
  )
bal_accuracy(sk_pred, word, .pred_class)

sk_pred %>% count(word, .pred_class) %>%
  pivot_wider(names_from = `.pred_class`, values_from = n)


```

<!--
Neural networks for classification can be thought of as additive
models where explanatory variables are transformed, usually through a
logistic function, added to other explanatory variables, transformed
again, and added again to yield class predictions. Aside from the data
mining literature, mentioned earlier, a good comprehensive and
accessible description for statisticians can be found in
\citeasnoun{CT94}. The model can be formulated as:

\[
\hat{y} = f(x) = \phi(\alpha+\sum_{h=1}^{s}
w_{h}\phi(\alpha_h+\sum_{i=1}^{p} w_{ih}x_i))
\]

\index{R package!\RPackage{nnet}}

\noindent where $x$ is the vector of explanatory variable values, $y$
is the target value, $p$ is the number of variables, $s$ is the number
of nodes in the single hidden layer, and $\phi$ is a fixed function,
usually a linear or logistic function. This model has a single hidden
layer and univariate output values.  The model is fit by minimizing
the sum of squared differences between observed values and fitted
values, and the minimization does not always converge. A neural
network is a black box that accepts inputs, computes, and spits out
predictions.  With graphics, some insight into the black box can be
gained. We use the feed-forward neural network provided in the {\tt
nnet} package of R \cite{VR02} to illustrate.

We continue to work with \Data{Olive Oils}, and we look at the
performance of the neural network in classifying the oils in the four
areas of the South, a difficult challenge. Because the software
does not include a method for computing the predictive error, we break
the data into training and test samples so we can better estimate the
predictive error.  (We could tweak the neural network to perfectly fit
all the data, but then we could not estimate how well it would perform
with new data.)

\begin{verbatim}
> indx.tst <- c(1,7,12,15,16,22,27,32,34,35,36,41,50,54,61,
 68,70,75,76,80,95,101,102,105,106,110,116,118,119,122,134,
 137,140,147,148,150,151,156,165,175,177,182,183,185,186,
 187,190,192,194,201,202,211,213,217,218,219,225,227,241,
 242,246,257,259,263,266,274,280,284,289,291,292,297,305,
 310,313,314,323,330,333,338,341,342,347,351,352,356,358,
 359,369,374,375,376,386,392,405,406,415,416,418,420,421,
 423,426,428,435,440,451,458,460,462,466,468,470,474,476,
 480,481,482,487,492,493,500,501,509,519,522,530,532,541,
 543,545,546,551,559,567,570)
> d.olive.train <- d.olive[-indx.tst,]
> d.olive.test <- d.olive[indx.tst,]
> d.olive.sth.train <- subset(d.olive.train, region==1, 
   select=area:eicosenoic)
> d.olive.sth.test <- subset(d.olive.test, region==1, 
   select=area:eicosenoic)
\end{verbatim}

% this may need a few more words

After trying several values for $s$, the number of nodes in the hidden
layer, we chose $s=4$; we also chose a linear $\phi$, $decay=0.005$,
and $range=0.06$. We fit the model using many different random
starting values, rejecting the results until it eventually converged
to a solution with a reasonably low error:

\begin{verbatim}
> library(nnet)
> olive.nn <- nnet(as.factor(area)~., d.olive.sth.train, 
  size=4, linout=T, decay=0.005, range=0.06, maxit=1000)
> targetr <- class.ind(d.olive.sth.train[,1])
> targets <- class.ind(d.olive.sth.test[,1])
> test.cl <- function(true, pred){
    true <- max.col(true)
    cres <- max.col(pred)
    table(true, cres)
  }
> test.cl(targetr, predict(olive.nn, 
  d.olive.sth.train[,-1]))
    cres
true   1   2   3   4
   1  16   0   1   2
   2   0  42   0   0
   3   0   1 155   2
   4   1   1   1  24
\end{verbatim}
\newpage  % Insert page break to avoid breaking the R output.
\begin{verbatim}
> test.cl(targets, predict(olive.nn, d.olive.sth.test[,-1]))
    cres
true  1  2  3  4
   1  3  2  0  1
   2  0 12  2  0
   3  0  2 45  1
   4  1  2  1  5
> parea <- c(max.col(predict(olive.nn, 
   d.olive.sth.train[,-1])),
   max.col(predict(olive.nn, d.olive.sth.test[,-1])))
> d.olive.nn <- cbind(rbind(d.olive.sth.train, 
    d.olive.sth.test), parea)
> gd <- ggobi(d.olive.nn)[1]
> glyph_color(gd) <- c(6,3,2,9)[d.olive.nn$area]
\end{verbatim}

Below are the misclassification tables for the training and test
samples.

\bigskip
\emph{Training:}
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\ \cline{3-7}

\T         & North Apulia & 16 & 0 & {\bf 1} & {\bf 2} & 0.158\\
\Vbl{area} & Calabria & 0 & 42 & 0 &  0 & 0.000\\
           & South Apulia & 0 & {\bf 1} & 155 & {\bf 2} & 0.019\\
\B         & Sicily & {\bf 1} & {\bf 1} & {\bf 1} & 24 & 0.111\\ \cline{3-7}
\T         &        &         &         &         &    & 0.037
\end{tabular}
\end{center}

\bigskip

\emph{Test:}
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\  \cline{3-7}

\T         & North Apulia & 3 & {\bf 2} & 0 & {\bf 1} & 0.333\\
\Vbl{area} & Calabria & 0 & 12 & {\bf 2} &  0 & 0.143\\
           & South Apulia & 0 & {\bf 2} & 45 & {\bf 1} & 0.063\\
\B         & Sicily & {\bf 1} & {\bf 2} & {\bf 1} & 5 & 0.444\\  \cline{3-7}
\T         &        &         &         &         &    & 0.156
\end{tabular}
\end{center}

\bigskip

\noindent The training error is $9/246=0.037$, and the test error is
$12/77=0.156$.  The overall errors, as in the random forest model, are
not uniform across classes.  This is particularly obvious in the test
error table: The error in classifying North Apulian oils is close to a
third, and it is even worse for Sicilian oils, which have an almost
even chance of being misclassified.

Our exploration of the misclassifications is shown in
Fig.~\ref{olive-nn}.  (The troublesome Sicilian oils have been
excluded from all plots in this figure.)  Consider first the plots in
the top row.  The left-hand plot shows the misclassification table.
Two samples of oils from North Apulia (orange $+$) have been
incorrectly classified as South Apulian (pink $\times$), and these two
points have been brushed as filled orange circles.  Note where these
points fall in the next two plots, which are linked 2D tour
projections. \index{brushing!linked}\index{tour!grand} One of the two
misclassified points is on the edge of the cluster of North Apulian
points, close to the Calabrian cluster. It is understandable that
there might be some confusion about this case. The other sample is on
the outer edge of the North Apulian cluster, but it is far from the
Calabrian cluster ~---~ this should not have been confused.

% Figure 13
\begin{figure*}[htbp]
\centerline{
 {\includegraphics[width=1.5in]{chap-class/olive-nn1.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-nn2.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-nn8.pdf}}}
\smallskip
\centerline{
 {\includegraphics[width=1.5in]{chap-class/olive-nn3.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-nn4.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-nn6.pdf}}}
\caption[Misclassifications of a feed-forward neural network
classifying the oils from the South]{Misclassifications of a
feed-forward neural network classifying the oils from the South by
\Vbl{area}. A representation of the misclassification table {\bf (left
column)} is linked to projections viewed in a 2D tour. Different
misclassifications are examined in the top and bottom rows. (The
Sicilian oils, which would have appeared in the top row of the
misclassification tables, have been removed from all plots.)  }
\label{olive-nn}
\end{figure*}

In the bottom row of plots, we follow the same procedure to examine
the single North Apulian sample misclassified as South Apulian. It is
painted as a filled orange circle in the misclassification plot and
viewed in a \index{tour!grand} tour. This point is on the outer edge
of the North Apulian cluster, but it is closer to the Calabrian cluster
than the South Apulian cluster. It would be understandable for it to
be misclassified as Calabrian, so it is puzzling that it is
misclassified as South Apulian.

In summary, a neural network is a black box method for tackling tough
classification problems. It will generate different solutions each
time the net is fit, some much better than others. When numerical
measures suggest that a reasonable model has been found, graphics can
be used to inspect the model in more detail.
-->
