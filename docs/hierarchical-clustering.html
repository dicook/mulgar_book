<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Interactive and dynamic graphics for high-dimensional data using R - 8&nbsp; Hierarchical clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./kmeans-clustering.html" rel="next">
<link href="./unsupervised.html" rel="prev">
<link href="./cover.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hierarchical clustering</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Interactive and dynamic graphics for high-dimensional data using R</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./dimension.html" class="sidebar-item-text sidebar-link">Dimension reduction</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pca.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Principal component analysis (PCA)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nldr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Non-linear dimension reduction</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./supervised.html" class="sidebar-item-text sidebar-link">Supervised learning</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression methods</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./LDA.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear discriminant analysis and MANOVA</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./forests.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Trees and forests</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nn.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Neural networks and deep learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./unsupervised.html" class="sidebar-item-text sidebar-link">Unsupervised learning</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./hierarchical-clustering.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hierarchical clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./kmeans-clustering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">k-means clustering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model-based-clustering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">model-based clustering</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./temporal.html" class="sidebar-item-text sidebar-link">Time series</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./multivariate-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multiple time series</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./toolbox.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Toolbox</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Data</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Hierarchical clustering</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>The aim of unsupervised classification, or cluster analysis, is to organize observations into similar groups. Cluster analysis is a commonly used, appealing, and conceptually intuitive, statistical method. Some of its uses include market segmentation, where customers are grouped into clusters with similar attributes for targeted marketing; gene expression analysis, where genes with similar expression patterns are grouped together; and the creation of taxonomies of animals, insects, or plants. A cluster analysis results in a simplification of a dataset for two reasons: first, because the dataset can be summarized by a description of each cluster, and second, because each cluster, which is now relatively homogeneous, can be analyzed separately. Thus, it can be used to effectively reduce the size of massive amounts of data.</p>
<p>Organizing objects into groups is a task that seems to come naturally to humans, even to small children, and perhaps this is why it is an apparently intuitive method in data analysis. However, cluster analysis is more complex than it initially appears. Many people imagine that it will produce neatly separated clusters like those in the top left plot of <span class="citation" data-cites="ideal-clusters">(<a href="#ref-ideal-clusters" role="doc-biblioref"><strong>ideal-clusters?</strong></a>)</span>, but it almost never does. Such ideal clusters are rarely encountered in real data, so we often need to modify our objective from <code>find the natural clusters in this data'' to</code>organize the cases into groups that are similar in some way.’’ Even though this may seem disappointing when compared with the ideal, it is still often an effective means of simplifying and understanding a dataset.</p>
% Figure 1
<p>At the heart of the clustering process is the work of discovering which variables are most important for defining the groups. It is often true that we only require a subset of the variables for finding clusters, whereas another subset (called ) has no impact. In the bottom left plot of Fig.~<span class="math inline">\(\ref{ideal-clusters}\)</span>, it is clear that the variable plotted horizontally is important for splitting this data into two clusters, whereas the variable plotted vertically is a nuisance variable. Nuisance is an apt term for these variables, because they can radically change the interpoint distances and impair the clustering process. </p>
<p>Dynamic graphical methods help us to find and understand the cluster structure in high dimensions. With the tools in our toolbox, primarily tours, along with linked scatterplots and parallel coordinate plots, we can see clusters in high-dimensional spaces. We can detect gaps between clusters, the shape and relative positions of clusters, and the presence of nuisance variables. We can even find unusually shaped clusters, like those in the bottom right plot in Fig.~<span class="math inline">\(\ref{ideal-clusters}\)</span>. In simple situations we can use graphics alone to group observations into clusters, using a ``spin and brush’’ method. In more difficult data problems, we can assess and refine numerical solutions using graphics. </p>
<p>This chapter discusses the use of interactive and dynamic graphics in the clustering of data. Section <span class="math inline">\(\ref{clust-bg}\)</span> introduces cluster analysis, focusing on interpoint distance measures. Section <span class="math inline">\(\ref{clust-graphics}\)</span> describes an example of a purely graphical approach to cluster analysis, the spin and brush method. In the example shown in that section, we were able to find simplifications of the data that had not been found using numerical clustering methods, and to find a variety of structures in high-dimensional space. Section <span class="math inline">\(\ref{clust-num}\)</span> describes methods for {reducing} the interpoint distance matrix to an intercluster distance matrix using hierarchical algorithms and model-based clustering, and shows how graphical tools are used to assess the results of numerical methods. Section <span class="math inline">\(\ref{clust-recap}\)</span> summarizes the chapter and revisits the data analysis strategies used in the examples. A good companion to the material presented in this chapter is , which provides data and code for practical examples of cluster analysis using R. Section <span class="math inline">\(\ref{clust-recap}\)</span> summarizes the chapter and revisits the data analysis strategies used in the examples.</p>
<p>~</p>
<p>Before we can begin finding groups of cases that are similar, we need to decide on a definition of similarity. How is similarity defined? Consider a dataset with three cases and four variables, described in matrix format as</p>
<p><span class="math display">\[\begin{eqnarray*}
\blX = \left[ \begin{array}{c}
     \blX_1 \\ \blX_2 \\ \blX_3 \\
%\blX_4 \\
%     \blX_5 \\ \blX_6 \\ \blX_7 \\ \blX_8 \\ \blX_9
     \end{array} \right] =
     \left[ \begin{array}{rrrr}
       7.3 &amp; 7.6 &amp; 7.7 &amp; 8.0 \\
       7.4 &amp; 7.2 &amp; 7.3 &amp; 7.2 \\
%       6.7 &amp; 7.2 &amp; 7.1 &amp; 7.4 \\
       4.1 &amp; 4.6 &amp; 4.6 &amp; 4.8 \\
%       5.9 &amp; 6.7 &amp; 6.7 &amp; 6.6 \\
%       8.8 &amp; 8.2 &amp; 8.1 &amp; 8.1 \\
%       6.1 &amp; 6.5 &amp; 6.4 &amp; 6.6 \\
%       6.3 &amp; 6.1 &amp; 6.7 &amp; 6.6 \\
%       8.3 &amp; 8.3 &amp; 8.6 &amp; 8.5 \\
     \end{array} \right]
\end{eqnarray*}\]</span></p>
<p>which is plotted in Fig.~<span class="math inline">\(\ref{similarity1}\)</span>. The Euclidean distance between two cases (rows of the matrix) is defined as</p>
<p><span class="math display">\[\begin{eqnarray*}
d_{\rm Euc}(\blX_i,\blX_j) &amp;=&amp; ||\blX_i-\blX_j|| %\\
% &amp;=&amp; \sqrt{(X_{i1}-X_{j1})^2+\dots + (X_{ip}-X_{jp})^2},
~~~~~~i,j=1,\dots, n,
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(||\blX_i||=\sqrt{X_{i1}^2+X_{i2}^2+\dots +X_{ip}^2}\)</span>. For example, the Euclidean distance between cases 1 and 2 in the above data, is</p>
<p>[ = 1.0. ]</p>
<p></p>
<p>For the three cases, the interpoint Euclidean distance matrix is</p>
<p><span class="math display">\[\begin{eqnarray*}
d_{\rm Euc} =
\left[ \begin{array}{ccc}
0.0  ~&amp;     &amp;   \\
1.0 ~&amp;  0.0 ~  &amp;  \\
6.3 ~&amp; 5.5 ~&amp;  0.0 ~ \\
\end{array} \right]
\begin{array}{r}
\blX_1 \\ \blX_2 \\ \blX_3 \\
\end{array}
\end{eqnarray*}\]</span></p>
% Figure 2
<p>Cases 1 and 2 are more similar to each other than they are to case 3, because the Euclidean distance between cases 1 and 2 is much smaller than the distance between cases 1 and 3 and between cases 2 and 3.</p>
<p>There are many different ways to calculate similarity. In recent years similarity measures based on correlation distance have become common. Correlation distance is typically used where similarity of structure is more important than similarity in magnitude.</p>
<p></p>
<p>As an example, see the parallel coordinate plot of the sample data at the right of Fig.~<span class="math inline">\(\ref{similarity1}\)</span>. Cases 1 and 3 are widely separated, but their shapes are similar (low, medium, medium, high). Case 2, although overlapping with Case 1, has a very different shape (high, medium, medium, low). The correlation between two cases is defined as</p>
<p><span class="math display">\[\begin{eqnarray}
\rho(\blX_i,\blX_j) = \frac{(\blX_i-c_i)'(\blX_j-c_j)}
{\sqrt{(\blX_i-c_i)'(\blX_i-c_i)} \sqrt{(\blX_j-c_j)'(\blX_j-c_j)}}
\label{corc}
\end{eqnarray}\]</span></p>
<p>When <span class="math inline">\(c_i, c_j\)</span> are the sample means <span class="math inline">\(\bar{\blX}_i,\bar{\blX}_j\)</span>, then <span class="math inline">\(\rho\)</span> is the Pearson correlation coefficient. If, indeed, they are set at 0, as is commonly done, <span class="math inline">\(\rho\)</span> is a generalized correlation that describes the angle between the two data vectors. The correlation is then converted to a distance metric; one equation for doing so is as follows:</p>
<p><span class="math display">\[\begin{eqnarray*}
d_{\rm Cor}(\blX_i,\blX_j) = \sqrt{2(1-\rho(\blX_i,\blX_j))}
\end{eqnarray*}\]</span></p>
<p>%Distance measures built on correlation are effectively %angular distances between points, because for two vectors <span class="math inline">\(\blX_i\)</span> and %<span class="math inline">\(\blX_j\)</span>, <span class="math inline">\(\cos (\angle(\blX_i,\blX_j)) \propto \blX_i'\blX_j\)</span>. The above distance metric will treat cases that are strongly negatively correlated as the most distant.</p>
<p>The interpoint distance matrix for the sample data using <span class="math inline">\(d_{\rm Cor}\)</span> and the Pearson correlation coefficient is</p>
<p><span class="math display">\[\begin{eqnarray*}
d_{\rm Cor} =
\left[ \begin{array}{rrrrrrrrr}
0.0  ~&amp;     &amp;  \\
3.6 ~ &amp; 0.0 ~ &amp;  \\
0.1 ~ &amp; 3.8 ~ &amp;  0.0 ~\\
\end{array} \right]
\end{eqnarray*}\]</span> % dist4 in R code</p>
<p>By this metric, cases 1 and 3 are the most similar, because the correlation distance is smaller between these two cases than the other pairs of cases.</p>
<p></p>
<p>Note that these interpoint distances differ dramatically from those for Euclidean distance. As a consequence, the way the cases would be clustered is also be very different. Choosing the appropriate distance measure is an important part of a cluster analysis.</p>
<p>After a distance metric has been chosen and a cluster analysis has been performed, the analyst must evaluate the results, and this is actually a difficult task. A cluster analysis does not generate <span class="math inline">\(p\)</span>-values or other numerical criteria, and the process tends to produce hypotheses rather than testing them. Even the most determined attempts to produce the ``best’’ results using modeling and validation techniques may result in clusters that, although seemingly significant, are useless for practical purposes. As a result, cluster analysis is best thought of as an exploratory technique, and it can be quite useful despite the lack of formal validation because of its power in data simplification.</p>
<p>The context in which the data arises is the key to assessing the results. If the clusters can be characterized in a sensible manner, and they increase our knowledge of the data, then we are on the right track. To use an even more pragmatic criterion, if a company can gain an economic advantage by using a particular clustering method to carve up their customer database, then that is the method they should use.</p>
<p>% This paragraph is a bit orphaned here. It should be part of a % longer discussion about assessing results. dfs %We’ve already drawn your attention to the parallel coordinate plot in %Fig.~<span class="math inline">\(\ref{similarity1}\)</span>. It’s a helpful plotting method to use with %cluster analysis, both for exploring the data and for assessing the %results.</p>
<p>% Introduce different clustering techniques here? hierarchical, k-means, % model-based, self-organizing maps.</p>
<p>~</p>
<p> </p>
<p>A purely graphical spin and brush approach to cluster analysis works well when there are good separations between groups, even when there are marked differences in variance structures between groups or when groups have non-linear boundaries. It does not work very well when there are clusters that overlap, or when there are no distinct clusters but rather we simply wish to partition the data. In these situations it may be better to begin with a numerical solution and to use visual tools to evaluate it, perhaps making refinements subsequently. Several examples of the spin and brush approach are documented in the literature, such as and .</p>
<p></p>
<p>This description of the spin and brush approach on , a particle physics dataset, follows that in . The data contains seven variables. We have no labels for the data, so when we begin, all the points have the same color and glyph. Watch the data in a tour for a few minutes and you will see that there are no natural clusters, but there is clearly structure.</p>
<p>%%% Difficulty rating, no separated clusters, low-dimensional %%% structure embedded in high-d….</p>
<p> </p>
<p>We will use the projection pursuit guided tour to help us find that structure. We will tour on the principal components, rather than the raw variables, because that improves the performance of the projection pursuit indexes. Two indexes are useful for detecting clusters: holes and central mass. The holes index is sensitive to projections where there are few points (i.e., a hole) in the center. The central mass index is the opposite: It is sensitive to projections that have too many points in the center. These indexes are explained in Chap.~<span class="math inline">\(\ref{toolbox}\)</span>.</p>
<p>The holes index is usually the most useful for clustering, but not for the particle physics data, because it does not have a ``hole’’ at the center. The central mass index is the most appropriate here. Alternate between optimization (a guided tour) and the unguided grand tour to find local maxima, each of which is a projection that is potentially useful for revealing clusters. The process is illustrated in Fig.~<span class="math inline">\(\ref{prim7-tour}\)</span>.</p>
<p>The top left plot shows the initial default projection, the second principal component plotted against the first. The plot next to it shows the projected data corresponding to the first local maximum found by the guided tour. It has three strands of points stretching out from the central clump and several outliers. We brush the points along each strand, in red, blue, and orange, and we paint the outliers with open circles. (See the next two plots.) We continue by choosing a new random start for the guided tour, and then waiting until new territory in the data is discovered. </p>
<p>The optimization settles on a projection where there are three strands visible, as observed in the leftmost plot in the second row. Two strands have been previously brushed, but a new one has appeared; this is painted yellow.</p>
<p>We also notice that there is another new strand hidden below the red strand. It is barely distinguishable from the red strand in this projection, but the two strands separate widely in other projections. It is tricky to brush it, because it is not well separated in this projection. We use a trick: Hide the red points, brush the new strand green, and ``unhide’’ the red points again (middle plot in the second row).</p>
<p>Five clusters have been easily identified, and now finding new clusters in this data is increasingly difficult. After several more alternations between the grand tour and the guided tour, we find something new (shown in the rightmost plot in the second row): One more strand has emerged, and we paint it pink.</p>
% Figure 3
<p>The results at this stage are summarized by the bottom row of plots. There is a very visible triangular component (in gray) revealed when all of the colored points are hidden. We check the shape of this cluster by drawing lines between outer points to contain the inner ones. Touring after the lines are drawn helps to check how well they match the shape of the clusters. The colored groups pair up at each vertex, and we draw in the shape of these too <sub>—</sub> a single line matches the structures reasonably well.</p>
<p>The final step of the spin and brush clustering is to clean up this solution, touching up the color groups by continuing to tour, and repainting a point here and there. When we finish, we have found seven clusters in this data that form a very strong geometric object in the data space: a two-dimensional (2D) triangle, with two one-dimensional (1D) strands extending in different directions from each vertex. The lines confirm our understanding of this object’s shape, because the points stay close to the lines in all of the projections observed in a tour.</p>
% Figure 4
<p>The next stage of cluster analysis is to characterize the nature of the clusters. To do that, we would calculate summary statistics for each cluster, and plot the clusters (Fig.~<span class="math inline">\(\ref{prim7-model}\)</span>). When we plot the clusters of the particle physics data, we find that the 2D triangle exists primarily in the plane defined by X3 and X5. If you do the same, notice that the variance in measurements for the gray group is large in variables X3 and X5, but negligible in the other variables. The linear pieces can also be characterized by their distributions on each of the variables. With this example, we have shown that it is possible to uncover very unusual clusters in data without any domain knowledge.</p>
Here are several tips about the spin and brush approach.
<p>Finally, the spin and brush method will not work well if there are no clear separations in the data, and the clusters are high-dimensional, unlike the low-dimensional clusters found in this example.</p>
<p>~</p>
<p></p>
<p> </p>
<p>Hierarchical cluster algorithms sequentially fuse neighboring points to form ever-larger clusters, starting from a full interpoint distance matrix. is described by a ``linkage method’’: For example, single linkage uses the smallest interpoint distance between the members of a pair of clusters, complete linkage uses the maximum interpoint distance, and average linkage uses the average of the interpoint distances. A good discussion on cluster analysis can be found in or .</p>
<p>% I changed the text to match the figure. %(Middle row) Clusters 1, 3 and 5 carve up the base %triangle of the data. (Bottom row) Clusters 4 and 6 divide one of the %arms, and cluster 7 is a singleton cluster.</p>
<p>Figure~<span class="math inline">\(\ref{prim7-hier}\)</span> contains several plots that illustrate the results of the hierarchical clustering of the particle physics data; we used Euclidean interpoint distances and the average linkage method. This is computed by:</p>
<p></p>
<p>The dendrogram at the top shows the result of the clustering process. Several large clusters were fused late in the process, with heights (indicated by the height of the horizontal segment connecting two clusters) well above those of the first joins; we will want to look at these. Two points were fused with the rest at the very last stages, which indicates that they are outliers and have been assigned to singleton clusters.</p>
% Figure 5
<p> We cut the dendrogram to produce nine clusters because we would expect to see seven clusters and a few outliers based on our observations from the spin and brush approach, and our choice looks reasonable given the structure of the dendrogram. (In practice, we would usually explore the clusters corresponding to several different cuts of the dendrogram.) We assign each cluster an integer identifier, and in the following plots, you see the results of highlighting one cluster at a time and then running the grand tour to focus on the placement of that cluster within the data. This R code follows this sequence of actions:</p>
<p>The top three plots show, respectively, clusters 1, 2, and 3: These clusters roughly divide the main triangular section of the data into three. The bottom row of plots show clusters labeled 5, and 6, which lie along the linear pieces, and cluster 7, which is a singleton cluster corresponding to an outlier in the data.</p>
<p>The results are reasonably easy to interpret. Recall that the basic geometry underlying this data is that there is a 2D triangle with two linear strands extending from each vertex. The hierarchical average linkage clustering of the particle physics data using nine clusters essentially divides the data into three chunks in the neighborhood of each vertex (clusters 1, 2, and 3), three pieces at the ends of the six linear strands (4, 5, and 6), and three clusters containing outliers (7, 8, and 9). This data provides a big challenge for any cluster algorithm <sub>—</sub> low-dimensional pieces embedded in high-dimensional space <sub>—</sub> and we are not surprised that no algorithm that we have tried will extract the structure we found using interactive tools.</p>
<p>The particle physics dataset is ill-suited to hierarchical clustering, but this extreme failure is an example of a common problem. When performing cluster analysis, we want to group the observations into clusters without knowing the distribution of the data. How many clusters are appropriate? What do the clusters look like? Could we just as confidently divide the data in several different ways and get very different but equally valid interpretations? Graphics can help us assess the results of a cluster analysis by helping us explore the distribution of the data and the characteristics of the clusters.</p>
<p></p>
<p>Model-based clustering fits a multivariate normal mixture model to the data. It uses the EM algorithm to fit the parameters for the mean, variance–covariance of each population, and the mixing proportion. The variance–covariance matrix is re-parametrized using an eigen-decomposition</p>
<p>[ _k = _kD_kA_kD_k’, ~~~k=1, , g ~~ ]</p>
<p>resulting in several model choices, ranging from simple to complex:</p>
<p>Note the distribution descriptions <code>spherical'' and</code>ellipsoidal.’’ These are descriptions of the shape of the variance–covariance for a multivariate normal distribution. A standard multivariate normal distribution has a variance–covariance matrix with zeros in the off-diagonal elements, which corresponds to spherically shaped data. When the variances (diagonals) are different or the variables are correlated, then the shape of data from a multivariate normal is ellipsoidal.</p>
<p></p>
<p></p>
<p>The models are typically scored using the Bayes Information Criterion (BIC), which is based on the log likelihood, number of variables, and number of mixture components. They should also be assessed using graphical methods, as we demonstrate using the data. %To introduce the methods We start with two of the five real-valued variables ( and ) and one (Blue).</p>
<p>The goal is to determine whether model-based methods can discover clusters that will distinguish between the two sexes.</p>
% Figure 6 – too large for the page, but maybe it can just % be reduced in the R code.
<p>Figure~<span class="math inline">\(\ref{model-based1}\)</span> contains the plots we will use to examine the results of model-based clustering on this reduced dataset. The top leftmost plot shows the data, with male and female crabs distinguished by color and glyph. The two sexes correspond to long cigar-shaped objects that overlap a bit, particularly for smaller crabs. The ``cigars’’ are not perfectly regular: The variance of the data is smaller at small values for both sexes, so that our cigars are somewhat wedge-shaped. The orientation of the longest direction of variance differs slightly between groups too: The association has a steeper slope for female crabs than for males, because female crabs have relatively larger than male crabs. With the heterogeneity in variance–covariance, this data does not strictly adhere to the multivariate normal mixture model underlying model-based methods, but we hope that the departure from regularity is not so extreme that it prevents the model from working.</p>
<p>The top right plot shows the BIC results for a full range of models, EEE, EEV, and VVV variance–covariance parametrization for one to nine clusters:</p>
<p>The best model, EEV-2, used the equal volume, equal shape, and different orientation variance–covariance parametrization and divided the data into two clusters. This solution seems to be perfect! We can imagine that this result corresponds to two equally shaped ellipses that intersect near the lowest values of the data and angle toward higher values. We will check by drawing ellipses representing the variance–covariance parametrization on the data plots. The parameter estimates are used to scale and center the ellipses:</p>
<p>yielding the plots in the middle and bottom rows of Fig.~<span class="math inline">\(\ref{model-based1}\)</span>. In the plot of the data alone, cluster id is used for the color and glyph of points. (Compare this plot with the one directly above it, in which the classes are known.) Cluster 1 mostly corresponds to the female crabs, and cluster 2 to the males, except that all the small crabs, both male and female, have been assigned to cluster 1. In the rightmost plot, we have added ellipses representing the estimated variance–covariances. The ellipses are the same shape, as specified by the model, but the ellipse for cluster 2 is shifted toward the large values.</p>
<p>The next two best models, according to the BIC values, are EEV-3 and VVV-2. The plots in the bottom row display representations of the variance–covariances for these models. EEV-3 organizes the crabs into three clusters according to the size, not the sex, of the crabs. The VVV-2 solution is similar to EEV-2.</p>
<p>What solution is the best for this data? If the EEV-3 model had done what we intuitively expected, it would have been ideal: The sexes of smaller crabs are indistinguishable, so they should be afforded their own cluster, whereas larger crabs could be clustered into males and females. However, the cluster that includes the small crabs also includes a fair number of middle-sized female crabs.</p>
<p>Finally, model-based clustering did not discover the true gender clusters. Still, it produced a useful and interpretable clustering of the crabs.</p>
<p>Plots are indispensable for choosing an appropriate cluster model. It is easy to visualize the models when there are only two variables but increasingly difficult as the number of variables grows. Tour methods save us from producing page upon page of plots. They allow us to look at many projections of the data, which enables us to conceptualize the shapes and relationships between clusters in more than two dimensions.</p>
<p>Figure~<span class="math inline">\(\ref{model-based2}\)</span> displays the graphics for the corresponding high-dimensional investigation using all five variables and four classes (two species, two sexes) of the . The cluster analysis is much more difficult now. Can model-based clustering uncover these four groups?</p>
<p>In the top row of plots, we display the raw data, before modeling. Each plot is a tour projection of the data, colored according to the four true classes. The blue and purple points are the male and female crabs of the blue species, and the yellow and orange points are the male and female crabs of the orange species. This table will help you keep track:</p>
<p>The clusters corresponding to the classes are long thin wedges in five dimensions (5D), with more separation and more variability at larger values, as we saw in the subset just discussed. The rightmost plot shows the ``looking down the barrel’’ view of the wedges. At small values the points corresponding to the sexes are mixed (leftmost plot). The species are reasonably well separated even for small crabs (middle plot). The variance–covariance is wedge-shaped rather than elliptical, but again we hope that modeling based on the normal distribution that has elliptical variance–covariance will be adequate.</p>
% Figure 7
<p>In the results from model-based clustering, there is very little difference in BIC value for variance–covariance models EEE, EEV, VEV, and VVV, with a number of clusters from three to eight. The best model is EEV-3, and EEV-4 is second best. We know that three clusters is insufficient to capture the four classes we have in mind, so we examine the four-cluster solution.</p>
<p>The bottom row of plots in Fig.~<span class="math inline">\(\ref{model-based2}\)</span> illustrates the four-cluster model in three different projections, matching the projections in the top row showing the data.</p>
<p>In each view, the ellipsoids representing the variance–covariance estimates for the four clusters are shown in four shades of gray, because none of these match any actual cluster in the data. Remember that these are 2D projections of 5D ellipsoids. The resulting clusters from the model do not match the true classes very well. The result roughly captures the two species, as we see in the plots in the first column, where the species are separated both in the data and in the ellipses. On the other hand, the grouping corresponding to is completely missed: See the plots in the middle and right-hand columns, where sexes are separated in the actual data but the ellipses are not separated. Just as in the smaller subset (two variables, one species) discussed earlier, there is a cluster for the smaller crabs of both species and sexes. The results of model-based clustering on the full 5D data are very unsatisfactory.</p>
<p>In summary, plots of the data and parameter estimates for model-based cluster analysis are very useful for understanding the solution, and choosing an appropriate model. Tours are very helpful for examining the results in higher dimensions, for arbitrary numbers of variables.</p>
<p>% Also, using principal components rather than the raw variables…</p>
<p></p>
<p>A self-organizing map (SOM) is constructed using a constrained <span class="math inline">\(k\)</span>-means algorithm. A 1D or 2D net is stretched through the data. The knots, in the net, form the cluster means, and the points closest to the knot are considered to belong to that cluster. The similarity of nodes (and their corresponding clusters) is defined as proportional to their distance from one another on the net.</p>
<p>We will demonstrate SOM using the music data. The data has 62 cases, each one corresponding to a piece of music. For each piece there are seven variables: the artist, the type of music, and five characteristics, based on amplitude and frequency, that were computed using the first 40 seconds of the piece on CD. The music used included popular rock songs by Abba, the Beatles, and the Eels; classical compositions by Vivaldi, Mozart and Beethoven; and several new wave pieces by Enya. Figure~<span class="math inline">\(\ref{music-som}\)</span> displays a typical view of the results of clustering using SOM on the music data. Each data point corresponds to a piece of music and is labeled by the band or the composer. The map was generated by this R code:</p>
<p>The left plot in Fig.~<span class="math inline">\(\ref{music-som}\)</span> is called the 2D map view. Here we have used a <span class="math inline">\(6\times 6\)</span> net pulled through the 5D data. The net that was wrapped through the high-dimensional space is straightened and laid out flat, and the points, like fish in a fishing net, are laid out where they have been trapped. In the plot shown here, the points have been jittered slightly, away from the knots of the net, so that the labels do not overlap too much. If the fit is good, the points that are close together in this 2D map view are close together in the high-dimensional data space and close to the net as it was placed in the high-dimensional space.</p>
<p>Much of the structure in the map is no surprise: The rock (purple) and classical tracks (green) are on opposing corners, with rock in the upper right and classical in the lower left. The Abba tracks are all grouped at the top and left of the map. The Beatles and Eels tracks are mixed. There are some unexpected associations: For example, one Beatles song, which turns out to be ``Hey Jude,’’ is mixed among the classical compositions!</p>
% Figure 8
<p>Construction of a self-organizing map is a dimension reduction method, akin to multidimensional scaling or principal component analysis . Using principal component analysis to find a low-dimensional approximation of the similarity between music pieces, yields the second plot in Fig.~<span class="math inline">\(\ref{music-som}\)</span>. There are many differences between the two representations. The SOM has a more even spread of music pieces across the grid, in contrast to the stronger clumping of points in the PCA view. Indeed, the PCA view shows several outliers, notably one of the Vivaldi compositions, which could lead us to learn things about the data that we might miss by relying exclusively on the SOM. </p>
<p>These two methods, SOM and PCA, have provided two contradictory clustering models. How can we determine which is the more accurate description of the data structure? An important part of model assessment is plotting the model in relation to the data. Although plotting the low-dimensional map is the common way to graphically assess the SOM results, it is woefully limited. If a model has flaws, they may not show up in this view and will only appear in plots of the model in the data space. We will use the grand tour to create these plots, and this will help us assess the two models.</p>
<p>%Although the reduced dimension view is the common way to graphically %assess the SOM results, it is woefully limited. What might appear to %be an appealing result when seen in the map view may in fact be a poor %fit, but we may not realize that without looking at plots of the model %in the data space.</p>
<p>We will use a grand tour to view the net wrapped in among the data, hoping to learn how the net converged to this solution, and how it wrapped through the data space. Actually, it is rather tricky to fit a SOM: Like many algorithms, it has a number of parameters and initialization conditions that affect the outcome.</p>
<p>To set up the data, we will need to add variables containing the map coordinates to the data:</p>
<p>Because this data has several useful categorical labels for each row, we will want to keep this information in the data when it is loaded into GGobi:</p>
<p>Add the edges that form the SOM net:</p>
<p>And finally color the points according to the type of music:</p>
<p>The results can be seen in Figs.~<span class="math inline">\(\ref{clust-SOMa}\)</span> and <span class="math inline">\(\ref{clust-SOMb}\)</span>. The plots show two different states of the fitting process and of the SOM net cast through the data. In both fits, a <span class="math inline">\(6\times 6\)</span> grid is used and the net is initialized in the direction of the first two principal components. In both fits the variables were standardized to have mean equal to zero and standard deviation equal to 1. The first SOM fit, shown in <span class="math inline">\(\ref{clust-SOMa}\)</span>, was obtained using the default settings; it gave terrible results. At the left is the map view, in which the fit looks deceptively reasonable. The points are spread evenly through the grid, with rock tracks (purple) at the upper right, classical tracks (green) at the lower left, and new wave tracks (the three black rectangles) in between. The tour view in the same figure, however, shows the fit to be inadequate. The net is a flat rectangle in the 5D space and has not sufficiently wrapped through the data. This is the result of stopping the algorithm too soon, thus failing to let it converge fully.</p>
% Figure 9
% Figure 10
<p>Figure <span class="math inline">\(\ref{clust-SOMb}\)</span> shows our favorite fit to the data. The data was standardized, we used a 6$$6 net, and we ran the SOM algorithm for 1,000 iterations. The map is at the top left, and it matches the map already shown in Fig.~<span class="math inline">\(\ref{music-som}\)</span>, except for the small jittering of points in the earlier figure. The other three plots show different projections from the grand tour. The upper right plot shows how the net curves with the nonlinear dependency in the data: The net is warped in some directions to fit the variance pattern. At the bottom right we see that one side of the net collects a long separated cluster of the Abba tracks. We can also see that the net has not been stretched out to the full extent of the range of the data. It is tempting to manually manipulate the net to stretch it in different directions and update the fit.</p>
<p>It turns out that the PCA view of the data more accurately reflects the structure in the data than the map view. The music pieces really are clumped together in the 5D space, and there are a few outliers. </p>
<p> </p>
<p>To compare the results of two methods we commonly compute a confusion table. For example, Table <span class="math inline">\(\ref{confusion}\)</span> is the confusion table for five-cluster solutions for the data from <span class="math inline">\(k\)</span>-means and Ward’s linkage hierarchical clustering, generated by:</p>
<p>The numerical labels of clusters are arbitrary, so these can be rearranged to better digest the table. There is a lot of agreement between the two methods: Both methods agree on the cluster for 48 tracks out of 62, or 77% of the time. We want to explore the data space to see where the agreement occurs and where the two methods disagree.</p>
% Figure 11
<p></p>
<p>In Fig.~<span class="math inline">\(\ref{clust-compare}\)</span>, we link jittered plots of the confusion table for the two clustering methods with 2D tour plots of the data. The first column contains two jittered plots of the confusion table. In the top row of the figure, we have highlighted a group of 14 points that both methods agree form a cluster, painting them as orange triangles. From the plot at the right, we see that this cluster is a closely grouped set of points in the data space. From the tour axes we see that has the largest axis pointing in the direction of the cluster separation, which suggests that music pieces in this cluster are characterized by high values on (variable 3 in the data); that is, they have large variance in frequency. By further investigating which tracks are in this cluster, we can learn that it consists of a mix of tracks by the Beatles (<code>Penny Lane,''</code>Help,’’ <code>Yellow Submarine,'' ...)  and the Eels (</code>Saturday Morning,’’ ``Love of the Loveless,’’ …).</p>
<p>In the bottom row of the figure, we have highlighted a second group of tracks that were clustered together by both methods, painting them again using orange triangles. In the plot to the right, we see that this cluster is closely grouped in the data space. Despite that, this cluster is a bit more difficult to characterize. It is oriented mostly in the negative direction of (variable 4), so it would have smaller values on this variable. But this vertical direction in the plot also has large contributions from variables 3 () and 7 (). If you label these eight points on your own, you will see that they are all Abba songs (<code>Dancing Queen,''</code>Waterloo,’’ ``Mamma Mia,’’ …). % slightly reworded this last sentence to make it clear that this % is not shown in any figure but still state the result. dfs</p>
<p>We have explored two groups of tracks where the methods agree. In a similar fashion, we could also explore the tracks where the methods disagree.</p>

<p>The final step in a cluster analysis is to characterize the clusters. Actually, we have engaged in cluster characterization throughout the examples, because it is an intrinsic part of assessing the results of any cluster analysis. If we cannot detect any numerical or qualitative differences between clusters, then our analysis was not successful, and we start over with a different distance metric or algorithm.</p>
<p>However, once we are satisfied that we have found a set of clusters that can be differentiated from one another, we want to describe them more formally, both quantitatively and qualitatively. We characterize them quantitatively by computing such statistics as cluster means and standard deviations for each variable. We can look at these results in tables and in plots, and we can refine the qualitative descriptions of the clusters we made during the assessment process.</p>
<p></p>
<p>The parallel coordinate plot is often used during this stage. Figure~<span class="math inline">\(\ref{clust-char}\)</span> shows the parallel coordinate plot for the first of the clusters of music pieces singled out for study in the previous section. Ward’s hierarchical linkage and <span class="math inline">\(k\)</span>-means both agreed that these music pieces form a cluster. Since the matrix and the number of clusters are both small, we plot the raw data; for larger problems, we might plot cluster statistics as well [see, for example, ].</p>
% figure 12
<p>This cluster containing a mix of Beatles and Eels music has high values on , medium values of , high values of , high values of , and varied values. That is, these pieces of music have a large variance in frequency, high frequency, and high energy relative to the other music pieces.</p>
%
<p>~</p>
<p>% Generating clusters % Interpreting clusters % Model assessment % confusion tables % SOM – grid % model-based – compare ellipse to shape</p>
<p>Graphics are invaluable for cluster analysis, whether they are used to find clusters or to interpret and evaluate the results of a cluster analysis arrived at by other means.</p>
<p>The spin and brush approach can be used to get an initial look at the data and to find clusters, and occasionally, it is sufficient. When the clustering is the result of an algorithm, a very useful first step is to paint the points by cluster membership and to look at the data to see whether the clustering seems sensible. How many clusters are there, and how big are they? What shape are they, and do they overlap one another? Which variables have contributed most to the clustering? Can the clusters be qualitatively described? All the plots we have described can be useful: scatterplots, parallel coordinate plots, and area plots, as well as static plots like dendrograms.</p>
<p>When the clusters have been generated by a model, we should also use graphics to help us assess the model. If the model makes distributional assumptions, we can generate ellipses and compare them with the clusters to see whether the shapes are consistent. For self-organizing maps the tour can assist in uncovering problems with the fit, such as when the map wraps in on itself through the data making it appear that some cases are far apart when they are truly close together. A confusion table can come alive with linked brushing, so that mismatches and agreements between methods can be explored. </p>
<p>% Euclidean distance, vs standardizing each row… % Principal component coordinates vs raw variables.</p>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./unsupervised.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Unsupervised learning</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./kmeans-clustering.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">k-means clustering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>