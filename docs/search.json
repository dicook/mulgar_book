[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interactive and dynamic graphics for high-dimensional data using R",
    "section": "",
    "text": "Although there are many resources available for data visualization, there are few comprehensive resources on high-dimensional data visualisation. This book fills this gap by providing a comprehensive and up-to-date guide to visualising high-dimensional data and models, with R.\nHigh-dimensional data spaces are fascinating places. You may think that there’s a lot of ways to plot one or two variables, and a lot of types of patterns that can be found. You might use a density plot and see skewness or a dot plot to find outliers. A scatterplot of two variables might reveal a non-linear relationship or a barrier beyond which no observations exist. We don’t as yet have so many different choices of plot types for high-dimensions, but these types of patterns are also what we seek in scatterplots of high-dimensional data. The additional dimensions can clarify these patterns, that clusters are likely to be more distinct. Observations that did not appear to be very different can be seen to be lonely anomalies in high-dimensions, that no other observations have quite the same combination of values.\nIf you encounter an error, you can report it as an issue at the Github repo for this book.\nPlease make a small reproducible example and report the error encountered. Reproducible examples have these components:"
  },
  {
    "objectID": "index.html#audience",
    "href": "index.html#audience",
    "title": "Interactive and dynamic graphics for high-dimensional data using R",
    "section": "Audience",
    "text": "Audience\nHigh-dimensional data arises in many fields such as biology, social sciences, finance, and more. Anyone who is doing exploratory data analysis and model fitting for more than two variables will benefit from learning how to effectively visualise high-dimensions. This book will be useful for students and teachers of mulitvariate data analysis and machine learning, and researchers, data analysts, and industry professionals who work in these areas."
  },
  {
    "objectID": "index.html#how-to-use-the-book",
    "href": "index.html#how-to-use-the-book",
    "title": "Interactive and dynamic graphics for high-dimensional data using R",
    "section": "How to use the book?",
    "text": "How to use the book?\nThe book is written with explanations followed by examples with R code. The toolbox chapter provides an overview of the primary high-dimensional visualisation methods. The remaining chapters focus on different application areas and how to use the high-dimensional visualisation to complement commonly used analytical methods.\n\nWhat should I know before reading this book?\nThe examples assume that you already use R, and have a working knowledge of base R and tidyverse way of thinking about data analysis. It also assumes that you have some knowledge of statistical methods, and some experience with machine learning methods.\nIf you feel like you need build up your skills in these areas in preparation for working through this book, these are our recommended resources:\n\nR for Data Science by Wickham and Grolemund for learning about tidyverse.\nIntroduction to Modern Statistics by Çetinkaya-Rundel and Hardin to learn about introductory statistics.\nHands-On Machine Learning with R by Boehmke and Greenwell to learn about machine learning."
  },
  {
    "objectID": "index.html#setting-up-your-workflow",
    "href": "index.html#setting-up-your-workflow",
    "title": "Interactive and dynamic graphics for high-dimensional data using R",
    "section": "Setting up your workflow",
    "text": "Setting up your workflow\nTo get started set up your computer with the current versions of R and Rstudio Desktop.\nIn addition, we have made an R package to share the data and functions used in this book, called mulgar.12\n\ninstall.packages(\"mulgar\")\n#| or the development version\ndevtools::install_github(\"dicook/mulgar\")"
  },
  {
    "objectID": "data.html#australian-football-league-women",
    "href": "data.html#australian-football-league-women",
    "title": "Appendix B — Data",
    "section": "B.1 Australian Football League Women",
    "text": "B.1 Australian Football League Women\n\nDescription\nThis is data from the 2021 Women’s Australian Football League. These are average player statistics across the season, with game statistics provided by the fitzRoy package. If you are new to the game of AFL, there is a nice explanation on Wikipedia.\n\n\nVariables\n\n\n\n\n\nRows: 381\nColumns: 35\n$ id              <chr> \"CD_I1001678\", \"CD_I1001679\", \"CD_I1001681\", \"CD_I1001…\n$ given_name      <chr> \"Jordan\", \"Brianna\", \"Jodie\", \"Ebony\", \"Emma\", \"Pepa\",…\n$ surname         <chr> \"Zanchetta\", \"Green\", \"Hicks\", \"Antonio\", \"King\", \"Ran…\n$ number          <int> 2, 3, 5, 12, 60, 21, 22, 23, 35, 14, 3, 8, 16, 12, 19,…\n$ team            <chr> \"Brisbane Lions\", \"West Coast Eagles\", \"GWS Giants\", \"…\n$ position        <chr> \"INT\", \"INT\", \"HFFR\", \"WL\", \"RK\", \"BPL\", \"INT\", \"INT\",…\n$ time_pct        <dbl> 63.00000, 61.25000, 76.50000, 74.90000, 85.10000, 77.4…\n$ goals           <dbl> 0.0000000, 0.0000000, 0.0000000, 0.1000000, 0.6000000,…\n$ behinds         <dbl> 0.0000000, 0.0000000, 0.5000000, 0.4000000, 0.4000000,…\n$ kicks           <dbl> 5.000000, 2.500000, 3.750000, 8.800000, 4.100000, 3.22…\n$ handballs       <dbl> 2.500000, 3.750000, 3.000000, 3.600000, 2.700000, 2.22…\n$ disposals       <dbl> 7.500000, 6.250000, 6.750000, 12.400000, 6.800000, 5.4…\n$ marks           <dbl> 1.5000000, 0.2500000, 1.0000000, 3.7000000, 2.2000000,…\n$ bounces         <dbl> 0.0000000, 0.0000000, 0.0000000, 0.6000000, 0.1000000,…\n$ tackles         <dbl> 3.000000, 2.250000, 2.250000, 3.900000, 2.000000, 1.77…\n$ contested       <dbl> 3.500000, 2.250000, 3.500000, 5.700000, 4.400000, 2.66…\n$ uncontested     <dbl> 3.500000, 4.500000, 3.000000, 7.000000, 2.800000, 1.77…\n$ possessions     <dbl> 7.000000, 6.750000, 6.500000, 12.700000, 7.200000, 4.4…\n$ marks_in50      <dbl> 1.0000000, 0.0000000, 0.2500000, 0.5000000, 0.9000000,…\n$ contested_marks <dbl> 1.0000000, 0.0000000, 0.0000000, 0.4000000, 1.2000000,…\n$ hitouts         <dbl> 0.0000000, 0.0000000, 0.0000000, 0.0000000, 19.4000000…\n$ one_pct         <dbl> 0.0000000, 1.5000000, 0.5000000, 1.2000000, 2.6000000,…\n$ disposal        <dbl> 60.25000, 67.15000, 37.20000, 65.96000, 61.72000, 66.8…\n$ clangers        <dbl> 2.000000, 0.500000, 2.500000, 3.100000, 2.400000, 1.33…\n$ frees_for       <dbl> 1.0000000, 0.5000000, 0.2500000, 2.5000000, 0.5000000,…\n$ frees_against   <dbl> 1.0000000, 0.5000000, 1.2500000, 1.3000000, 1.1000000,…\n$ rebounds_in50   <dbl> 0.0000000, 0.5000000, 0.2500000, 1.1000000, 0.0000000,…\n$ assists         <dbl> 0.00000000, 0.00000000, 0.00000000, 0.20000000, 0.2000…\n$ accuracy        <dbl> 0.00000, 0.00000, 0.00000, 5.00000, 30.00000, 0.00000,…\n$ turnovers       <dbl> 1.500000, 1.000000, 2.500000, 4.000000, 1.700000, 1.22…\n$ intercepts      <dbl> 2.0000000, 2.0000000, 0.5000000, 5.3000000, 1.3000000,…\n$ tackles_in50    <dbl> 0.5000000, 0.0000000, 0.7500000, 0.5000000, 0.5000000,…\n$ shots           <dbl> 0.5000000, 0.0000000, 0.7500000, 1.0000000, 1.2000000,…\n$ metres          <dbl> 72.50000, 58.50000, 76.00000, 225.90000, 89.80000, 76.…\n$ clearances      <dbl> 0.5000000, 0.2500000, 1.2500000, 0.4000000, 0.9000000,…\n\n\n\n\nPurpose\nThe primary analysis is to summarise the variation using principal component analysis, which gives information about relationships between the statistics or skills sets common in players. One also might be tempted to cluster the players, but there are no obvious clusters so it could be frustrating. At best one could partition the players into groups, while recognising there are no absolutely distinct and separated groups.\n\n\nSource\nSee the information provided with the fitzRoy package.\n\n\nPre-processing\nThe code for downloading and pre-processing the data is available at the mulgar website in the data-raw folder. The data provided by the fitzRoy package was pre-processed to reduce the variables to only those that relate to player skills and performance. It is possible that using some transformations on the variables would be useful to make them less skewed."
  },
  {
    "objectID": "data.html#notes-to-self",
    "href": "data.html#notes-to-self",
    "title": "Appendix B — Data",
    "section": "B.4 Notes to self",
    "text": "B.4 Notes to self\nThere were fifteen datasets listed in chapter 7 of the first edition. Several of these were related to networks which we are not including this time. I have tried to give a mix of things on a variety of topics. It would be nice to see if there’s updated versions of the “tips” (there seems to be a lot of noise in the literature here and no open datasets) and Di’s music data (perhaps we could scrape our own spotify accounts to get an equivalent), there are also a few audio challenge datasets like FSD50K . I think it would be useful to have more unstructured data sets like natural text that we have used for 1010.\n\nOther possible sources for data\nThere are now many search engines available for datasets that originate from research contexts that list licensing information and DOIs:\n\nhttps://zenodo.org (mostly ecology/biology)\nhttps://datadryad.org/stash (mostly biology)\nhttps://dataverse.harvard.edu/dataverse/harvard/ (mostly social sciences, but has a mixture of things)\nThere’s also a big list of datasets here: https://docs.google.com/spreadsheets/d/1ejOJTNTL5ApCuGTUciV0REEEAqvhI2Rd2FCoj7afops/edit#gid=0 (all psychology related)\nTidyTuesday\n\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md Would need to rearrange data to look at count, participation, revenue, expenditure\n\ngapminder\naccounting records\nLyn’s ecology data\nlearningtower, yowie\n\n\n\n\n\nAustralian Bureau of Agricultural and Resource Economics and Sciences. 2018. “Forests of Australia.” https://www.agriculture.gov.au/abares/forestsaustralia/forest-data-maps-and-tools/spatial-data/forest-cover.\n\n\nDepartment of Environment, Land, Water & Planning. 2019. “Fire Origins - Current and Historical.” https://discover.data.vic.gov.au/dataset/fire-origins-current-and-historical.\n\n\n———. 2020a. “CFA - Fire Station.” https://discover.data.vic.gov.au/dataset/cfa-fire-station-vmfeat-geomark_point.\n\n\n———. 2020b. “Recreation Sites.” https://discover.data.vic.gov.au/dataset/recreation-sites.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nIowa State University. 2020. “ASOS-AWOS-METAR Data Download.” https://mesonet.agron.iastate.edu/request/download.phtml?network=AU__ASOS.\n\n\nMcVicar, Tim. 2011. “Near-Surface Wind Speed. V10. CSIRO. Data Collection.” https://doi.org/10.25919/5c5106acbcb02.\n\n\nOpenStreetMap contributors. 2020. “Planet dump retrieved from https://planet.osm.org .” https://www.openstreetmap.org.\n\n\nP-Tree System. 2020. “JAXA Himawari Monitor - User’s Guide.” https://www.eorc.jaxa.jp/ptree/userguide.html.\n\n\nSparks, Adam H., Jonathan Carroll, James Goldie, Dean Marchiori, Paul Melloy, Mark Padgham, Hugh Parsonage, and Keith Pembleton. 2020. bomrang: Australian Government Bureau of Meteorology (BOM) Data Client. https://CRAN.R-project.org/package=bomrang."
  },
  {
    "objectID": "hierarchical-clustering.html",
    "href": "hierarchical-clustering.html",
    "title": "8  Hierarchical clustering",
    "section": "",
    "text": "Hierarchical cluster algorithms sequentially fuse neighboring points to form ever-larger clusters, starting from a full interpoint distance matrix. Figure 8.1 illustrates the hierarchical clustering approach for a simple simulated data set (a) with two well-separated clusters in 2D. The dendrogram (b) is a representation of the order that points are joined into clusters. The dendrogram strongly indicates two clusters because there are two branches branches representing the last join are much longer than all of the other branches. Although, the dendrogram is usually a good summary of the steps taken by the algorithm, it can be misleading. The dendrogram might strongly suggest a clustering but it might be a terrible solution. To check this we need to show the model with the data, as shown in plot (c). The segments show how the points and clusters are joined. Note that once points are joined into a cluster, the centroid of that cluster is used as the join location with other points or other clusters, and this is represented by a “+”. We can see that the longest edge is the one stretching across the gap between the two clusters, which is the location where the dendrogram would be cut to produce the two-cluster solution. This two-cluster solution is shown in plot (d).\n\n\nCode\nlibrary(ggplot2)\nlibrary(mulgar)\ndata(simple_clusters)\n# Data has two well-separated clusters\npd <- ggplot(simple_clusters, aes(x=x1, y=x2)) +\n  geom_point(colour=\"orange\", size=2, alpha=0.8) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio=1) \n\n# Compute hierarchical clustering with Ward's linkage\ncl_hw <- hclust(dist(simple_clusters[,1:2]),\n                method=\"ward.D2\")\nlibrary(ggdendro)\ncl_ggd <- dendro_data(cl_hw, type = \"triangle\")\nph <- ggplot() +\n  geom_segment(data=cl_ggd$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=cl_ggd$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(b)\") +\n  theme_dendro()\n\n# Compute dendrogram in data\ncl_hfly <- hierfly(simple_clusters, cl_hw, scale=FALSE)\n\npdh <- ggplot() +\n  geom_segment(data=cl_hfly$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=cl_hfly$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(c)\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nlibrary(dplyr)\nsimple_clusters <- simple_clusters %>%\n  mutate(clw = factor(cutree(cl_hw, 2)))\npc <- ggplot(simple_clusters) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_brewer(palette=\"Set1\") +\n  ggtitle(\"(d)\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\nlibrary(patchwork)\npd + ph + pdh + pc + plot_layout(ncol=2)\n\n\n\n\n\nFigure 8.1: Hierarchical clustering on simulated data: (a) data, (b) dendrogram, (c) dendrogram on the data, and (d) two cluster solution. Nodes of the dendrogram indicated by + when it is drawn on the data.\n\n\n\n\nDistance between clusters is described by a “linkage method”: For example, single linkage uses the smallest interpoint distance between the members of a pair of clusters, complete linkage uses the maximum interpoint distance, and average linkage uses the average of the interpoint distances. A good discussion on cluster analysis can be found in Boehmke and Greenwell (2019), on Wikipedia or any multivariate textbook.\n–THIS STUFF IS THE OLD MATERIAL–\n?fig-prim7-hier contains several plots that illustrate the results of the hierarchical clustering of the particle physics data; we used Euclidean interpoint distances and the average linkage method. This is computed by:\n> library(rggobi)\n> d.prim7 <- read.csv(\"prim7.csv\")\n> d.prim7.dist <- dist(d.prim7)\n> d.prim7.dend <- hclust(d.prim7.dist, method=\"average\")\n> plot(d.prim7.dend)\n\nThe dendrogram at the top shows the result of the clustering process. Several large clusters were fused late in the process, with heights (indicated by the height of the horizontal segment connecting two clusters) well above those of the first joins; we will want to look at these. Two points were fused with the rest at the very last stages, which indicates that they are outliers and have been assigned to singleton clusters.\n\n We cut the dendrogram to produce nine clusters because we would expect to see seven clusters and a few outliers based on our observations from the spin and brush approach, and our choice looks reasonable given the structure of the dendrogram. (In practice, we would usually explore the clusters corresponding to several different cuts of the dendrogram.) We assign each cluster an integer identifier, and in the following plots, you see the results of highlighting one cluster at a time and then running the grand tour to focus on the placement of that cluster within the data. This R code follows this sequence of actions:\n> gd <- ggobi(d.prim7)[1]\n> clust9 <- cutree(d.prim7.dend, k=9)\n> glyph_color(gd)[clust9==1] <- 9 # highlight triangle\n> glyph_color(gd)[clust9==1] <- 1 # reset color\n> glyph_color(gd)[clust9==2] <- 9 # highlight cluster 2\nThe top three plots show, respectively, clusters 1, 2, and 3: These clusters roughly divide the main triangular section of the data into three. The bottom row of plots show clusters labeled 5, and 6, which lie along the linear pieces, and cluster 7, which is a singleton cluster corresponding to an outlier in the data.\nThe results are reasonably easy to interpret. Recall that the basic geometry underlying this data is that there is a 2D triangle with two linear strands extending from each vertex. The hierarchical average linkage clustering of the particle physics data using nine clusters essentially divides the data into three chunks in the neighborhood of each vertex (clusters 1, 2, and 3), three pieces at the ends of the six linear strands (4, 5, and 6), and three clusters containing outliers (7, 8, and 9). This data provides a big challenge for any cluster algorithm - low-dimensional pieces embedded in high-dimensional space - and we are not surprised that no algorithm that we have tried will extract the structure we found using interactive tools.\nThe particle physics dataset is ill-suited to hierarchical clustering, but this extreme failure is an example of a common problem. When performing cluster analysis, we want to group the observations into clusters without knowing the distribution of the data. How many clusters are appropriate? What do the clusters look like? Could we just as confidently divide the data in several different ways and get very different but equally valid interpretations? Graphics can help us assess the results of a cluster analysis by helping us explore the distribution of the data and the characteristics of the clusters.\n\n\n\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning with r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377."
  },
  {
    "objectID": "hierarchical-clustering.html#overview",
    "href": "hierarchical-clustering.html#overview",
    "title": "8  Hierarchical clustering",
    "section": "8.1 Overview",
    "text": "8.1 Overview\nHierarchical cluster algorithms sequentially fuse neighboring points to form ever-larger clusters, starting from a full interpoint distance matrix. Figure 8.1 illustrates the hierarchical clustering approach for a simple simulated data set (a) with two well-separated clusters in 2D. The dendrogram (b) is a representation of the order that points are joined into clusters. The dendrogram strongly indicates two clusters because there are two branches branches representing the last join are much longer than all of the other branches. Although, the dendrogram is usually a good summary of the steps taken by the algorithm, it can be misleading. The dendrogram might strongly suggest a clustering but it might be a terrible solution. To check this we need to show the model with the data, as shown in plot (c). The segments show how the points and clusters are joined. Note that once points are joined into a cluster, the centroid of that cluster is used as the join location with other points or other clusters, and this is represented by a “+”. We can see that the longest edge is the one stretching across the gap between the two clusters, which is the location where the dendrogram would be cut to produce the two-cluster solution. This two-cluster solution is shown in plot (d).\n\n\nCode\nlibrary(ggplot2)\nlibrary(mulgar)\nlibrary(ggdendro)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(palmerpenguins)\nlibrary(tourr)\nlibrary(plotly)\nlibrary(htmlwidgets)\n\n\n\n\nCode\ndata(simple_clusters)\n# Data has two well-separated clusters\npd <- ggplot(simple_clusters, aes(x=x1, y=x2)) +\n  geom_point(colour=\"orange\", size=2, alpha=0.8) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio=1) \n\n# Compute hierarchical clustering with Ward's linkage\ncl_hw <- hclust(dist(simple_clusters[,1:2]),\n                method=\"ward.D2\")\ncl_ggd <- dendro_data(cl_hw, type = \"triangle\")\nph <- ggplot() +\n  geom_segment(data=cl_ggd$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=cl_ggd$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(b)\") +\n  theme_dendro()\n\n# Compute dendrogram in data\ncl_hfly <- hierfly(simple_clusters, cl_hw, scale=FALSE)\n\npdh <- ggplot() +\n  geom_segment(data=cl_hfly$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=cl_hfly$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(c)\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nsimple_clusters <- simple_clusters %>%\n  mutate(clw = factor(cutree(cl_hw, 2)))\npc <- ggplot(simple_clusters) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_brewer(palette=\"Set1\") +\n  ggtitle(\"(d)\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd + ph + pdh + pc + plot_layout(ncol=2)\n\n\n\n\n\nFigure 8.1: Hierarchical clustering on simulated data: (a) data, (b) dendrogram, (c) dendrogram on the data, and (d) two cluster solution. Nodes of the dendrogram indicated by + when it is drawn on the data.\n\n\n\n\nClustering algorithms are all prone to being confused by different problems occurring in data. For hierarchical clustering, plotting the dendrogram on the data provides another way to assess the solution. For hierarchical clustering additional the complications arise from a range of choices for defining distance once points have been joined into clusters.\nDistance between clusters is described by a “linkage method”, of which there are many. For example, single linkage measures the distance between clusters by the smallest interpoint distance between the members of the two clusters clusters, complete linkage uses the maximum interpoint distance, and average linkage uses the average of the interpoint distances. Wards linkage, which usually produces the best clustering solutions, defines the distance as the reduction in the within-group variance. A good discussion on cluster analysis and linkage can be found in Boehmke and Greenwell (2019), on Wikipedia or any multivariate textbook."
  },
  {
    "objectID": "hierarchical-clustering.html#what-can-go-wrong",
    "href": "hierarchical-clustering.html#what-can-go-wrong",
    "title": "8  Hierarchical clustering",
    "section": "8.2 What can go wrong",
    "text": "8.2 What can go wrong\n\n\nCode\n# Nuisance observations\nset.seed(20190514)\nx <- (runif(20)-0.5)*4\ny <- x\nd1 <- data.frame(x1 = c(rnorm(50, -3), \n                            rnorm(50, 3), x),\n                 x2 = c(rnorm(50, -3), \n                            rnorm(50, 3), y),\n                 cl = factor(c(rep(\"A\", 50), \n                             rep(\"B\", 70))))\nd1 <- d1 %>% \n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\npd1 <- ggplot(data=d1, aes(x=x1, y=x2)) + \n  geom_point() +\n    ggtitle(\"Nuisance observations\") +\n    theme(aspect.ratio=1) \n\n# Nuisance variables\nset.seed(20190512)\nd2 <- data.frame(x1=c(rnorm(50, -4), \n                            rnorm(50, 4)),\n                 x2=c(rnorm(100)),\n                 cl = factor(c(rep(\"A\", 50), \n                             rep(\"B\", 50))))\nd2 <- d2 %>% \n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\npd2 <- ggplot(data=d2, aes(x=x1, y=x2)) + \n  geom_point() +\n    ggtitle(\"Nuisance variables\") +\n    theme(aspect.ratio=1)\n\npd1 + pd2 + plot_layout(ncol=2)\n\n\n\n\n\nFigure 8.2: Two examples of data structure that causes problems for hierarchical clustering. Nuisance observations can cause problems because the close observations between the two clusters can cause some chaining in the hierarchical joining of observations. Nuisance variables can cause problems because observations across the gap can seem closer than observations at the end of each cluster.\n\n\n\n\n\n\nCode\n# Compute single linkage\nd1_hs <- hclust(dist(d1[,1:2]),\n                method=\"single\")\nd1_ggds <- dendro_data(d1_hs, type = \"triangle\")\npd1s <- ggplot() +\n  geom_segment(data=d1_ggds$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d1_ggds$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(a) Single linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd1_hflys <- hierfly(d1, d1_hs, scale=FALSE)\n\npd1hs <- ggplot() +\n  geom_segment(data=d1_hflys$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d1_hflys$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(b) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd1 <- d1 %>%\n  mutate(cls = factor(cutree(d1_hs, 2)))\npc_d1s <- ggplot(d1) +\n  geom_point(aes(x=x1, y=x2, colour=cls), \n             size=2, alpha=0.8) +\n  scale_colour_brewer(palette=\"Set1\") +\n  ggtitle(\"(c) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Compute Wards linkage\nd1_hw <- hclust(dist(d1[,1:2]),\n                method=\"ward.D2\")\nd1_ggdw <- dendro_data(d1_hw, type = \"triangle\")\npd1w <- ggplot() +\n  geom_segment(data=d1_ggdw$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d1_ggdw$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(d) Ward's linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd1_hflyw <- hierfly(d1, d1_hw, scale=FALSE)\n\npd1hw <- ggplot() +\n  geom_segment(data=d1_hflyw$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d1_hflyw$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(e) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd1 <- d1 %>%\n  mutate(clw = factor(cutree(d1_hw, 2)))\npc_d1w <- ggplot(d1) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_brewer(palette=\"Set1\") +\n  ggtitle(\"(f) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd1s + pd1hs + pc_d1s + \n  pd1w + pd1hw + pc_d1w +\n  plot_layout(ncol=3)\n\n\n\n\n\nFigure 8.3: Single linkage clustering on nuisance cases in comparison to Ward’s linkage.\n\n\n\n\n\n\nCode\n# Compute complete linkage\nd2_hc <- hclust(dist(d2[,1:2]),\n                method=\"complete\")\nd2_ggdc <- dendro_data(d2_hc, type = \"triangle\")\npd2c <- ggplot() +\n  geom_segment(data=d2_ggdc$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d2_ggdc$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(a) Complete linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd2_hflyc <- hierfly(d2, d2_hc, scale=FALSE)\n\npd2hc <- ggplot() +\n  geom_segment(data=d2_hflyc$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d2_hflyc$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(b) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd2 <- d2 %>%\n  mutate(clc = factor(cutree(d2_hc, 2)))\npc_d2c <- ggplot(d2) +\n  geom_point(aes(x=x1, y=x2, colour=clc), \n             size=2, alpha=0.8) +\n  scale_colour_brewer(palette=\"Set1\") +\n  ggtitle(\"(c) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Compute Wards linkage\nd2_hw <- hclust(dist(d2[,1:2]),\n                method=\"ward.D2\")\nd2_ggdw <- dendro_data(d2_hw, type = \"triangle\")\npd2w <- ggplot() +\n  geom_segment(data=d2_ggdw$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d2_ggdw$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(d) Ward's linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd2_hflyw <- hierfly(d2, d2_hw, scale=FALSE)\n\npd2hw <- ggplot() +\n  geom_segment(data=d2_hflyw$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d2_hflyw$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(e) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd2 <- d2 %>%\n  mutate(clw = factor(cutree(d2_hw, 2)))\npc_d2w <- ggplot(d2) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_brewer(palette=\"Set1\") +\n  ggtitle(\"(f) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd2c + pd2hc + pc_d2c + \n  pd2w + pd2hw + pc_d2w +\n  plot_layout(ncol=3)\n\n\n\n\n\nFigure 8.4: Complete linkage clustering on nuisance variables in comparison to Ward’s linkage. The complete linkage dendrogram looks quite reasonably suggesting a two-cluster solution, but when it is plotted amongst the data that it is clearly not a good two-cluster solution."
  },
  {
    "objectID": "hierarchical-clustering.html#clustering-in-high-dimensions",
    "href": "hierarchical-clustering.html#clustering-in-high-dimensions",
    "title": "8  Hierarchical clustering",
    "section": "8.3 Clustering in high-dimensions",
    "text": "8.3 Clustering in high-dimensions\nCheck the data: pretend we don’t know the clusters. Think you can see three elliptical clusters. One is further from the others.\n\n\nCode\npenguins <- penguins %>%\n  na.omit() # 11 observations out of 344 removed\n# use only vars of interest, and standardise\n# them for easier interpretation\npenguins_cl <- penguins[,c(1, 3:6)] %>% \n  mutate(across(where(is.numeric),  ~\n                  mulgar:::scale2(.))) %>%\n  rename(bl = bill_length_mm,\n         bd = bill_depth_mm,\n         fl = flipper_length_mm,\n         bm = body_mass_g) %>%\n  select(bl, bd, fl, bm, species) %>%\n  as.data.frame()\n\n\n\nset.seed(20230329)\nb <- basis_random(4,2)\npt1 <- save_history(penguins_cl[,1:4], \n                    max_bases = 500, \n                    start = b)\nsave(pt1, file=\"data/penguins_tour_path.rda\")\nanimate_xy(penguins_cl[,1:4], \n           tour_path = planned_tour(pt1), \n           axes=\"off\", rescale=FALSE, \n           half_range = 3.5)\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\nrender_gif(penguins_cl[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\"),\n           gif_file=\"gifs/penguins_gt.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\nFigure 8.5: Grand tour of the penguins data\n\n\n\n\nCode\np_dist <- dist(penguins_cl[,1:4])\np_hcw <- hclust(p_dist, method=\"ward.D2\")\np_hcs <- hclust(p_dist, method=\"single\")\n\np_clw <- penguins_cl %>% mutate(cl = factor(cutree(p_hcw, 3)))\np_cls <- penguins_cl %>% mutate(cl = factor(cutree(p_hcs, 3)))\n\np_w_hfly <- hierfly(p_clw, p_hcw, scale=FALSE)\np_s_hfly <- hierfly(p_cls, p_hcs, scale=FALSE)\n\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\n# library(RColorBrewer)\n#pal <- brewer.pal(length(unique(p_w_hfly$data$cl)), \"Set2\")\n#colw <- pal[p_w_hfly$data$cl]\n#cols <- pal[p_s_hfly$data$cl]\nglyphs <- c(16, 46)\npchw <- glyphs[p_w_hfly$data$node+1]\npchs <- glyphs[p_s_hfly$data$node+1]\n\nanimate_xy(p_w_hfly$data[,1:4], \n           #col=colw, \n           tour_path = planned_tour(pt1),\n           pch = pchw,\n           edges=p_w_hfly$edges, \n           axes=\"bottomleft\")\n\nanimate_xy(p_s_hfly$data[,1:4], \n           #col=colw, \n           tour_path = planned_tour(pt1),\n           pch = pchs,\n           edges=p_s_hfly$edges, \n           axes=\"bottomleft\")\n\nrender_gif(p_w_hfly$data[,1:4], \n           planned_tour(pt1),\n           display_xy(half_range=0.9,            \n                      pch = pchw,\n                      edges = p_w_hfly$edges,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_hflyw.gif\",\n           frames=500,\n           loop=FALSE)\n\nrender_gif(p_s_hfly$data[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9,            \n                      pch = pchs,\n                      edges = p_s_hfly$edges,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_hflys.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\nFigure 8.6: Dendrogram for Wards linkage of the penguins data\n\n\n\n\n\nFigure 8.7: Dendrogram for single linkage of the penguins data\n\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\n# Create a smaller one, for space concerns\npt1i <- interpolate(pt1[,,1:5], 0.1)\npw_anim <- render_anim(p_w_hfly$data, vars=1:4,\n                         frames=pt1i, \n                       edges = p_w_hfly$edges,\n             obs_labels=paste0(1:nrow(p_w_hfly$data),\n                               p_w_hfly$data$cl))\n\npw_gp <- ggplot() +\n     #geom_path(data=pw_anim$circle, \n     #          aes(x=c1, y=c2,\n     #              frame=frame), linewidth=0.1) +\n     #geom_segment(data=pw_anim$axes, \n     #             aes(x=x1, y=y1, \n     #                 xend=x2, yend=y2, \n     #                 frame=frame), \n     #             linewidth=0.1) +\n     #geom_text(data=pw_anim$axes, \n     #          aes(x=x2, y=y2, \n     #              frame=frame, \n     #              label=axis_labels), \n     #          size=5) +\n     geom_segment(data=pw_anim$edges, \n                    aes(x=x, xend=xend,\n                        y=y, yend=yend,\n                        frame=frame)) +\n     geom_point(data=pw_anim$frames, \n                aes(x=P1, y=P2, \n                    frame=frame, \n                    shape=factor(node),\n                    label=obs_labels), \n                alpha=0.8, size=1) +\n     xlim(-1,1) + ylim(-1,1) +\n     scale_shape_manual(values=c(16, 46)) +\n     coord_equal() +\n     theme_bw() +\n     theme(legend.position=\"none\", \n           axis.text=element_blank(),\n           axis.title=element_blank(),\n           axis.ticks=element_blank(),\n           panel.grid=element_blank())\n\npwg <- ggplotly(pw_gp, width=450, height=500) %>%\n       animation_button(label=\"Go\") %>%\n       animation_slider(len=0.8, x=0.5,\n                        xanchor=\"center\") %>%\n       animation_opts(easing=\"linear\", transition = 0)\nhtmlwidgets::saveWidget(pwg,\n          file=\"html/penguins_cl_ward.html\",\n          selfcontained = TRUE)\n\n# Single\nps_anim <- render_anim(p_s_hfly$data, vars=1:4,\n                         frames=pt1i, \n                       edges = p_s_hfly$edges,\n             obs_labels=paste0(1:nrow(p_s_hfly$data),\n                               p_s_hfly$data$cl))\n\nps_gp <- ggplot() +\n     #geom_path(data=pw_anim$circle, \n     #          aes(x=c1, y=c2,\n     #              frame=frame), linewidth=0.1) +\n     #geom_segment(data=pw_anim$axes, \n     #             aes(x=x1, y=y1, \n     #                 xend=x2, yend=y2, \n     #                 frame=frame), \n     #             linewidth=0.1) +\n     #geom_text(data=pw_anim$axes, \n     #          aes(x=x2, y=y2, \n     #              frame=frame, \n     #              label=axis_labels), \n     #          size=5) +\n     geom_segment(data=ps_anim$edges, \n                    aes(x=x, xend=xend,\n                        y=y, yend=yend,\n                        frame=frame)) +\n     geom_point(data=ps_anim$frames, \n                aes(x=P1, y=P2, \n                    frame=frame, \n                    shape=factor(node),\n                    label=obs_labels), \n                alpha=0.8, size=1) +\n     xlim(-1,1) + ylim(-1,1) +\n     scale_shape_manual(values=c(16, 46)) +\n     coord_equal() +\n     theme_bw() +\n     theme(legend.position=\"none\", \n           axis.text=element_blank(),\n           axis.title=element_blank(),\n           axis.ticks=element_blank(),\n           panel.grid=element_blank())\n\npsg <- ggplotly(ps_gp, width=450, height=500) %>%\n       animation_button(label=\"Go\") %>%\n       animation_slider(len=0.8, x=0.5,\n                        xanchor=\"center\") %>%\n       animation_opts(easing=\"linear\", transition = 0)\nhtmlwidgets::saveWidget(psg,\n          file=\"html/penguins_cl_single.html\",\n          selfcontained = TRUE)\n\n\n\n\n\nFigure 8.8: Animation of dendrogram from Wards linkage clustering of the penguins data.\n\n\n\n\n\nFigure 8.9: Animation of dendrogram from single linkage clustering of the penguins data.\n\n\n\n\n\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning with r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377."
  },
  {
    "objectID": "pca.html#exercises",
    "href": "pca.html#exercises",
    "title": "2  Principal component analysis (PCA)",
    "section": "2.1 Exercises",
    "text": "2.1 Exercises\n\nMake a scatterplot matrix of the first four PCs. Is the branch pattern visible in any pair?\nConstruct five new variables to measure these skills offense, defense, playing time, ball movement, errors. Using the tour, examine the relationship between these variables. Map out how a few players could be characterised based on these directions of skills."
  },
  {
    "objectID": "model-based-clustering.html",
    "href": "model-based-clustering.html",
    "title": "10  Model-based clustering",
    "section": "",
    "text": "# Model-based clustering\nModel-based clustering Fraley and Raftery (2002) fits a multivariate normal mixture model to the data. It uses the EM algorithm to fit the parameters for the mean, variance–covariance of each population, and the mixing proportion. The variance-covariance matrix is re-parametrized using an eigen-decomposition\n\\[\n\\Sigma_k = \\lambda_kD_kA_kD_k', ~~~k=1, \\dots, g ~~\\mbox{(number of clusters)}\n\\]\nresulting in several model choices, ranging from simple to complex:\n\n\n\n\nParameterizations of the covariance matrix\n \n  \n    Model \n    Sigma \n    Family \n    Volume \n    Shape \n    Orientation \n  \n \n\n  \n    EII \n    $$\\lambda I$$ \n    Spherical \n    Equal \n    Equal \n    NA \n  \n  \n    VII \n    $$\\lambda_k I$$ \n    Spherical \n    Variable \n    Equal \n    NA \n  \n  \n    EEI \n    $$\\lambda A$$ \n    Diagonal \n    Equal \n    Equal \n    Coordinate axes \n  \n  \n    VEI \n    $$\\lambda_kA$$ \n    Diagonal \n    Variable \n    Equal \n    Coordinate axes \n  \n  \n    EVI \n    $$\\lambda A_k$$ \n    Diagonal \n    Equal \n    Variable \n    Coordinate axes \n  \n  \n    VVI \n    $$\\lambda_k A_k$$ \n    Diagonal \n    Variable \n    Variable \n    Coordinate axes \n  \n  \n    EEE \n    $$\\lambda DAD^T$$ \n    Diagonal \n    Equal \n    Equal \n    Equal \n  \n  \n    EVE \n    $$\\lambda DA_kD^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Equal \n  \n  \n    VEE \n    $$\\lambda_k DAD^T$$ \n    Ellipsoidal \n    Variable \n    Equal \n    Equal \n  \n  \n    VVE \n    $$\\lambda_k DA_kD^T$$ \n    Ellipsoidal \n    Variable \n    Equal \n    Equal \n  \n  \n    EEV \n    $$\\lambda D_kAD_k^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Variable \n  \n  \n    VEV \n    $$\\lambda_k D_kAD_k^T$$ \n    Ellipsoidal \n    Variable \n    Variable \n    Variable \n  \n  \n    EVV \n    $$\\lambda D_kA_kD_k^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Variable \n  \n  \n    VVV \n    $$\\lambda_k D_kA_kD_k^T$$ \n    Ellipsoidal \n    Variable \n    Variable \n    Variable \n  \n\n\n\n\n\nNote the distribution descriptions “spherical” and “ellipsoidal”. These are descriptions of the shape of the variance-covariance for a multivariate normal distribution. A standard multivariate normal distribution has a variance-covariance matrix with zeros in the off-diagonal elements, which corresponds to spherically shaped data. When the variances (diagonals) are different or the variables are correlated, then the shape of data from a multivariate normal is ellipsoidal.\n\nThe models are typically scored using the Bayes Information Criterion (BIC), which is based on the log likelihood, number of variables, and number of mixture components. They should also be assessed using graphical methods, as we demonstrate using the data. We start with two of the four real-valued variables (bl, fl) and the three species.\n\n\nCode\nlibrary(ggplot2)\n#library(colorspace)\nload(\"data/penguins_sub.rda\")\nggplot(penguins_sub, aes(x=bl, \n                         y=fl)) + #, \n                         #colour=species)) +\n  geom_point() +\n  geom_density2d() +\n  #scale_color_discrete_qualitative(\"Dark 3\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\nFigure 10.1: Scatterplot of flipper length by bill length of the penguins data.\n\n\n\n\nThe goal is to determine whether model-based methods can discover clusters that closely correspond to the three species. Based on the scatterplot in Figure 10.1 we would expect it to do well, and suggest an elliptical variance-covariance of roughly equal sizes as the model.\n\n\nCode\nlibrary(mclust)\nlibrary(mulgar)\nlibrary(patchwork)\nlibrary(colorspace)\npenguins_BIC <- mclustBIC(penguins_sub[,c(1,3)])\nggmc <- ggmcbic(penguins_BIC, cl=2:9, top=4) + ggtitle(\"(a)\")\npenguins_mc <- Mclust(penguins_sub[,c(1,3)], \n                      G=3, \n                      modelNames = \"EVE\")\npenguins_mce <- mc_ellipse(penguins_mc)\npenguins_cl <- penguins_sub[,c(1,3)]\npenguins_cl$cl <- factor(penguins_mc$classification)\nggell <- ggplot() +\n   geom_point(data=penguins_cl, aes(x=bl, y=fl,\n                                    colour=cl),\n              alpha=0.3) +\n   geom_point(data=penguins_mce$ell, aes(x=bl, y=fl,\n                                         colour=cl),\n              shape=16) +\n   geom_point(data=penguins_mce$mn, aes(x=bl, y=fl,\n                                        colour=cl),\n              shape=3, size=2) +\n  scale_color_discrete_qualitative(palette = \"Dark 3\") +\n   theme(aspect.ratio=1, legend.position=\"none\") +\n  ggtitle(\"(b)\")\nggmc + ggell + plot_layout(ncol=2)\n\n\n\n\n\nFigure 10.2: Summary plots from model-based clustering: (a) BIC values for clusters 2-9 of top four models, (b) variance-covariance ellipses and cluster means (+) corresponding to the best model. The best model is three-cluster EVE, which has differently shaped variance-covariances albeit the same volume and orientation.\n\n\n\n\nFigure 10.2 summarises the results. All models agree that three clusters is the best. The different variance-covariance models for three clusters have similar BIC values with EVE (different shape, same volume and orientation) being slightly higher. These plots are made from the mclust package output using the ggmcbic and mc_ellipse functions fro the mulgar package.\n\n?fig-model-based1 contains the plots we will use to examine the results of model-based clustering on this reduced dataset. The top leftmost plot shows the data, with male and female crabs distinguished by color and glyph. The two sexes correspond to long cigar-shaped objects that overlap a bit, particularly for smaller crabs. The “cigars” are not perfectly regular: The variance of the data is smaller at small values for both sexes, so that our cigars are somewhat wedge-shaped. The orientation of the longest direction of variance differs slightly between groups too: The association has a steeper slope for female crabs than for males, because female crabs have relatively larger rear width than male crabs. With the heterogeneity in variance-covariance, this data does not strictly adhere to the multivariate normal mixture model underlying model-based methods, but we hope that the departure from regularity is not so extreme that it prevents the model from working.\nThe top right plot shows the BIC results for a full range of models, EEE, EEV, and VVV variance-covariance parametrization for one to nine clusters:\n> blue.crabBIC <- mclustBIC(subset(d.blue.crabs,\n    select=c(FL,RW)), modelNames=c(\"EEE\",\"EEV\",\"VVV\"))\n> blue.crabBIC\n BIC:\n        EEE       EEV       VVV\n1 -810.3289 -810.3289 -810.3289\n2 -820.7272 -778.6450 -783.7705\n3 -832.7712 -792.5937 -821.8645\n4 -824.8927 -835.5631 -835.7799\n5 -805.8402 -805.9425 -853.1395\n6 -807.8380 -821.1586 -879.3500\n7 -827.1099 -860.7258 -878.0679\n8 -833.8051 -861.1460 -891.9757\n9 -835.6620 -854.6120 -904.6108\n> plot(blue.crabBIC)\nEEE EEV VVV \n 15  12   0 \nThe best model, EEV-2, used the equal volume, equal shape, and different orientation variance–covariance parametrization and divided the data into two clusters. This solution seems to be perfect! We can imagine that this result corresponds to two equally shaped ellipses that intersect near the lowest values of the data and angle toward higher values. We will check by drawing ellipses representing the variance–covariance parametrization on the data plots. The parameter estimates are used to scale and center the ellipses:\n> mclst1 <- mclustBIC(subset(d.blue.crabs,\n    select=c(FL,RW)), G=2, modelNames=\"EEV\")\n> mclst1\n BIC:\n       EEV\n2 -778.645\n> smry1 <- mclustModel(subset(d.blue.crabs,\n    select=c(FL,RW)), mclst1, G=2, modelNames=\"EEV\")\n> vc <- smry1$parameters$variance$sigma[,,1]\n> xm <- smry1$parameters$mean[,1]\n> y1 <- f.vc.ellipse(vc,xm,500)\n> ...\nyielding the plots in the middle and bottom rows of ?fig-model-based1. In the plot of the data alone, cluster id is used for the color and glyph of points. (Compare this plot with the one directly above it, in which the classes are known.) Cluster 1 mostly corresponds to the female crabs, and cluster 2 to the males, except that all the small crabs, both male and female, have been assigned to cluster 1. In the rightmost plot, we have added ellipses representing the estimated variance–covariances. The ellipses are the same shape, as specified by the model, but the ellipse for cluster 2 is shifted toward the large values.\nThe next two best models, according to the BIC values, are EEV-3 and VVV-2. The plots in the bottom row display representations of the variance–covariances for these models. EEV-3 organizes the crabs into three clusters according to the size, not the sex, of the crabs. The VVV-2 solution is similar to EEV-2.\nWhat solution is the best for this data? If the EEV-3 model had done what we intuitively expected, it would have been ideal: The sexes of smaller crabs are indistinguishable, so they should be afforded their own cluster, whereas larger crabs could be clustered into males and females. However, the cluster that includes the small crabs also includes a fair number of middle-sized female crabs.\nFinally, model-based clustering did not discover the true gender clusters. Still, it produced a useful and interpretable clustering of the crabs.\nPlots are indispensable for choosing an appropriate cluster model. It is easy to visualize the models when there are only two variables but increasingly difficult as the number of variables grows. Tour methods save us from producing page upon page of plots. They allow us to look at many projections of the data, which enables us to conceptualize the shapes and relationships between clusters in more than two dimensions.\n?fig-model-based2 displays the graphics for the corresponding high-dimensional investigation using all five variables and four classes (two species, two sexes) of the . The cluster analysis is much more difficult now. Can model-based clustering uncover these four groups?\nIn the top row of plots, we display the raw data, before modeling. Each plot is a tour projection of the data, colored according to the four true classes. The blue and purple points are the male and female crabs of the blue species, and the yellow and orange points are the male and female crabs of the orange species. This table will help you keep track:\n\nThe clusters corresponding to the classes are long thin wedges in five dimensions (5D), with more separation and more variability at larger values, as we saw in the subset just discussed. The rightmost plot shows the ``looking down the barrel’’ view of the wedges. At small values the points corresponding to the sexes are mixed (leftmost plot). The species are reasonably well separated even for small crabs (middle plot). The variance–covariance is wedge-shaped rather than elliptical, but again we hope that modeling based on the normal distribution that has elliptical variance–covariance will be adequate.\n\nIn the results from model-based clustering, there is very little difference in BIC value for variance–covariance models EEE, EEV, VEV, and VVV, with a number of clusters from three to eight. The best model is EEV-3, and EEV-4 is second best. We know that three clusters is insufficient to capture the four classes we have in mind, so we examine the four-cluster solution.\n> mclst4 <- mclustBIC(subset(d.blue.crabs,select=c(FL:BD)),\n    G=1:8, modelNames=c(\"EEE\",\"EEV\",\"VVV\"))\n> plot(mclst4)\nEEE EEV VVV \n 15  12   0 \n> mclst5 <- mclustBIC(subset(d.blue.crabs,select=c(FL:BD)), \n   G=4, modelNames=\"EEV\")\n> smry5 <- mclustModel(subset(d.blue.crabs,select=c(FL:BD)), \n   mclst5, G=4, modelNames=\"EEV\")\nThe bottom row of plots in ?fig-model-based2 illustrates the four-cluster model in three different projections, matching the projections in the top row showing the data.\n> vc <- smry5$parameters$variance$sigma[,,1]\n> mn <- smry5$parameters$mean[,1]\n> y1 <- f.vc.ellipse(vc, mn)\n> ...\n> mclst5.model <- cbind(matrix(NA,500*4,3),\n    rbind(y1,y2,y3,y4))\n> colnames(mclst5.model) <-\n    c(\"Species\",\"Sex\",\"Index\",\"FL\",\"RW\",\"CL\",\"CW\",\"BD\")\n> d.crabs.model <- rbind(d.crabs, mclst5.model)\n> gd <- ggobi(d.crabs.model)[1]\n> glyph_color(gd) <- c(rep(4,50), rep(1,50), rep(9,50), \n    rep(6,50), rep(8,2000))\nIn each view, the ellipsoids representing the variance–covariance estimates for the four clusters are shown in four shades of gray, because none of these match any actual cluster in the data. Remember that these are 2D projections of 5D ellipsoids. The resulting clusters from the model do not match the true classes very well. The result roughly captures the two species, as we see in the plots in the first column, where the species are separated both in the data and in the ellipses. On the other hand, the grouping corresponding to is completely missed: See the plots in the middle and right-hand columns, where sexes are separated in the actual data but the ellipses are not separated. Just as in the smaller subset (two variables, one species) discussed earlier, there is a cluster for the smaller crabs of both species and sexes. The results of model-based clustering on the full 5D data are very unsatisfactory.\nIn summary, plots of the data and parameter estimates for model-based cluster analysis are very useful for understanding the solution, and choosing an appropriate model. Tours are very helpful for examining the results in higher dimensions, for arbitrary numbers of variables.\n\n\n\n\n\nFraley, C., and A. E. Raftery. 2002. “Model-Based Clustering, Discriminant Analysis, Density Estimation.” Journal of the American Statistical Association 97: 611–31."
  },
  {
    "objectID": "hierarchical-clustering.html#explanation-of-the-algorithm",
    "href": "hierarchical-clustering.html#explanation-of-the-algorithm",
    "title": "8  Hierarchical clustering",
    "section": "8.1 Explanation of the algorithm",
    "text": "8.1 Explanation of the algorithm\nHierarchical cluster algorithms sequentially fuse neighboring points to form ever-larger clusters, starting from a full interpoint distance matrix. Figure 8.1 illustrates the hierarchical clustering approach for a simple simulated data set (a) with two well-separated clusters in 2D. The dendrogram (b) is a representation of the order that points are joined into clusters. The dendrogram strongly indicates two clusters because there are two branches branches representing the last join are much longer than all of the other branches. Although, the dendrogram is usually a good summary of the steps taken by the algorithm, it can be misleading. The dendrogram might strongly suggest a clustering but it might be a terrible solution. To check this we need to show the model with the data, as shown in plot (c). The segments show how the points and clusters are joined. Note that once points are joined into a cluster, the centroid of that cluster is used as the join location with other points or other clusters, and this is represented by a “+”. We can see that the longest edge is the one stretching across the gap between the two clusters, which is the location where the dendrogram would be cut to produce the two-cluster solution. This two-cluster solution is shown in plot (d).\n\n\nCode\nlibrary(ggplot2)\nlibrary(mulgar)\nlibrary(ggdendro)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(tourr)\nlibrary(plotly)\nlibrary(htmlwidgets)\n#library(RColorBrewer)\nlibrary(colorspace)\nlibrary(GGally)\n\n\n\n\nCode\ndata(simple_clusters)\n# Data has two well-separated clusters\npd <- ggplot(simple_clusters, aes(x=x1, y=x2)) +\n  geom_point(colour=\"orange\", size=2, alpha=0.8) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio=1) \n\n# Compute hierarchical clustering with Ward's linkage\ncl_hw <- hclust(dist(simple_clusters[,1:2]),\n                method=\"ward.D2\")\ncl_ggd <- dendro_data(cl_hw, type = \"triangle\")\nph <- ggplot() +\n  geom_segment(data=cl_ggd$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=cl_ggd$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(b)\") +\n  theme_dendro()\n\n# Compute dendrogram in data\ncl_hfly <- hierfly(simple_clusters, cl_hw, scale=FALSE)\n\npdh <- ggplot() +\n  geom_segment(data=cl_hfly$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=cl_hfly$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(c)\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nsimple_clusters <- simple_clusters %>%\n  mutate(clw = factor(cutree(cl_hw, 2)))\npc <- ggplot(simple_clusters) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(d)\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd + ph + pdh + pc + plot_layout(ncol=2)\n\n\n\n\n\nFigure 8.1: Hierarchical clustering on simulated data: (a) data, (b) dendrogram, (c) dendrogram on the data, and (d) two cluster solution. Nodes of the dendrogram indicated by + when it is drawn on the data.\n\n\n\n\nClustering algorithms are all prone to being confused by different problems occurring in data. For hierarchical clustering, plotting the dendrogram on the data provides another way to assess the solution. For hierarchical clustering additional the complications arise from a range of choices for defining distance once points have been joined into clusters.\nDistance between clusters is described by a “linkage method”, of which there are many. For example, single linkage measures the distance between clusters by the smallest interpoint distance between the members of the two clusters clusters, complete linkage uses the maximum interpoint distance, and average linkage uses the average of the interpoint distances. Wards linkage, which usually produces the best clustering solutions, defines the distance as the reduction in the within-group variance. A good discussion on cluster analysis and linkage can be found in Boehmke and Greenwell (2019), on Wikipedia or any multivariate textbook."
  },
  {
    "objectID": "hierarchical-clustering.html#why-you-should-look-at-the-dendrogram-on-the-data",
    "href": "hierarchical-clustering.html#why-you-should-look-at-the-dendrogram-on-the-data",
    "title": "8  Hierarchical clustering",
    "section": "8.2 Why you should look at the dendrogram on the data",
    "text": "8.2 Why you should look at the dendrogram on the data\n\n\nCode\n# Nuisance observations\nset.seed(20190514)\nx <- (runif(20)-0.5)*4\ny <- x\nd1 <- data.frame(x1 = c(rnorm(50, -3), \n                            rnorm(50, 3), x),\n                 x2 = c(rnorm(50, -3), \n                            rnorm(50, 3), y),\n                 cl = factor(c(rep(\"A\", 50), \n                             rep(\"B\", 70))))\nd1 <- d1 %>% \n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\npd1 <- ggplot(data=d1, aes(x=x1, y=x2)) + \n  geom_point() +\n    ggtitle(\"Nuisance observations\") +\n    theme(aspect.ratio=1) \n\n# Nuisance variables\nset.seed(20190512)\nd2 <- data.frame(x1=c(rnorm(50, -4), \n                            rnorm(50, 4)),\n                 x2=c(rnorm(100)),\n                 cl = factor(c(rep(\"A\", 50), \n                             rep(\"B\", 50))))\nd2 <- d2 %>% \n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\npd2 <- ggplot(data=d2, aes(x=x1, y=x2)) + \n  geom_point() +\n    ggtitle(\"Nuisance variables\") +\n    theme(aspect.ratio=1)\n\npd1 + pd2 + plot_layout(ncol=2)\n\n\n\n\n\nFigure 8.2: Two examples of data structure that causes problems for hierarchical clustering. Nuisance observations can cause problems because the close observations between the two clusters can cause some chaining in the hierarchical joining of observations. Nuisance variables can cause problems because observations across the gap can seem closer than observations at the end of each cluster.\n\n\n\n\n\n\nCode\n# Compute single linkage\nd1_hs <- hclust(dist(d1[,1:2]),\n                method=\"single\")\nd1_ggds <- dendro_data(d1_hs, type = \"triangle\")\npd1s <- ggplot() +\n  geom_segment(data=d1_ggds$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d1_ggds$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(a) Single linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd1_hflys <- hierfly(d1, d1_hs, scale=FALSE)\n\npd1hs <- ggplot() +\n  geom_segment(data=d1_hflys$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d1_hflys$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(b) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd1 <- d1 %>%\n  mutate(cls = factor(cutree(d1_hs, 2)))\npc_d1s <- ggplot(d1) +\n  geom_point(aes(x=x1, y=x2, colour=cls), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(c) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Compute Wards linkage\nd1_hw <- hclust(dist(d1[,1:2]),\n                method=\"ward.D2\")\nd1_ggdw <- dendro_data(d1_hw, type = \"triangle\")\npd1w <- ggplot() +\n  geom_segment(data=d1_ggdw$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d1_ggdw$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(d) Ward's linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd1_hflyw <- hierfly(d1, d1_hw, scale=FALSE)\n\npd1hw <- ggplot() +\n  geom_segment(data=d1_hflyw$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d1_hflyw$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(e) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd1 <- d1 %>%\n  mutate(clw = factor(cutree(d1_hw, 2)))\npc_d1w <- ggplot(d1) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(f) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd1s + pd1hs + pc_d1s + \n  pd1w + pd1hw + pc_d1w +\n  plot_layout(ncol=3)\n\n\n\n\n\nFigure 8.3: Single linkage clustering on nuisance cases in comparison to Ward’s linkage.\n\n\n\n\n\n\nCode\n# Compute complete linkage\nd2_hc <- hclust(dist(d2[,1:2]),\n                method=\"complete\")\nd2_ggdc <- dendro_data(d2_hc, type = \"triangle\")\npd2c <- ggplot() +\n  geom_segment(data=d2_ggdc$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d2_ggdc$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(a) Complete linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd2_hflyc <- hierfly(d2, d2_hc, scale=FALSE)\n\npd2hc <- ggplot() +\n  geom_segment(data=d2_hflyc$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d2_hflyc$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(b) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd2 <- d2 %>%\n  mutate(clc = factor(cutree(d2_hc, 2)))\npc_d2c <- ggplot(d2) +\n  geom_point(aes(x=x1, y=x2, colour=clc), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(c) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Compute Wards linkage\nd2_hw <- hclust(dist(d2[,1:2]),\n                method=\"ward.D2\")\nd2_ggdw <- dendro_data(d2_hw, type = \"triangle\")\npd2w <- ggplot() +\n  geom_segment(data=d2_ggdw$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d2_ggdw$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(d) Ward's linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd2_hflyw <- hierfly(d2, d2_hw, scale=FALSE)\n\npd2hw <- ggplot() +\n  geom_segment(data=d2_hflyw$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d2_hflyw$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(e) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd2 <- d2 %>%\n  mutate(clw = factor(cutree(d2_hw, 2)))\npc_d2w <- ggplot(d2) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(f) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd2c + pd2hc + pc_d2c + \n  pd2w + pd2hw + pc_d2w +\n  plot_layout(ncol=3)\n\n\n\n\n\nFigure 8.4: Complete linkage clustering on nuisance variables in comparison to Ward’s linkage. The complete linkage dendrogram looks quite reasonably suggesting a two-cluster solution, but when it is plotted amongst the data that it is clearly not a good two-cluster solution."
  },
  {
    "objectID": "hierarchical-clustering.html#dendrograms-in-high-dimensions",
    "href": "hierarchical-clustering.html#dendrograms-in-high-dimensions",
    "title": "8  Hierarchical clustering",
    "section": "8.3 Dendrograms in high-dimensions",
    "text": "8.3 Dendrograms in high-dimensions\nCheck the data: pretend we don’t know the clusters. Think you can see three elliptical clusters. One is further from the others.\n\n\nCode\nload(\"data/penguins_sub.rda\")\n\n\n\n\nCode\nggscatmat(penguins_sub[,1:4]) \n\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\nℹ The deprecated feature was likely used in the GGally package.\n  Please report the issue at <https://github.com/ggobi/ggally/issues>.\n\n\n\n\n\nScatterplot matrix of the penguins data, assuming we don’t know the species\n\n\n\n\n\nset.seed(20230329)\nb <- basis_random(4,2)\npt1 <- save_history(penguins_sub[,1:4], \n                    max_bases = 500, \n                    start = b)\nsave(pt1, file=\"data/penguins_tour_path.rda\")\nanimate_xy(penguins_sub[,1:4], \n           tour_path = planned_tour(pt1), \n           axes=\"off\", rescale=FALSE, \n           half_range = 3.5)\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\nrender_gif(penguins_sub[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\"),\n           gif_file=\"gifs/penguins_gt.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\nFigure 8.5: Grand tour of the penguins data\n\n\n\n\nCode\np_dist <- dist(penguins_sub[,1:4])\np_hcw <- hclust(p_dist, method=\"ward.D2\")\np_hcs <- hclust(p_dist, method=\"single\")\n\np_clw <- penguins_sub %>% \n  mutate(cl = factor(cutree(p_hcw, 3)))\np_cls <- penguins_sub %>% \n  mutate(cl = factor(cutree(p_hcs, 3)))\n\np_w_hfly <- hierfly(p_clw, p_hcw, scale=FALSE)\np_s_hfly <- hierfly(p_cls, p_hcs, scale=FALSE)\n\n\n\n\nCode\n# Generate the dendrograms in 2D\np_hcw_dd <- dendro_data(p_hcw)\npw_dd <- ggplot() +\n  geom_segment(data=p_hcw_dd$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=p_hcw_dd$labels, aes(x=x, y=y),\n             alpha=0.8) +\n  theme_dendro()\n\np_hcs_dd <- dendro_data(p_hcs)\nps_dd <- ggplot() +\n  geom_segment(data=p_hcs_dd$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=p_hcs_dd$labels, aes(x=x, y=y),\n             alpha=0.8) +\n  theme_dendro()\n\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\nglyphs <- c(16, 46)\npchw <- glyphs[p_w_hfly$data$node+1]\npchs <- glyphs[p_s_hfly$data$node+1]\n\nanimate_xy(p_w_hfly$data[,1:4], \n           #col=colw, \n           tour_path = planned_tour(pt1),\n           pch = pchw,\n           edges=p_w_hfly$edges, \n           axes=\"bottomleft\")\n\nanimate_xy(p_s_hfly$data[,1:4], \n           #col=colw, \n           tour_path = planned_tour(pt1),\n           pch = pchs,\n           edges=p_s_hfly$edges, \n           axes=\"bottomleft\")\n\nrender_gif(p_w_hfly$data[,1:4], \n           planned_tour(pt1),\n           display_xy(half_range=0.9,            \n                      pch = pchw,\n                      edges = p_w_hfly$edges,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_hflyw.gif\",\n           frames=500,\n           loop=FALSE)\n\nrender_gif(p_s_hfly$data[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9,            \n                      pch = pchs,\n                      edges = p_s_hfly$edges,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_hflys.gif\",\n           frames=500,\n           loop=FALSE)\n\n# Show three cluster solutions\nclrs <- qualitative_hcl(3, \"Dark 3\")\nw3_col <- clrs[p_w_hfly$data$cl[p_w_hfly$data$node == 0]]\nrender_gif(p_w_hfly$data[p_w_hfly$data$node == 0, 1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9,   \n                      col=w3_col,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_w3.gif\",\n           frames=500,\n           loop=FALSE)\n\nclrs <- qualitative_hcl(3, \"Dark 3\")\ns3_col <- clrs[p_s_hfly$data$cl[p_w_hfly$data$node == 0]]\nrender_gif(p_s_hfly$data[p_w_hfly$data$node == 0,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9,   \n                      col=s3_col,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_s3.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\n\n\n\n\n\n(a) Wards linkage\n\n\n\n\n\n\n\n\n\n(b) single linkage\n\n\n\n\n\n\n\n\n\n\n(c) Wards linkage\n\n\n\n\n\n\n\n(d) Single linkage\n\n\n\n\n\n\n\n\n\n(e) Wards linkage\n\n\n\n\n\n\n\n(f) Single linkage\n\n\n\n\nFigure 8.6: Dendrograms for Wards and single linkage of the penguins data, shown in 2D (top) and in 4D (middle), and the three-cluster solution of each.\n\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\n# Create a smaller one, for space concerns\npt1i <- interpolate(pt1[,,1:5], 0.1)\npw_anim <- render_anim(p_w_hfly$data,\n                       vars=1:4,\n                       frames=pt1i, \n                       edges = p_w_hfly$edges,\n             obs_labels=paste0(1:nrow(p_w_hfly$data),\n                               p_w_hfly$data$cl))\n\npw_gp <- ggplot() +\n     #geom_path(data=pw_anim$circle, \n     #          aes(x=c1, y=c2,\n     #              frame=frame), linewidth=0.1) +\n     #geom_segment(data=pw_anim$axes, \n     #             aes(x=x1, y=y1, \n     #                 xend=x2, yend=y2, \n     #                 frame=frame), \n     #             linewidth=0.1) +\n     #geom_text(data=pw_anim$axes, \n     #          aes(x=x2, y=y2, \n     #              frame=frame, \n     #              label=axis_labels), \n     #          size=5) +\n     geom_segment(data=pw_anim$edges, \n                    aes(x=x, xend=xend,\n                        y=y, yend=yend,\n                        frame=frame)) +\n     geom_point(data=pw_anim$frames, \n                aes(x=P1, y=P2, \n                    frame=frame, \n                    shape=factor(node),\n                    label=obs_labels), \n                alpha=0.8, size=1) +\n     xlim(-1,1) + ylim(-1,1) +\n     scale_shape_manual(values=c(16, 46)) +\n     coord_equal() +\n     theme_bw() +\n     theme(legend.position=\"none\", \n           axis.text=element_blank(),\n           axis.title=element_blank(),\n           axis.ticks=element_blank(),\n           panel.grid=element_blank())\n\npwg <- ggplotly(pw_gp, width=450, height=500,\n                tooltip=\"label\") %>%\n       animation_button(label=\"Go\") %>%\n       animation_slider(len=0.8, x=0.5,\n                        xanchor=\"center\") %>%\n       animation_opts(easing=\"linear\", transition = 0)\nhtmlwidgets::saveWidget(pwg,\n          file=\"html/penguins_cl_ward.html\",\n          selfcontained = TRUE)\n\n# Single\nps_anim <- render_anim(p_s_hfly$data, vars=1:4,\n                         frames=pt1i, \n                       edges = p_s_hfly$edges,\n             obs_labels=paste0(1:nrow(p_s_hfly$data),\n                               p_s_hfly$data$cl))\n\nps_gp <- ggplot() +\n     #geom_path(data=pw_anim$circle, \n     #          aes(x=c1, y=c2,\n     #              frame=frame), linewidth=0.1) +\n     #geom_segment(data=pw_anim$axes, \n     #             aes(x=x1, y=y1, \n     #                 xend=x2, yend=y2, \n     #                 frame=frame), \n     #             linewidth=0.1) +\n     #geom_text(data=pw_anim$axes, \n     #          aes(x=x2, y=y2, \n     #              frame=frame, \n     #              label=axis_labels), \n     #          size=5) +\n     geom_segment(data=ps_anim$edges, \n                    aes(x=x, xend=xend,\n                        y=y, yend=yend,\n                        frame=frame)) +\n     geom_point(data=ps_anim$frames, \n                aes(x=P1, y=P2, \n                    frame=frame, \n                    shape=factor(node),\n                    label=obs_labels), \n                alpha=0.8, size=1) +\n     xlim(-1,1) + ylim(-1,1) +\n     scale_shape_manual(values=c(16, 46)) +\n     coord_equal() +\n     theme_bw() +\n     theme(legend.position=\"none\", \n           axis.text=element_blank(),\n           axis.title=element_blank(),\n           axis.ticks=element_blank(),\n           panel.grid=element_blank())\n\npsg <- ggplotly(ps_gp, width=450, height=500,\n                tooltip=\"label\") %>%\n       animation_button(label=\"Go\") %>%\n       animation_slider(len=0.8, x=0.5,\n                        xanchor=\"center\") %>%\n       animation_opts(easing=\"linear\", transition = 0)\nhtmlwidgets::saveWidget(psg,\n          file=\"html/penguins_cl_single.html\",\n          selfcontained = TRUE)\n\n\n\n\n\n\n\n\nFigure 8.7: Animation of dendrogram from Wards (top) and single (bottom) linkage clustering of the penguins data.\n\n\n\n\n\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning with r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377."
  },
  {
    "objectID": "nldr.html",
    "href": "nldr.html",
    "title": "3  Non-linear dimension reduction",
    "section": "",
    "text": "Code\nlibrary(liminal)\nlibrary(Rtsne)\ndata(fake_trees)\nset.seed(2020)\ntsne <- Rtsne::Rtsne(dplyr::select(fake_trees, dplyr::starts_with(\"dim\")))\ntsne_df <- data.frame(tsneX = tsne$Y[, 1],\n                      tsneY = tsne$Y[, 2])\nlimn_tour_link(\n  tsne_df,\n  fake_trees,\n  cols = dim1:dim10,\n  color = branches\n)\n\n\n\n\n\nFigure 3.1: Linked views of t-SNE dimension reduction with a tour of the fake trees data. The t-SNE view clearly shows ten 1D non-linear clusters, while the tour of the full 100 variables suggests a lot more variation in the data, and less difference between clusters.\n\n\n \n\n\n\nFigure 3.2: The t-SNE mapping of the penguins data inaccurately splits one of the clusters. The three clusters are clearly distinct when viewed with the tour.\n\n\n\n\nCode\nload(\"data/penguins_sub.rda\")\n\nset.seed(2022)\np_tsne <- Rtsne::Rtsne(penguins_sub[,2:5])\np_tsne_df <- data.frame(tsneX = p_tsne$Y[, 1], tsneY = p_tsne$Y[, 2])\nlimn_tour_link(\n  p_tsne_df,\n  penguins_sub,\n  cols = bl:bm,\n  color = species\n)"
  },
  {
    "objectID": "data.html#palmer-penguins",
    "href": "data.html#palmer-penguins",
    "title": "Appendix B — Data",
    "section": "B.2 Palmer penguins",
    "text": "B.2 Palmer penguins\n\n\nCode\nlibrary(palmerpenguins)\npenguins <- penguins %>%\n  na.omit() # 11 observations out of 344 removed\n# use only vars of interest, and standardise\n# them for easier interpretation\npenguins_sub <- penguins[,c(1, 3:6)] %>% \n  mutate(across(where(is.numeric),  ~ scale(.)[,1])) %>%\n  rename(bl = bill_length_mm,\n         bd = bill_depth_mm,\n         fl = flipper_length_mm,\n         bm = body_mass_g)\nsave(penguins_sub, file=\"data/penguins_sub.rda\")\n\n\n\nDescription\nThis data measure four physical characteristics of three species of penguins.\n\n\nVariables\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nbl\na number denoting bill length (millimeters)\n\n\nbd\na number denoting bill depth (millimeters)\n\n\nfl\nan integer denoting flipper length (millimeters)\n\n\nbm\nan integer denoting body mass (grams)\n\n\nspecies\na factor denoting penguin species (Adélie, Chinstrap and Gentoo)\n\n\n\n\n\nPurpose\nThe primary goal is to find a combination of the four variables where the three species are distinct. This is also a useful data set to illustrate cluster analysis.\n\n\nSource\nDetails of the penguins data can be found at https://allisonhorst.github.io/palmerpenguins/, and Horst, Hill, and Gorman (2020) is the package source.\n\n\nPre-processing\nThe data is loaded from the palmerpenguins package. The four physical measurement variables and the species are selected, and the penguins with missing values are removed. Variables are standardised, and their names are shortened.\n\nlibrary(palmerpenguins)\npenguins <- penguins %>%\n  na.omit() # 11 observations out of 344 removed\n# use only vars of interest, and standardise\n# them for easier interpretation\npenguins_sub <- penguins[,c(3:6, 1)] %>% \n  mutate(across(where(is.numeric),  ~ scale(.)[,1])) %>%\n  rename(bl = bill_length_mm,\n         bd = bill_depth_mm,\n         fl = flipper_length_mm,\n         bm = body_mass_g) %>%\n  as.data.frame()\nsave(penguins_sub, file=\"data/penguins_sub.rda\")"
  },
  {
    "objectID": "unsupervised.html#sec-clust-bg",
    "href": "unsupervised.html#sec-clust-bg",
    "title": "Unsupervised learning",
    "section": "Background",
    "text": "Background\nBefore we can begin finding groups of cases that are similar, we need to decide on a definition of similarity. How is similarity defined? Consider a dataset with three cases \\((a_1, a_2, a_3)\\) and four variables \\((V_1, V_2, V_3, V_4)\\), described in matrix format as\n\n\\[\n\\require{mathtools}\n\\definecolor{grey}{RGB}{192, 192, 192}\n\\]\n\n\\[\\begin{align*}\nX = \\begin{bmatrix}\n& {\\color{grey} V_1} & {\\color{grey} V_2} & {\\color{grey} V_3} & {\\color{grey} V_4} \\\\\\hline\n{\\color{grey} a_1} | & x_{11} & x_{12} & x_{13} & x_{14} \\\\\n{\\color{grey} a_2} | & x_{21} & x_{22} & x_{23} & x_{24} \\\\\n{\\color{grey} a_3} | & x_{31} & x_{32} & x_{33} & x_{34}    \n\\end{bmatrix}\n=  \\begin{bmatrix}\n& {\\color{grey} V_1} & {\\color{grey} V_2} & {\\color{grey} V_3} & {\\color{grey} V_4} \\\\\\hline\n{\\color{grey} a_1} | & 7.3 & 7.6 & 7.7 & 8.0 \\\\\n{\\color{grey} a_2} | & 7.4 & 7.2 & 7.3 & 7.2 \\\\\n{\\color{grey} a_3} | & 4.1 & 4.6 & 4.6 & 4.8\n\\end{bmatrix}\n\n\\end{align*}\\]\nwhich is plotted in Figure 2. The Euclidean distance between two cases (rows of the matrix) with \\(p\\) elements is defined as\n\\[\\begin{align*}\nd_{\\rm Euc}(a_i,a_j) &=& ||a_i-a_j|| %\\\\\n% &=& \\sqrt{(x_{i1}-x_{j1})^2+\\dots + (x_{ip}-x_{jp})^2},\n~~~~~~i,j=1,\\dots, n,\n\\end{align*}\\]\nwhere \\(||x_i||=\\sqrt{x_{i1}^2+x_{i2}^2+\\dots +x_{ip}^2}\\). For example, the Euclidean distance between cases 1 and 2 in the above data, is\n\\[\\begin{align*}\nd_{\\rm Euc}(a_1,a_2) &= \\sqrt{(7.3-7.4)^2+(7.6-7.2)^2+ (7.7-7.3)^2+(8.0-7.2)^2} \\\\\n&= 1.0\n\\end{align*}\\]\n\nFor the three cases, the interpoint Euclidean distance matrix is\n\\[\\begin{align*}\nd_{\\rm Euc} =\n\\left[ \\begin{array}{ccc}\n0.0  ~&     &   \\\\\n1.0 ~&  0.0 ~  &  \\\\\n6.3 ~& 5.5 ~&  0.0 ~ \\\\\n\\end{array} \\right]\n\\begin{array}{r}\na_1 \\\\ a_2 \\\\ a_3 \\\\\n\\end{array}\n\\end{align*}\\]\n\n\n\n\n\nCode\nx <- tibble::tibble(V1 = c(7.3, 7.4, 4.1),\n                    V2 = c(7.6, 7.2, 4.6),\n                    V3 = c(7.7, 7.3, 4.6),\n                    V4 = c(8.0, 7.2, 4.8),\n                    point = factor(c(\"a1\", \"a2\", \"a3\")))\nlibrary(GGally)\nlibrary(colorspace)\nlibrary(gridExtra)\npscat <- ggpairs(x, columns=1:4,\n                 upper=list(continuous=\"points\"),\n                 diag=list(continuous=\"blankDiag\"),\n                 axisLabels=\"internal\",\n                 ggplot2::aes(colour=point)) +\n    scale_colour_discrete_qualitative(\n      palette = \"Dark 3\") +\n    theme(aspect.ratio=1)\npscat\n\n\n\n\n\n\n\nCode\nppar <- ggparcoord(x, columns=1:4, \n                   groupColumn = 5, \n                   scale = \"globalminmax\") +\n          scale_colour_discrete_qualitative(\n            palette = \"Dark 3\") +\n  xlab(\"\") + ylab(\"\") + \n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank(),\n        legend.title = element_blank())\nppar\n\n\n\n\n\n\nFigure 2: The scatterplot matrix (left) shows that cases \\(a_1\\) and \\(a_2\\) have similar values. The parallel coordinate plot (right) allows a comparison of other structure, which shows the similarity in the trend of the profiles on cases \\(a_1\\) and \\(a_3\\).\n\n\nCases \\(a_1\\) and \\(a_2\\) are more similar to each other than they are to case \\(a_3\\), because the Euclidean distance between cases \\(a_1\\) and \\(a_2\\) is much smaller than the distance between cases \\(a_1\\) and \\(a_3\\) and between cases \\(a_2\\) and \\(a_3\\).\nThere are many different ways to calculate similarity. Similarity measures based on correlation distance have become common. Correlation distance is typically used where similarity of structure is more important than similarity in magnitude.\n\nAs an example, see the parallel coordinate plot of the sample data at the right of Figure 2. Cases \\(a_1\\) and \\(a_3\\) are widely separated, but their shapes are similar (low, medium, medium, high). Case \\(a_2\\), although overlapping with Case \\(a_1\\), has a very different shape (high, medium, medium, low). The correlation between two cases is defined as\n\\[\\begin{align*}\n\\rho(a_i,a_j) = \\frac{(a_i-c_i)'(a_j-c_j)}\n{\\sqrt{(a_i-c_i)'(a_i-c_i)} \\sqrt{(a_j-c_j)'(a_j-c_j)}}\n\\label{corc}\n\\end{align*}\\]\nWhen \\(c_i, c_j\\) are the sample means \\(\\bar{a}_i,\\bar{a}_j\\), then \\(\\rho\\) is the Pearson correlation coefficient. If, indeed, they are set at 0, as is commonly done, \\(\\rho\\) is a generalized correlation that describes the angle between the two data vectors. The correlation is then converted to a distance metric; one equation for doing so is as follows:\n\\[\\begin{align*}\nd_{\\rm Cor}(a_i,a_j) = \\sqrt{2(1-\\rho(a_i,a_j))}\n\\end{align*}\\]\nThe above distance metric will treat cases that are strongly negatively correlated as the most distant.\nThe interpoint distance matrix for the sample data using \\(d_{\\rm Cor}\\) and the Pearson correlation coefficient is\n\\[\\begin{align*}\nd_{\\rm Cor} =\n\\left[ \\begin{array}{rrrrrrrrr}\n0.0  ~&     &  \\\\\n3.6 ~ & 0.0 ~ &  \\\\\n0.1 ~ & 3.8 ~ &  0.0 ~\\\\\n\\end{array} \\right]\n\\begin{array}{r}\na_1 \\\\ a_2 \\\\ a_3 \\\\\n\\end{array}\n\\end{align*}\\]\nBy this metric, cases \\(a_1\\) and \\(a_3\\) are the most similar, because the correlation distance is smaller between these two cases than the other pairs of cases. \nNote that these interpoint distances differ dramatically from those for Euclidean distance. As a consequence, the way the cases would be clustered is also be very different. Choosing the appropriate distance measure is an important part of a cluster analysis.\nAfter a distance metric has been chosen and a cluster analysis has been performed, the analyst must evaluate the results, and this is actually a difficult task. A cluster analysis does not generate \\(p\\)-values or other numerical criteria, and the process tends to produce hypotheses rather than testing them. Even the most determined attempts to produce the “best” results using modeling and validation techniques may result in clusters that, although seemingly significant, are useless for practical purposes. As a result, cluster analysis is best thought of as an exploratory technique, and it can be quite useful despite the lack of formal validation because of its power in data simplification.\nThe context in which the data arises is the key to assessing the results. If the clusters can be characterized in a sensible manner, and they increase our knowledge of the data, then we are on the right track. To use an even more pragmatic criterion, if a company can gain an economic advantage by using a particular clustering method to carve up their customer database, then that is the method they should use.\n\n\n\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning with r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377.\n\n\nGiordani, Paolo, Maria Brigida Ferraro, and Francesca Martella. 2020. An Introduction to Clustering with r. Springer Singapore. https://doi.org/10.1007/978-981-13-0553-5.\n\n\nHennig, Christian, Marina Meila, Fionn Murtagh, and Roberto Rocci, eds. 2015. Handbook of Cluster Analysis. Chapman; Hall/CRC. https://doi.org/10.1201/b19706.\n\n\nKassambara, Alboukadel. 2017. Practical Guide to Cluster Analysis in r: Unsupervised Machine Learning. STHDA.\n\n\nLeisch, Friedrich, and Bettina Gruen. 2023. “CRAN Task View: Cluster Analysis & Finite Mixture Models.” https://cran.r-project.org/web/views/Cluster.html.\n\n\nVenables, W. N., and B. Ripley. 2002. Modern Applied Statistics with S. New York: Springer-Verlag."
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "Supervised learning",
    "section": "",
    "text": "This chapter will methods for visualising data and models when there is a response variable."
  },
  {
    "objectID": "som.html",
    "href": "som.html",
    "title": "11  Self-organizing maps",
    "section": "",
    "text": "A self-organizing map (SOM) is constructed using a constrained \\(k\\)-means algorithm. A 1D or 2D net is stretched through the data. The knots, in the net, form the cluster means, and the points closest to the knot are considered to belong to that cluster. The similarity of nodes (and their corresponding clusters) is defined as proportional to their distance from one another on the net.\nWe will demonstrate SOM using the music data. The data has 62 cases, each one corresponding to a piece of music. For each piece there are seven variables: the artist, the type of music, and five characteristics, based on amplitude and frequency, that were computed using the first 40 seconds of the piece on CD. The music used included popular rock songs by Abba, the Beatles, and the Eels; classical compositions by Vivaldi, Mozart and Beethoven; and several new wave pieces by Enya. Figure~\\(\\ref{music-som}\\) displays a typical view of the results of clustering using SOM on the music data. Each data point corresponds to a piece of music and is labeled by the band or the composer. The map was generated by this R code:\n> library(som)\n> d.music <- read.csv(\"music-sub.csv\", row.names=1)\n> d.music.std <- cbind(subset(d.music.std,\n   select=c(artist,type)),\n   apply(subset(d.music.std,select=lvar:lfreq), \n   2, f.std.data))\n> music.som <- som(subset(d.music.std,select=lvar:lfreq), \n   6, 6, neigh=\"bubble\", rlen=1000)\nThe left plot in ?fig-music-som is called the 2D map view. Here we have used a \\(6\\times 6\\) net pulled through the 5D data. The net that was wrapped through the high-dimensional space is straightened and laid out flat, and the points, like fish in a fishing net, are laid out where they have been trapped. In the plot shown here, the points have been jittered slightly, away from the knots of the net, so that the labels do not overlap too much. If the fit is good, the points that are close together in this 2D map view are close together in the high-dimensional data space and close to the net as it was placed in the high-dimensional space.\nMuch of the structure in the map is no surprise: The rock (purple) and classical tracks (green) are on opposing corners, with rock in the upper right and classical in the lower left. The Abba tracks are all grouped at the top and left of the map. The Beatles and Eels tracks are mixed. There are some unexpected associations: For example, one Beatles song, which turns out to be “Hey Jude”, is mixed among the classical compositions!\n\nConstruction of a self-organizing map is a dimension reduction method, akin to multidimensional scaling Borg and Groenen (2005) or principal component analysis Johnson and Wichern (2002). Using principal component analysis to find a low-dimensional approximation of the similarity between music pieces, yields the second plot in ?fig-music-som. There are many differences between the two representations. The SOM has a more even spread of music pieces across the grid, in contrast to the stronger clumping of points in the PCA view. Indeed, the PCA view shows several outliers, notably one of the Vivaldi compositions, which could lead us to learn things about the data that we might miss by relying exclusively on the SOM. \nThese two methods, SOM and PCA, have provided two contradictory clustering models. How can we determine which is the more accurate description of the data structure? An important part of model assessment is plotting the model in relation to the data. Although plotting the low-dimensional map is the common way to graphically assess the SOM results, it is woefully limited. If a model has flaws, they may not show up in this view and will only appear in plots of the model in the data space. We will use the grand tour to create these plots, and this will help us assess the two models.\nWe will use a grand tour to view the net wrapped in among the data, hoping to learn how the net converged to this solution, and how it wrapped through the data space. Actually, it is rather tricky to fit a SOM: Like many algorithms, it has a number of parameters and initialization conditions that affect the outcome.\nTo set up the data, we will need to add variables containing the map coordinates to the data:\n> d.music.som <- f.ggobi.som(subset(d.music.std,\n   select=lvar:lfreq), music.som)\nBecause this data has several useful categorical labels for each row, we will want to keep this information in the data when it is loaded into the tour:\n> d.music.som <- data.frame(\n  Songs=factor(c(as.character(row.names(d.music)),\n    rep(\"0\",36))),\n  artist=factor(c(as.character(d.music[,1]),rep(\"0\",36))),\n  type=factor(c(as.character(d.music[,2]),rep(\"0\",36))),\n  lvar=d.music.som[,1], \n  lave=d.music.som[,2], \n  lmax=d.music.som[,3],\n  lfener=d.music.som[,4], lfreq=d.music.som[,5],\n  Map.1=d.music.som[,6], Map.2=d.music.som[,7])\n> gd <- ggobi(d.music.som)[1]\nAdd the edges that form the SOM net:\n> d.music.som.net <- f.ggobi.som.net(music.som)\n> edges(gd) <- d.music.som.net + 62\nAnd finally color the points according to the type of music:\n> gcolor <- rep(8,98)\n> gcolor[d.music.som$Type==\"Rock\"] <- 6\n> gcolor[d.music.som$Type==\"Classical\"] <- 4\n> gcolor[d.music.som$Type==\"New wave\"] <- 1\n> glyph_color(g) <- gcolor\nThe results can be seen in ?fig-clust-SOMa and ?fig-clust-SOMb. The plots show two different states of the fitting process and of the SOM net cast through the data. In both fits, a \\(6\\times 6\\) grid is used and the net is initialized in the direction of the first two principal components. In both fits the variables were standardized to have mean equal to zero and standard deviation equal to 1. The first SOM fit, shown in ?fig-clust-SOMa, was obtained using the default settings; it gave terrible results. At the left is the map view, in which the fit looks deceptively reasonable. The points are spread evenly through the grid, with rock tracks (purple) at the upper right, classical tracks (green) at the lower left, and new wave tracks (the three black rectangles) in between. The tour view in the same figure, however, shows the fit to be inadequate. The net is a flat rectangle in the 5D space and has not sufficiently wrapped through the data. This is the result of stopping the algorithm too soon, thus failing to let it converge fully.\n\n?fig-clust-SOMb shows our favorite fit to the data. The data was standardized, we used a 6$$6 net, and we ran the SOM algorithm for 1,000 iterations. The map is at the top left, and it matches the map already shown in ?fig-music-som, except for the small jittering of points in the earlier figure. The other three plots show different projections from the grand tour. The upper right plot shows how the net curves with the nonlinear dependency in the data: The net is warped in some directions to fit the variance pattern. At the bottom right we see that one side of the net collects a long separated cluster of the Abba tracks. We can also see that the net has not been stretched out to the full extent of the range of the data. It is tempting to manually manipulate the net to stretch it in different directions and update the fit.\nIt turns out that the PCA view of the data more accurately reflects the structure in the data than the map view. The music pieces really are clumped together in the 5D space, and there are a few outliers. \n\n\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern Multidimensional Scaling. New York: Springer.\n\n\nJohnson, R. A., and D. W. Wichern. 2002. Applied Multivariate Statistical Analysis (5th Ed). Englewood Cliffs, NJ: Prentice-Hall."
  },
  {
    "objectID": "unsupervised-summary.html#chap-clust-recap",
    "href": "unsupervised-summary.html#chap-clust-recap",
    "title": "12  Comparing methods",
    "section": "12.1 Recap",
    "text": "12.1 Recap\nGraphics are invaluable for cluster analysis, whether they are used to find clusters or to interpret and evaluate the results of a cluster analysis arrived at by other means.\nThe spin and brush approach can be used to get an initial look at the data and to find clusters, and occasionally, it is sufficient. When the clustering is the result of an algorithm, a very useful first step is to paint the points by cluster membership and to look at the data to see whether the clustering seems sensible. How many clusters are there, and how big are they? What shape are they, and do they overlap one another? Which variables have contributed most to the clustering? Can the clusters be qualitatively described? All the plots we have described can be useful: scatterplots, parallel coordinate plots, and area plots, as well as static plots like dendrograms.\nWhen the clusters have been generated by a model, we should also use graphics to help us assess the model. If the model makes distributional assumptions, we can generate ellipses and compare them with the clusters to see whether the shapes are consistent. For self-organizing maps the tour can assist in uncovering problems with the fit, such as when the map wraps in on itself through the data making it appear that some cases are far apart when they are truly close together. A confusion table can come alive with linked brushing, so that mismatches and agreements between methods can be explored. \n\n\n\n\nDasu, T., D. F. Swayne, and D. Poole. 2005. “Grouping Multivariate Time Series: A Case Study.” In Proceedings of the IEEE Workshop on Temporal Data Mining: Algorithms, Theory and Applications, in Conjunction with the Conference on Data Mining, Houston, November 27, 2005, 25–32. IEEE Computer Society."
  },
  {
    "objectID": "model-based-clustering.html#overview",
    "href": "model-based-clustering.html#overview",
    "title": "11  Model-based clustering",
    "section": "11.1 Overview",
    "text": "11.1 Overview\nModel-based clustering Fraley and Raftery (2002) fits a multivariate normal mixture model to the data. It uses the EM algorithm to fit the parameters for the mean, variance–covariance of each population, and the mixing proportion. The variance-covariance matrix is re-parametrized using an eigen-decomposition\n\\[\n\\Sigma_k = \\lambda_kD_kA_kD_k', ~~~k=1, \\dots, g ~~\\mbox{(number of clusters)}\n\\]\nresulting in several model choices, ranging from simple to complex, as shown in Table 11.1.\n\n\nCode\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(mclust)\nlibrary(mulgar)\nlibrary(patchwork)\nlibrary(colorspace)\nlibrary(tourr)\n\n\n\n\n\n\nTable 11.1:  Parameterizations of the covariance matrix. \n \n  \n    Model \n    Sigma \n    Family \n    Volume \n    Shape \n    Orientation \n  \n \n\n  \n    EII \n    $$\\lambda I$$ \n    Spherical \n    Equal \n    Equal \n    NA \n  \n  \n    VII \n    $$\\lambda_k I$$ \n    Spherical \n    Variable \n    Equal \n    NA \n  \n  \n    EEI \n    $$\\lambda A$$ \n    Diagonal \n    Equal \n    Equal \n    Coordinate axes \n  \n  \n    VEI \n    $$\\lambda_kA$$ \n    Diagonal \n    Variable \n    Equal \n    Coordinate axes \n  \n  \n    EVI \n    $$\\lambda A_k$$ \n    Diagonal \n    Equal \n    Variable \n    Coordinate axes \n  \n  \n    VVI \n    $$\\lambda_k A_k$$ \n    Diagonal \n    Variable \n    Variable \n    Coordinate axes \n  \n  \n    EEE \n    $$\\lambda DAD^T$$ \n    Diagonal \n    Equal \n    Equal \n    Equal \n  \n  \n    EVE \n    $$\\lambda DA_kD^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Equal \n  \n  \n    VEE \n    $$\\lambda_k DAD^T$$ \n    Ellipsoidal \n    Variable \n    Equal \n    Equal \n  \n  \n    VVE \n    $$\\lambda_k DA_kD^T$$ \n    Ellipsoidal \n    Variable \n    Equal \n    Equal \n  \n  \n    EEV \n    $$\\lambda D_kAD_k^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Variable \n  \n  \n    VEV \n    $$\\lambda_k D_kAD_k^T$$ \n    Ellipsoidal \n    Variable \n    Variable \n    Variable \n  \n  \n    EVV \n    $$\\lambda D_kA_kD_k^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Variable \n  \n  \n    VVV \n    $$\\lambda_k D_kA_kD_k^T$$ \n    Ellipsoidal \n    Variable \n    Variable \n    Variable \n  \n\n\n\n\n\n\nNote the distribution descriptions “spherical” and “ellipsoidal”. These are descriptions of the shape of the variance-covariance for a multivariate normal distribution. A standard multivariate normal distribution has a variance-covariance matrix with zeros in the off-diagonal elements, which corresponds to spherically shaped data. When the variances (diagonals) are different or the variables are correlated, then the shape of data from a multivariate normal is ellipsoidal.\n\nThe models are typically scored using the Bayes Information Criterion (BIC), which is based on the log likelihood, number of variables, and number of mixture components. They should also be assessed using graphical methods, as we demonstrate using the data."
  },
  {
    "objectID": "model-based-clustering.html#two-variables",
    "href": "model-based-clustering.html#two-variables",
    "title": "11  Model-based clustering",
    "section": "11.2 Two variables",
    "text": "11.2 Two variables\nWe start with two of the four real-valued variables (bl, fl) and the three species. The goal is to determine whether model-based methods can discover clusters that closely correspond to the three species. Based on the scatterplot in Figure 11.1 we would expect it to do well, and suggest an elliptical variance-covariance of roughly equal sizes as the model.\n\n\nCode\nload(\"data/penguins_sub.rda\")\nggplot(penguins_sub, aes(x=bl, \n                         y=fl)) + #, \n                         #colour=species)) +\n  geom_point() +\n  geom_density2d() +\n  #scale_color_discrete_qualitative(\"Dark 3\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\nFigure 11.1: Scatterplot of flipper length by bill length of the penguins data.\n\n\n\n\n\n\nCode\npenguins_BIC <- mclustBIC(penguins_sub[,c(1,3)])\nggmc <- ggmcbic(penguins_BIC, cl=2:9, top=4) + ggtitle(\"(a)\")\npenguins_mc <- Mclust(penguins_sub[,c(1,3)], \n                      G=3, \n                      modelNames = \"EVE\")\npenguins_mce <- mc_ellipse(penguins_mc)\npenguins_cl <- penguins_sub[,c(1,3)]\npenguins_cl$cl <- factor(penguins_mc$classification)\nggell <- ggplot() +\n   geom_point(data=penguins_cl, aes(x=bl, y=fl,\n                                    colour=cl),\n              alpha=0.3) +\n   geom_point(data=penguins_mce$ell, aes(x=bl, y=fl,\n                                         colour=cl),\n              shape=16) +\n   geom_point(data=penguins_mce$mn, aes(x=bl, y=fl,\n                                        colour=cl),\n              shape=3, size=2) +\n  scale_color_discrete_qualitative(palette = \"Dark 3\") +\n   theme(aspect.ratio=1, legend.position=\"none\") +\n  ggtitle(\"(b)\")\nggmc + ggell + plot_layout(ncol=2)\n\n\n\n\n\nFigure 11.2: Summary plots from model-based clustering: (a) BIC values for clusters 2-9 of top four models, (b) variance-covariance ellipses and cluster means (+) corresponding to the best model. The best model is three-cluster EVE, which has differently shaped variance-covariances albeit the same volume and orientation.\n\n\n\n\nFigure 11.2 summarises the results. All models agree that three clusters is the best. The different variance-covariance models for three clusters have similar BIC values with EVE (different shape, same volume and orientation) being slightly higher. These plots are made from the mclust package output using the ggmcbic and mc_ellipse functions fro the mulgar package."
  },
  {
    "objectID": "model-based-clustering.html#high-dimensions",
    "href": "model-based-clustering.html#high-dimensions",
    "title": "11  Model-based clustering",
    "section": "11.3 High-dimensions",
    "text": "11.3 High-dimensions\nNow we will examine how model-based clustering will group the penguins data using all four variables.\n\n\nCode\npenguins_BIC <- mclustBIC(penguins_sub[,1:4])\nggmc <- ggmcbic(penguins_BIC, cl=2:9, top=7) \nggmc\n\n\n\n\n\nFigure 11.3: BIC values for the top models for 2-9 clusters on the penguins data. The interpretation is mixed: if one were to choose three clusters any of the variance-covariance models would be equally as good, but the very best model is the four-cluster VEE.\n\n\n\n\n\n\nCode\npenguins_mc <- Mclust(penguins_sub[,1:4], \n                      G=4, \n                      modelNames = \"VEE\")\npenguins_mce <- mc_ellipse(penguins_mc)\npenguins_cl <- penguins_sub\npenguins_cl$cl <- factor(penguins_mc$classification)\n\npenguins_mc_data <- penguins_cl %>%\n  select(bl:bm, cl) %>%\n  mutate(type = \"data\") %>%\n  bind_rows(bind_cols(penguins_mce$ell,\n                      type=rep(\"ellipse\",\n                               nrow(penguins_mce$ell)))) %>%\n  mutate(type = factor(type))\n\nanimate_xy(penguins_mc_data[,1:4],\n           col=penguins_mc_data$cl,\n           pch=c(4, 20 )[as.numeric(penguins_mc_data$type)], \n           axes=\"off\")\n\n# \nload(\"data/penguins_tour_path.rda\")\nrender_gif(penguins_mc_data[,1:4], \n           planned_tour(pt1), \n           display_xy(col=penguins_mc_data$cl,\n               pch=c(4, 20)[\n                 as.numeric(penguins_mc_data$type)], \n                      axes=\"off\",\n               half_range = 0.7),\n           gif_file=\"gifs/penguins_best_mc.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\nCode\npenguins_mc <- Mclust(penguins_sub[,1:4], \n                      G=3, \n                      modelNames = \"EEE\")\npenguins_mce <- mc_ellipse(penguins_mc)\npenguins_cl <- penguins_sub\npenguins_cl$cl <- factor(penguins_mc$classification)\n\npenguins_mc_data <- penguins_cl %>%\n  select(bl:bm, cl) %>%\n  mutate(type = \"data\") %>%\n  bind_rows(bind_cols(penguins_mce$ell,\n                      type=rep(\"ellipse\",\n                               nrow(penguins_mce$ell)))) %>%\n  mutate(type = factor(type))\n\nanimate_xy(penguins_mc_data[,1:4],\n           col=penguins_mc_data$cl,\n           pch=c(4, 20)[as.numeric(penguins_mc_data$type)], \n           axes=\"off\")\n\n# Save the animated gif\nload(\"data/penguins_tour_path.rda\")\nrender_gif(penguins_mc_data[,1:4], \n           planned_tour(pt1), \n           display_xy(col=penguins_mc_data$cl,\n               pch=c(4, 20)[\n                 as.numeric(penguins_mc_data$type)], \n                      axes=\"off\",\n               half_range = 0.7),\n           gif_file=\"gifs/penguins_simpler_mc.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\n\n\n\n\n(a) Best model: four-cluster VEE\n\n\n\n\n\n\n\n(b) Three-cluster EEE\n\n\n\n\nFigure 11.4: Examining the model-based clustering results for the penguins data: (a) best model according to BIC value, (b) simpler three-cluster model. Dots are ellipse points, and “x” are data points. It is important to note that the three cluster solution fits the data better, even though it has a lower BIC.\n\n\n\n\n\n\n\n\n\n\n\nFraley, C., and A. E. Raftery. 2002. “Model-Based Clustering, Discriminant Analysis, Density Estimation.” Journal of the American Statistical Association 97: 611–31."
  },
  {
    "objectID": "toolbox.html",
    "href": "toolbox.html",
    "title": "Appendix A — Toolbox",
    "section": "",
    "text": "Description and explanation of primary methods used throughout the book. Mostly focusing on tour methods.\nWe need a really nice friendly introduction to tour methods:\n\n2D to 1D projections as basic explanation\nmaybe 3D to 2D, or even 4D to 2D illustration\n\nShow examples of what can be gained from looking at combinations as opposed to pairs plots, for example. Clusters is a good situation, but also collinearity and outliers\nExplain data needed for input, what happens if discrete data is included, or time series\nWorking with many dimensions, how to adapt\nGrand tour, and show paths on the space, starting with 1D. How more time gives more coverage of the sphere. Then the torus and paths on torus.\nDifferent types of tours, and when to use them.\nAnd finally how to save tours, and make plot of single projection\nNeed to include - half_range - standardizing variables"
  },
  {
    "objectID": "forests.html#trees",
    "href": "forests.html#trees",
    "title": "6  Trees and forests",
    "section": "6.1 Trees",
    "text": "6.1 Trees\nThe tree algorithm Breiman et al. (1984) is a long-established versatile algorithmic method for supervised classification. The basic tree algorithm generates a classification rule by sequentially splitting the data into two buckets. Splits are made between sorted data values of individual variables, with the goal of obtaining pure classes on each side of the split. The inputs for a simple tree classifier commonly include (1) an impurity measure, an indication of the relative diversity among the cases in the terminal nodes; (2) a parameter that sets the minimum number of cases in a node, or the minimum number of observations in a terminal node of the tree; and (3) a complexity measure that controls the growth of a tree, balancing the use of a simple generalizable tree against a more accurate tree tailored to the sample. When applying tree methods, exploring the effects of the input parameters on the tree is instructive; for example, it helps us to assess the stability of the tree model.\nAlthough algorithmic models do not depend on distributional assumptions, that does not mean that every algorithm is suitable for all data. For example, the tree model works best when all variables are independent within each class, because it does not take such dependencies into account. Visualization can help us to determine whether a particular model should be applied. In classification problems, it is useful to explore the cluster structure, comparing the clusters with the classes and looking for evidence of correlation within each class. The upper left-hand plot in ?fig-lda-assumptions shows a strong correlation between tars1 and tars2 within each cluster, which indicates that the tree model may not give good results for the . The plots in ?fig-misclassifications provide added evidence. They use background color to display the class predictions for LDA and a tree. The LDA boundaries, which are formed from a linear combination of tars1 and tars2, look more appropriate than the rectangular boundaries of the tree classifier.\n\nHastie, Tibshirani, and Friedman (2001) and Bishop (2006) include thorough discussions of algorithms for supervised classification presented from a modeling perspective with a theoretical emphasis. Ripley (1996) is an early volume describing and illustrating both classical statistical methods and algorithms for supervised classification. All three books contain some excellent examples of the use of graphics to examine two-dimensional (2D) boundaries generated by different classifiers. The discussions in these and other writings on data mining algorithms take a less exploratory approach than that of this chapter, and they lack treatments of the use of graphics to examine the high-dimensional spaces in which the classifiers operate.\n\n6.1.1 Studying the fit\n \nA classifier’s performance is usually assessed using its error or, conversely, its accuracy. Error is calculated by comparing the predicted class with the known true class, using a misclassification table. For example, below are the respective misclassification tables for LDA and the tree classifier applied to the :\n\nThe total error is the number of misclassified samples divided by the total number of cases: \\(4/74=0.054\\) for LDA and \\(5/74=0.068\\) for the tree classifier.\nIt is informative to study the misclassified cases and to see which pockets of the data space contain more error. The misclassified cases for LDA and tree classifiers are highlighted (large orange \\(\\times\\)es and large green circles) in ?fig-misclassifications. Some errors made by the tree classifier, such as the uppermost large green circle, seem especially egregious. As noted earlier, they result from the limitations of the algorithm when variables are correlated.\n\nTo be useful, the error estimate should predict the performance of the classifier on new samples not yet seen. However, if the error is calculated using the same data that was used by the classifier, it is likely to be too low. Many methods are used to avoid double-dipping from the data, including several types of . A simple example of cross-validation is to split the data into a training sample (used by the classifier) and a test sample (used for calculating error).\n \nEnsemble methods build cross-validation into the error calculations. Ensembles are constructed by using multiple classifiers and by pooling the predictions using a voting scheme. A random forest Breiman and Cutler (2004), for example, builds in cross-validation by constructing multiple trees, each of which is generated by randomly sampling the input variables and the cases. Because each tree is built using a sample of the cases, there is in effect a training sample and a test sample for each tree. (See sec-random-forests for more detail.)"
  },
  {
    "objectID": "forests.html#random-forests",
    "href": "forests.html#random-forests",
    "title": "6  Trees and forests",
    "section": "6.2 Random forests",
    "text": "6.2 Random forests\n\n\n\nA random forest Breiman and Cutler (2004) is a classifier that is built from multiple trees generated by randomly sampling the cases and the variables. The random sampling (with replacement) of cases has the fortunate effect of creating a training (in-bag'') and a test (out-of-bag’’) sample for each tree computed. The class of each case in the out-of-bag sample for each tree is predicted, and the predictions for all trees are combined into a vote for the class identity.\nA random forest is a computationally intensive method, a ``black box’’ classifier, but it produces various diagnostics that make the outcome less mysterious. Some diagnostics that help us to assess the model are the votes, the measures of variable importance, the error estimate, and as usual, the misclassification tables.\n\nWe test the method on the by building a random forest classifier of 500 trees, using the R package randomForest Liaw et al. (2006):\n\n\n\nEach tree used a random sample of four of the eight variables, as well as a random sample of about a third of the 572 cases. The votes are displayed in the left-hand plot of ?fig-olive-forest, next to a projection from a 2D tour. Since there are three classes, the votes form a triangle, with one vertex for each region, with oils from the South at the far right, Sardinian oils at the top, and Northern oils at the lower left. Samples that are consistently classified correctly are close to the vertices; cases that are commonly misclassified are further from a vertex. Although forests perfectly classify this data, the number of points falling between the Northern and the Sardinian vertices suggests some potential for error in classifying future samples.\nFor more understanding of the votes, we turn to another diagnostic: variable importance. Forests return two measures of variable importance, both of which give similar results. Based on the Gini measure, the most important variables, in order, are eicosenoic, linoleic, oleic, palmitic, arachidic, palmitoleic, linolenic, and stearic.\nSome of this ordering is as expected, given the initial graphical inspection of the data (?sec-class-plots). The importance of eicosenoic was our first discovery, as shown in the top row of ?fig-olive-1d. And yes, linoleic is next in importance: The first two plots in ?fig-olive-2d make that clear. The surprise is that the forest should consider arachidic to be less important than palmitic. This is not what we found, as shown in the right-hand plot in that figure.\nDid we overlook something important in our earlier investigation? We return to the use of the manual manipulation of the tour to see whether palmitic does in fact perform better than arachidic at finding a gap between the two regions. But it does not. By overlooking the importance of arachidic, the random forest never finds an adequate gap between the oils of the Northern and the Sardinian regions, and that probably explains why there is more confusion about some Northern samples than there should be.\nWe rebuild the forest using a new variable constructed from a linear combination of linoleic and arachidic (linoarach), just as we did when applying the single tree classifier. Since correlated variables reduce each other’s importance, we need to remove linoleic and oleic when we add linoarach. Once we have done this, the confusion between Northern and Sardinian oils disappears ?fig-olive-forest, lower plot): The points are now tightly clumped at each vertex, which indicates more certainty in their class predictions. The new variable becomes the second most important variable according to the importance diagnostic.\nClassifying the oils by the three large regions is too easy a problem for forests; they are designed to tackle more challenging classification tasks. We will use them to examine the oils from the areas in the Southern region (North and South Apulia, Calabria, and Sicily). Remember the initial graphical inspection of the data, which showed that oils from the four areas were not completely separable. The samples from Sicily overlapped those of the three other areas. We will use a forest classifier to see how well it can differentiate the Southern oils by area:\nAfter experimenting with several input parameters, we show the results for a forest of 1,500 trees, sampling two variables at each tree node, and yielding an error rate of 0.068. The misclassification table is:\n\nThe error of the forest is surprisingly low, but the error is definitely not uniform across classes. Predictions for Sicily are wrong about a third of the time. Figure~\\(\\ref{olive-forest2}\\) shows some more interesting aspects of the results. For this figure, the following table describes the correspondence between area and symbol:\n\n\nLook first at the top row of the figure. The misclassification table is represented by a jittered scatterplot, at the left. A plot from a 2D tour of the four voting variables is in the center. Because there are four groups, the votes lie on a 3D tetrahedron (a simplex). The votes from three of the areas are pretty well separated, one at each ``corner,’’ but those from Sicily overlap all of them. Remember that when points are clumped at the vertex, class members are consistently predicted correctly. Since this does not occur for Sicilian oils, we see that there is more uncertainty in the predictions for this area.\nThe plot at right confirms this observation. It is a projection from a 2D tour of the four most important variables, showing a pattern we have seen before. We can achieve pretty good separation of the oils from North Apulia, Calabria, and South Apulia, but the oils from Sicily overlap all three clusters. Clearly these are tough samples to classify correctly.\n\n\nWe remove the Sicilian oils from the plots so we can focus on the other three areas (bottom row of plots). The points representing North Apulian oils form a very tight cluster at a vertex, with three exceptions. Two of these points are misclassified as Calabrian, and we have highlighted them as large filled circles by painting the misclassification plot.\nThe pattern of the votes (middle plot) suggests that there is high certainty in the predictions for North Apulian oils, with the exception of these two samples. When we watch the votes in the tour for a while, we see that the votes of these two samples travel as if they were in a cluster all their own, which is distinct from the remaining North Apulian oils.\nHowever, when we look at the data, we find the votes for these two samples a bit puzzling. We watch the four most important variables in the tour for a while (as in the right plot), and these two points do not behave as if they were in a distinct cluster; they travel with the rest of the samples from North Apulia. They do seem to be outliers with respect their class, but they are not so far from their group - it is a bit surprising that the forest has trouble classifying these cases.\nRather than exploring the other misclassifications, we leave that for the reader.\nIn summary, a random forest is a useful method for tackling tough classification problems. Its diagnostics provide a rich basis for graphical exploration, which helps us to digest and evaluate the solution."
  },
  {
    "objectID": "forests.html#exercises",
    "href": "forests.html#exercises",
    "title": "6  Trees and forests",
    "section": "Exercises",
    "text": "Exercises\n\nFor the :\n\nSplit the samples from North Italy into \\(2/3\\) training and \\(1/3\\) test samples for each area.\nBuild a tree model to classify the oils by area for the three areas of North Italy. Which are the most important variables? Make plots of these variables. What is the accuracy of the model for the training and test sets?\nBuild a random forest to classify oils into the three areas of North Italy. Compare the order of importance of variables with what you found from a single tree. Make a parallel coordinate plot in the order of variable importance.\n\n\n\n\n\n\nBishop, C. M. 2006. Pattern Recognition and Machine Learning. New York: Springer.\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.\n\n\nBreiman, L., and A. Cutler. 2004. “Random Forests.” http://www.math.usu.edu/\\(\\sim\\)adele/forests/cc_home.htm.\n\n\nBreiman, L., J. Friedman, C. Olshen, and C. Stone. 1984. Classification and Regression Trees. Monterey, CA: Wadsworth; Brooks/Cole.\n\n\nHastie, T., R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical Learning. New York: Springer.\n\n\nLiaw, A., M. Wiener, L. Breiman, and A. Cutler. 2006. “randomForest: Breiman and Cutler’s Random Forests for Classification and Regression.” http://www.R-project.org.\n\n\nRipley, B. 1996. Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press."
  },
  {
    "objectID": "data.html#bushfires",
    "href": "data.html#bushfires",
    "title": "Appendix B — Data",
    "section": "B.3 Bushfires",
    "text": "B.3 Bushfires\n\nDescription\nThis data was collated by Weihao (Patrick) Li as part of his Honours research at Monash University. It contains fire ignitions as detected from satellite hotspots, and processed using the spotoroo package, augmented with measurements on weather, vegetation, proximity to human activity. The cause variable is predicted based on historical fire ignition data collected by County Fire Authority personnel.\n\n\nVariables\n\n\nRows: 1,021\nColumns: 60\n$ id            <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ lon           <dbl> 141.1300, 141.3000, 141.4800, 147.1600, 148.1050, 144.18…\n$ lat           <dbl> -37.13000, -37.65000, -37.35000, -37.85000, -37.57999, -…\n$ time          <date> 2019-10-01, 2019-10-01, 2019-10-02, 2019-10-02, 2019-10…\n$ FOR_CODE      <dbl> 41, 41, 91, 44, 0, 44, 0, 102, 0, 91, 45, 41, 45, 45, 45…\n$ FOR_TYPE      <chr> \"Eucalypt Medium Woodland\", \"Eucalypt Medium Woodland\", …\n$ FOR_CAT       <chr> \"Native forest\", \"Native forest\", \"Commercial plantation…\n$ COVER         <dbl> 1, 1, 4, 2, 6, 2, 6, 5, 6, 4, 2, 1, 2, 2, 2, 2, 6, 6, 6,…\n$ HEIGHT        <dbl> 2, 2, 4, 2, 6, 2, 6, 5, 6, 4, 3, 2, 3, 3, 3, 2, 6, 6, 6,…\n$ FOREST        <dbl> 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,…\n$ rf            <dbl> 0.0, 0.0, 15.4, 4.8, 6.0, 11.6, 11.6, 0.6, 0.2, 0.6, 0.0…\n$ arf7          <dbl> 5.0857143, 2.4000000, 2.4000000, 0.7142857, 0.8571429, 1…\n$ arf14         <dbl> 2.8142857, 1.7428571, 1.8000000, 1.6714286, 1.5714286, 1…\n$ arf28         <dbl> 1.9785714, 1.5357143, 1.5357143, 3.7857143, 1.9000000, 1…\n$ arf60         <dbl> 2.3033333, 1.7966667, 1.7966667, 4.0000000, 2.5333333, 1…\n$ arf90         <dbl> 1.2566667, 1.0150000, 1.0150000, 2.9600000, 2.1783333, 1…\n$ arf180        <dbl> 0.9355556, 0.8444444, 0.8444444, 2.3588889, 1.7866667, 1…\n$ arf360        <dbl> 1.3644444, 1.5255556, 1.5255556, 1.7272222, 1.4716667, 1…\n$ arf720        <dbl> 1.3011111, 1.5213889, 1.5213889, 1.7111111, 1.5394444, 1…\n$ se            <dbl> 3.8, 4.6, 14.2, 23.7, 23.8, 16.8, 18.0, 12.9, 14.7, 12.9…\n$ ase7          <dbl> 18.02857, 18.50000, 21.41429, 23.08571, 23.11429, 22.014…\n$ ase14         <dbl> 17.03571, 17.44286, 18.03571, 19.17143, 18.45714, 18.628…\n$ ase28         <dbl> 19.32857, 18.47500, 19.33929, 18.23571, 16.86071, 19.375…\n$ ase60         <dbl> 20.38644, 19.99153, 20.39492, 19.90847, 19.26780, 20.449…\n$ ase90         <dbl> 22.54118, 21.93193, 22.04370, 20.59328, 20.04538, 21.809…\n$ ase180        <dbl> 20.79106, 19.93966, 19.99385, 19.11006, 18.66760, 19.810…\n$ ase360        <dbl> 15.55153, 14.83259, 14.87883, 14.69276, 14.44318, 14.755…\n$ ase720        <dbl> 15.52350, 14.75049, 14.77427, 14.53463, 14.32656, 14.540…\n$ maxt          <dbl> 21.3, 17.8, 15.4, 20.8, 19.8, 15.8, 19.5, 12.6, 18.8, 12…\n$ amaxt7        <dbl> 22.38571, 20.44286, 22.21429, 24.21429, 23.14286, 21.671…\n$ amaxt14       <dbl> 21.42857, 19.72857, 19.86429, 21.80000, 20.89286, 19.578…\n$ amaxt28       <dbl> 20.71071, 19.10000, 19.18929, 19.75000, 19.05714, 18.885…\n$ amaxt60       <dbl> 24.02667, 22.28000, 22.38667, 22.93167, 22.12000, 21.031…\n$ amaxt90       <dbl> 27.07750, 25.77667, 25.89833, 24.93667, 23.93750, 23.164…\n$ amaxt180      <dbl> 26.92000, 25.92722, 25.98500, 24.84056, 23.95389, 23.343…\n$ amaxt360      <dbl> 21.55389, 20.79778, 20.81333, 20.21972, 19.99389, 19.505…\n$ amaxt720      <dbl> 21.47750, 20.57222, 20.57694, 20.13153, 20.03875, 19.650…\n$ mint          <dbl> 9.6, 9.0, 7.3, 7.7, 8.3, 8.3, 6.1, 5.9, 7.4, 5.9, 6.9, 7…\n$ amint7        <dbl> 9.042857, 7.971429, 9.171429, 10.328571, 11.200000, 10.6…\n$ amint14       <dbl> 9.928571, 9.235714, 9.421429, 10.007143, 10.900000, 10.7…\n$ amint28       <dbl> 8.417857, 7.560714, 7.353571, 8.671429, 9.575000, 10.060…\n$ amint60       <dbl> 11.156667, 9.903333, 9.971667, 10.971667, 11.975000, 12.…\n$ amint90       <dbl> 11.96667, 10.81250, 10.87833, 12.49000, 13.46167, 13.638…\n$ amint180      <dbl> 11.96778, 11.01056, 11.02000, 12.41944, 13.42500, 13.695…\n$ amint360      <dbl> 9.130556, 8.459722, 8.448333, 9.588611, 10.456389, 11.03…\n$ amint720      <dbl> 8.854861, 8.266250, 8.254028, 9.674861, 10.517083, 10.96…\n$ dist_cfa      <dbl> 9442.206, 6322.438, 7957.374, 7790.785, 10692.055, 6054.…\n$ dist_camp     <dbl> 50966.485, 6592.893, 31767.235, 8816.272, 15339.702, 941…\n$ ws            <dbl> 1.263783, 1.263783, 1.456564, 5.424445, 4.219751, 4.1769…\n$ aws_m0        <dbl> 2.644795, 2.644795, 2.644795, 5.008369, 3.947659, 5.2316…\n$ aws_m1        <dbl> 2.559202, 2.559202, 2.559202, 5.229680, 4.027398, 4.9704…\n$ aws_m3        <dbl> 2.446211, 2.446211, 2.446211, 5.386005, 3.708622, 5.3045…\n$ aws_m6        <dbl> 2.144843, 2.144843, 2.144843, 5.132617, 3.389890, 5.0355…\n$ aws_m12       <dbl> 2.545008, 2.545008, 2.548953, 5.045297, 3.698736, 5.2341…\n$ aws_m24       <dbl> 2.580671, 2.580671, 2.584047, 5.081100, 3.745286, 5.2522…\n$ dist_road     <dbl> 498.75145, 102.22032, 1217.22446, 281.69151, 215.56176, …\n$ log_dist_cfa  <dbl> 9.152945, 8.751860, 8.981854, 8.960697, 9.277256, 8.7084…\n$ log_dist_camp <dbl> 10.838924, 8.793748, 10.366191, 9.084354, 9.638200, 9.15…\n$ log_dist_road <dbl> 6.212108, 4.627130, 7.104329, 5.640813, 5.373247, 5.0047…\n$ cause         <chr> \"lightning\", \"lightning\", \"lightning\", \"lightning\", \"lig…\n\n\n\n\nPurpose\nThe primary goal is to predict the cause of the bushfire using the weather and distance from human activity variables provided.\n\n\nSource\nCollated data was part of Weihao Li’s Honours thesis, which is not publicly available. The hotspots data was collected from P-Tree System (2020), climate data was taken from the Australian Bureau of Meteorology using the bomrang package (Sparks et al. 2020), wind data from McVicar (2011) and Iowa State University (2020), vegetation data from Australian Bureau of Agricultural and Resource Economics and Sciences (2018), distance from roads calculated using OpenStreetMap contributors (2020), CFA stations from Department of Environment, Land, Water & Planning (2020a), and campsites from Department of Environment, Land, Water & Planning (2020b). The cause was predicted from training data provided by Department of Environment, Land, Water & Planning (2019).\n\n\nPre-processing\nThe 60 variables are too many to view with a tour, so it should be pre-processed using principal component analysis. The categorical variables of FOR_TYPE and FOR_CAT are removed. It would be possible to keep these if they are converted to dummy (binary variables)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Australian Bureau of Agricultural and Resource Economics and Sciences.\n2018. “Forests of Australia.”\nhttps://www.agriculture.gov.au/abares/forestsaustralia/forest-data-maps-and-tools/spatial-data/forest-cover.\n\n\nBishop, C. M. 2006. Pattern Recognition and\nMachine Learning. New York: Springer.\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning\nwith r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377.\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern\nMultidimensional Scaling. New York:\nSpringer.\n\n\nBreiman, L. 2001. “Random Forests.”\nMachine Learning 45 (1): 5–32.\n\n\nBreiman, L., J. Friedman, C. Olshen, and C. Stone. 1984.\nClassification and Regression Trees.\nMonterey, CA: Wadsworth; Brooks/Cole.\n\n\nDasu, T., D. F. Swayne, and D. Poole. 2005. “Grouping\nMultivariate Time Series: A\nCase Study.” In Proceedings of the\nIEEE Workshop on Temporal Data\nMining: Algorithms, Theory and\nApplications, in Conjunction with the Conference on Data\nMining, Houston, November 27, 2005, 25–32. IEEE Computer Society.\n\n\nDepartment of Environment, Land, Water & Planning. 2019.\n“Fire Origins - Current and\nHistorical.” https://discover.data.vic.gov.au/dataset/fire-origins-current-and-historical.\n\n\n———. 2020a. “CFA - Fire Station.” https://discover.data.vic.gov.au/dataset/cfa-fire-station-vmfeat-geomark_point.\n\n\n———. 2020b. “Recreation Sites.” https://discover.data.vic.gov.au/dataset/recreation-sites.\n\n\nFraley, C., and A. E. Raftery. 2002. “Model-Based\nClustering, Discriminant\nAnalysis, Density\nEstimation.” Journal of the American Statistical\nAssociation 97: 611–31.\n\n\nGiordani, Paolo, Maria Brigida Ferraro, and Francesca Martella. 2020.\nAn Introduction to Clustering with r. Springer Singapore. https://doi.org/10.1007/978-981-13-0553-5.\n\n\nHastie, T., R. Tibshirani, and J. Friedman. 2001. The\nElements of Statistical\nLearning. New York: Springer.\n\n\nHennig, Christian, Marina Meila, Fionn Murtagh, and Roberto Rocci, eds.\n2015. Handbook of Cluster Analysis. Chapman;\nHall/CRC. https://doi.org/10.1201/b19706.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://doi.org/10.5281/zenodo.3960218.\n\n\nIowa State University. 2020. “ASOS-AWOS-METAR Data\nDownload.” https://mesonet.agron.iastate.edu/request/download.phtml?network=AU__ASOS.\n\n\nJohnson, R. A., and D. W. Wichern. 2002. Applied Multivariate\nStatistical Analysis (5th Ed). Englewood Cliffs, NJ: Prentice-Hall.\n\n\nKassambara, Alboukadel. 2017. Practical Guide to Cluster Analysis in\nr: Unsupervised Machine Learning. STHDA.\n\n\nLeisch, Friedrich, and Bettina Gruen. 2023. “CRAN Task View:\nCluster Analysis & Finite Mixture Models.”\nhttps://cran.r-project.org/web/views/Cluster.html.\n\n\nLiaw, A., M. Wiener, L. Breiman, and A. Cutler. 2006.\n“randomForest: Breiman and\nCutler’s Random Forests for\nClassification and Regression.”\nhttp://www.R-project.org.\n\n\nMcVicar, Tim. 2011. “Near-Surface Wind Speed. V10. CSIRO. Data\nCollection.” https://doi.org/10.25919/5c5106acbcb02.\n\n\nOpenStreetMap contributors. 2020. “Planet\ndump retrieved from https://planet.osm.org .”\nhttps://www.openstreetmap.org.\n\n\nP-Tree System. 2020. “JAXA Himawari Monitor -\nUser’s Guide.” https://www.eorc.jaxa.jp/ptree/userguide.html.\n\n\nRipley, B. 1996. Pattern Recognition and\nNeural Networks. Cambridge: Cambridge\nUniversity Press.\n\n\nSparks, Adam H., Jonathan Carroll, James Goldie, Dean Marchiori, Paul\nMelloy, Mark Padgham, Hugh Parsonage, and Keith Pembleton. 2020.\nbomrang: Australian Government Bureau of\nMeteorology (BOM) Data Client. https://CRAN.R-project.org/package=bomrang.\n\n\nVenables, W. N., and B. Ripley. 2002. Modern Applied\nStatistics with S. New York:\nSpringer-Verlag."
  },
  {
    "objectID": "intro.html#notation-conventions-and-r-objects",
    "href": "intro.html#notation-conventions-and-r-objects",
    "title": "1  Introduction",
    "section": "1.1 Notation conventions and R objects",
    "text": "1.1 Notation conventions and R objects\nData matrix"
  },
  {
    "objectID": "intro.html#whats-different-about-space-beyond-2d",
    "href": "intro.html#whats-different-about-space-beyond-2d",
    "title": "1  Introduction",
    "section": "1.2 What’s different about space beyond 2D",
    "text": "1.2 What’s different about space beyond 2D\n\nCubes\nA lot of Ursula’s work like the figures in the burning sage paper could be useful here\nA good spot to introduce a metaphor for the tour"
  },
  {
    "objectID": "intro.html#interactive-and-dynamic-graphics-literature",
    "href": "intro.html#interactive-and-dynamic-graphics-literature",
    "title": "1  Introduction",
    "section": "1.3 Interactive and dynamic graphics literature",
    "text": "1.3 Interactive and dynamic graphics literature\nA short history of the literature, how does that stat graphics literature differ from info vis?"
  },
  {
    "objectID": "intro.html#an-opening-case-study",
    "href": "intro.html#an-opening-case-study",
    "title": "1  Introduction",
    "section": "1.4 An opening case study",
    "text": "1.4 An opening case study\nTo give a taste of the approach used in the book"
  },
  {
    "objectID": "intro.html#the-big-picture-of-the-book",
    "href": "intro.html#the-big-picture-of-the-book",
    "title": "1  Introduction",
    "section": "1.5 The big picture of the book",
    "text": "1.5 The big picture of the book\n\nOutline of the different chapters"
  },
  {
    "objectID": "LDA.html",
    "href": "LDA.html",
    "title": "5  Linear discriminant analysis",
    "section": "",
    "text": "Discriminant analysis dates to the early 1900s. It’s one of the most elegant and simple techniques for both modeling separation between groups, and producing a low-dimensional representation of the differences between groups.\nFisher’s linear discriminant (Fisher 1936) computes a linear combination of the variables that separates two classes by comparing the differences between class means with the variance of values within each class. It makes no assumptions about the distribution of the data.\nLinear discriminant analysis (LDA), as proposed by Rao (1948), formalizes Fisher’s approach, by recognising that it arises from making the assumption that the data values for each class arise from a \\(p\\)-dimensional multivariate normal distribution, sharing a common variance-covariance matrix with data from other classes. When this assumption holds, Fisher’s linear discriminant gives the optimal separation between the two groups.\nFor two equally weighted groups, where \\(Y\\) is coded as \\(\\{0, 1\\}\\), the LDA rule is:\nAllocate a new observation \\(X_0\\) to group 1 if\n\\[(\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}X_0 \\geq\n  \\frac{1}{2}(\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}\n  (\\bar{X}_1+\\bar{X}_2)\\]\nelse allocate it to group 2,\nwhere ${X}_k $ are the class mean vectors of an \\(n\\times p\\) data matrix \\(X_k ~~(k=1,2)\\),\n\\[S_{\\rm pooled} = \\frac{(n_1-1) S_1}{(n_1-1)+(n_2-1)} + \\frac{(n_2-1) S_2}{(n_1-1)+(n_2-1)}\\]\nis the pooled variance-covariance matrix, and\n\\[S_k = \\frac{1}{n-1}\\sum_{i=1}^{n}\n(X_{ki}-\\bar{X}_k)(X_{ki}-\\bar{X}_k)', ~~k=1,2\\]\nis the class variance–covariance matrix. The linear discriminant part of this rule is \\((\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}\\), which defines the linear combination of variables that best separates the two groups. To define a classification rule, we compute the value of the new observation \\(X_0\\) on this line and compare it with the value of the average of the two class means \\((\\bar{X}_1+\\bar{X}_2)/2\\) on the same line.\nFor multiple \\((g)\\) classes, the rule and the discriminant space are constructed using the between-group sum-of-squares matrix,\n\\[B=\\sum_{k=1}^g n_k(\\bar{X}_k-\\bar{X})(\\bar{X}_k-\\bar{X})'\\]\nwhich measures the differences between the class means, compared with the overall data mean \\(\\bar{X}\\) and the within-group sum-of-squares matrix,\n\\[W =\n\\sum_{k=1}^g\\sum_{i=1}^{n_k}\n(X_{ki}-\\bar{X}_k)(X_{ki}-\\bar{X}_k)'\\]\nwhich measures the variation of values around each class mean. The linear discriminant space is generated by computing the eigenvectors (canonical coordinates) of \\(W^{-1}B\\), and this is the space where the group means are most separated with respect to the pooled variance–covariance. The resulting classification rule is to allocate a new observation to the class with the highest value of\n\\[\\bar{X}_k'S^{-1}_{\\rm pooled}X_0 -\n\\frac{1}{2}\\bar{X}_k'S^{-1}_{\\rm pooled}\\bar{X}_k ~~~k=1,...,g\\]\nwhich results in allocating the new observation into the class with the closest mean.\nThis LDA approach is widely applicable, but it is useful to check the underlying assumptions on which it depends: (1) that the cluster structure corresponding to each class forms an ellipse, showing that the class is consistent with a sample from a multivariate normal distribution, and (2) that the variance of values around each mean is nearly the same. Figure 5.1 and Figure 5.2 illustrates two datasets, of which only one is consistent with these assumptions. Other parametric models, such as quadratic discriminant analysis or logistic regression, also depend on assumptions about the data which should be validated. \n\n\nCode\nlibrary(dplyr)\nlibrary(colorspace)\nlibrary(ggplot2)\nlibrary(mulgar)\nlibrary(MASS)\nlibrary(ggpubr)\nload(\"data/penguins_sub.rda\")\nlda1 <- ggplot(penguins_sub, aes(x=bl, \n                         y=bd, \n                         colour=species)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(-2.5, 3) + ylim(-2.5, 2.5) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio = 1) \np_ell <- NULL\nfor (i in unique(penguins_sub$species)) {\n  x <- penguins_sub %>% dplyr::filter(species == i)\n  e <- gen_xvar_ellipse(x[,1:2], n=150, nstd=1.5)\n  e$species <- i\n  p_ell <- bind_rows(p_ell, e)\n}\nlda2 <- ggplot(p_ell, aes(x=bl, \n                         y=bd, \n                         colour=species)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(-2.5, 3) + ylim(-2.5, 2.5) +\n  ggtitle(\"(b)\") +\n  theme(aspect.ratio = 1)\nggarrange(lda1, lda2, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n\n\n\n\n\nFigure 5.1: Scatterplot of flipper length by bill length of the penguins data, and corresponding variance-covariance ellipses. There is a small amount of difference between the ellipses, but they are similar enough to be confident in assuming the population variance-covariances are equal.\n\n\n\n\n\n\nCode\n# Now repeat for a data set that violates assumptions\ndata(bushfires)\nlda3 <- ggplot(bushfires, aes(x=log_dist_cfa, \n                         y=log_dist_road, \n                         colour=cause)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(6, 11) + ylim(-1, 10.5) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio = 1)\nb_ell <- NULL\nfor (i in unique(bushfires$cause)) {\n  x <- bushfires %>% dplyr::filter(cause == i)\n  e <- gen_xvar_ellipse(x[,c(57, 59)], n=150, nstd=2)\n  e$cause <- i\n  b_ell <- bind_rows(b_ell, e)\n}\nlda4 <- ggplot(b_ell, aes(x=log_dist_cfa, \n                         y=log_dist_road, \n                         colour=cause)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(6, 11) + ylim(-1, 10.5) +\n  ggtitle(\"(b)\") +\n  theme(aspect.ratio = 1)\nggarrange(lda3, lda4, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n\n\n\n\n\nFigure 5.2: Scatterplot of distance to cfa and road for the bushfires data, and corresponding variance-covariance ellipses. There is a lot of difference between the ellipses, so it cannot be assumed that the population variance-covariances are equal.\n\n\n\n\nOur description is derived from Venables and Ripley (2002) and Ripley (1996). A good general treatment of parametric methods for supervised classification can be found in Johnson and Wichern (2002) or another similar multivariate analysis textbook. It’s also useful to know that hypothesis testing for difference in multivariate means using multivariate analysis of variance (MANOVA) uses the same assumption of equal variance-covariances. The method of looking at ellipses here can be used to check that assumption, too.\nMultivariate textbooks are generally lacking in good explanations of how to use interactive graphics both to check the assumptions underlying the methods and to explore the results. This chapter aims to fill this gap.\nAlgorithmic methods have overtaken parametric methods in the practice of supervised classification. A parametric method such as linear discriminant analysis yields a set of interpretable output parameters, so it leaves a clear trail helping us to understand what was done to produce the results. An algorithmic method, on the other hand, is more or less a black box, with various input parameters that are adjusted to tune the algorithm. The algorithm’s input and output parameters do not always correspond in any obvious way to the interpretation of the results. Because if it’s simplicity, if the assumptions (approximately) hold, LDA can provide an elegant classification solution.\n\n\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7: 179–88.\n\n\nJohnson, R. A., and D. W. Wichern. 2002. Applied Multivariate Statistical Analysis (5th Ed). Englewood Cliffs, NJ: Prentice-Hall.\n\n\nRao, C. R. 1948. “The Utilization of Multiple Measurements in Problems of Biological Classification (with Discussion).” Journal of the Royal Statistical Society, Series B 10: 159–203.\n\n\nRipley, B. 1996. Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press.\n\n\nVenables, W. N., and B. Ripley. 2002. Modern Applied Statistics with S. New York: Springer-Verlag."
  },
  {
    "objectID": "LDA.html#definition",
    "href": "LDA.html#definition",
    "title": "5  Linear discriminant analysis",
    "section": "5.1 Definition",
    "text": "5.1 Definition\nFisher’s linear discriminant (Fisher 1936) computes a linear combination of the variables that separates two classes by comparing the differences between class means with the variance of values within each class. It makes no assumptions about the distribution of the data.\nLinear discriminant analysis (LDA), as proposed by Rao (1948), formalizes Fisher’s approach, by recognising that it arises from making the assumption that the data values for each class arise from a \\(p\\)-dimensional multivariate normal distribution, sharing a common variance-covariance matrix with data from other classes. When this assumption holds, Fisher’s linear discriminant gives the optimal separation between the two groups.\nFor two equally weighted groups, where \\(Y\\) is coded as \\(\\{0, 1\\}\\), the LDA rule is:\nAllocate a new observation \\(X_0\\) to group 1 if\n\\[(\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}X_0 \\geq\n  \\frac{1}{2}(\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}\n  (\\bar{X}_1+\\bar{X}_2)\\]\nelse allocate it to group 2,\nwhere ${X}_k $ are the class mean vectors of an \\(n\\times p\\) data matrix \\(X_k ~~(k=1,2)\\),\n\\[S_{\\rm pooled} = \\frac{(n_1-1) S_1}{(n_1-1)+(n_2-1)} + \\frac{(n_2-1) S_2}{(n_1-1)+(n_2-1)}\\]\nis the pooled variance-covariance matrix, and\n\\[S_k = \\frac{1}{n-1}\\sum_{i=1}^{n}\n(X_{ki}-\\bar{X}_k)(X_{ki}-\\bar{X}_k)', ~~k=1,2\\]\nis the class variance–covariance matrix. The linear discriminant part of this rule is \\((\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}\\), which defines the linear combination of variables that best separates the two groups. To define a classification rule, we compute the value of the new observation \\(X_0\\) on this line and compare it with the value of the average of the two class means \\((\\bar{X}_1+\\bar{X}_2)/2\\) on the same line.\nFor multiple \\((g)\\) classes, the rule and the discriminant space are constructed using the between-group sum-of-squares matrix,\n\\[B=\\sum_{k=1}^g n_k(\\bar{X}_k-\\bar{X})(\\bar{X}_k-\\bar{X})'\\]\nwhich measures the differences between the class means, compared with the overall data mean \\(\\bar{X}\\) and the within-group sum-of-squares matrix,\n\\[W =\n\\sum_{k=1}^g\\sum_{i=1}^{n_k}\n(X_{ki}-\\bar{X}_k)(X_{ki}-\\bar{X}_k)'\\]\nwhich measures the variation of values around each class mean. The linear discriminant space is generated by computing the eigenvectors (canonical coordinates) of \\(W^{-1}B\\), and this is the space where the group means are most separated with respect to the pooled variance–covariance. The resulting classification rule is to allocate a new observation to the class with the highest value of\n\\[\\bar{X}_k'S^{-1}_{\\rm pooled}X_0 -\n\\frac{1}{2}\\bar{X}_k'S^{-1}_{\\rm pooled}\\bar{X}_k ~~~k=1,...,g\\]\nwhich results in allocating the new observation into the class with the closest mean."
  },
  {
    "objectID": "LDA.html#checking-assumptions",
    "href": "LDA.html#checking-assumptions",
    "title": "5  Linear discriminant analysis",
    "section": "5.2 Checking assumptions",
    "text": "5.2 Checking assumptions\nThis LDA approach is widely applicable, but it is useful to check the underlying assumptions on which it depends: (1) that the cluster structure corresponding to each class forms an ellipse, showing that the class is consistent with a sample from a multivariate normal distribution, and (2) that the variance of values around each mean is nearly the same. Figure 5.1 and Figure 5.2 illustrates two datasets, of which only one is consistent with these assumptions. Other parametric models, such as quadratic discriminant analysis or logistic regression, also depend on assumptions about the data which should be validated. \n\n\nCode\nlibrary(dplyr)\nlibrary(colorspace)\nlibrary(ggplot2)\nlibrary(mulgar)\nlibrary(MASS)\nlibrary(ggpubr)\nload(\"data/penguins_sub.rda\")\nlda1 <- ggplot(penguins_sub, aes(x=bl, \n                         y=bd, \n                         colour=species)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(-2.5, 3) + ylim(-2.5, 2.5) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio = 1) \np_ell <- NULL\nfor (i in unique(penguins_sub$species)) {\n  x <- penguins_sub %>% dplyr::filter(species == i)\n  e <- gen_xvar_ellipse(x[,1:2], n=150, nstd=1.5)\n  e$species <- i\n  p_ell <- bind_rows(p_ell, e)\n}\nlda2 <- ggplot(p_ell, aes(x=bl, \n                         y=bd, \n                         colour=species)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(-2.5, 3) + ylim(-2.5, 2.5) +\n  ggtitle(\"(b)\") +\n  theme(aspect.ratio = 1)\nggarrange(lda1, lda2, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n\n\n\n\n\nFigure 5.1: Scatterplot of flipper length by bill length of the penguins data, and corresponding variance-covariance ellipses. There is a small amount of difference between the ellipses, but they are similar enough to be confident in assuming the population variance-covariances are equal.\n\n\n\n\n\n\nCode\n# Now repeat for a data set that violates assumptions\ndata(bushfires)\nlda3 <- ggplot(bushfires, aes(x=log_dist_cfa, \n                         y=log_dist_road, \n                         colour=cause)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(6, 11) + ylim(-1, 10.5) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio = 1)\nb_ell <- NULL\nfor (i in unique(bushfires$cause)) {\n  x <- bushfires %>% dplyr::filter(cause == i)\n  e <- gen_xvar_ellipse(x[,c(57, 59)], n=150, nstd=2)\n  e$cause <- i\n  b_ell <- bind_rows(b_ell, e)\n}\nlda4 <- ggplot(b_ell, aes(x=log_dist_cfa, \n                         y=log_dist_road, \n                         colour=cause)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(6, 11) + ylim(-1, 10.5) +\n  ggtitle(\"(b)\") +\n  theme(aspect.ratio = 1)\nggarrange(lda3, lda4, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n\n\n\n\n\nFigure 5.2: Scatterplot of distance to cfa and road for the bushfires data, and corresponding variance-covariance ellipses. There is a lot of difference between the ellipses, so it cannot be assumed that the population variance-covariances are equal.\n\n\n\n\n\n\nCode\nlibrary(tourr)\np_ell <- NULL\nfor (i in unique(penguins_sub$species)) {\n  x <- penguins_sub %>% dplyr::filter(species == i)\n  e <- gen_xvar_ellipse(x[,1:4], n=150, nstd=1.5)\n  e$species <- i\n  p_ell <- bind_rows(p_ell, e)\n}\np_ell$species <- factor(p_ell$species)\nload(\"data/penguins_tour_path.rda\")\nanimate_xy(p_ell[,1:4], col=factor(p_ell$species))\nrender_gif(penguins_sub[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\", col=penguins_sub$species),\n           gif_file=\"gifs/penguins_lda1.gif\",\n           frames=500,\n           loop=FALSE)\nrender_gif(p_ell[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\", col=p_ell$species),\n           gif_file=\"gifs/penguins_lda2.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\n\n\n\n\n(a) Data\n\n\n\n\n\n\n\n(b) Variance-covariance ellipses\n\n\n\n\nFigure 5.3: Checking the assumption of equal variance-covariance matrices for the 4D penguins data."
  },
  {
    "objectID": "LDA.html#examining-results",
    "href": "LDA.html#examining-results",
    "title": "5  Linear discriminant analysis",
    "section": "5.3 Examining results",
    "text": "5.3 Examining results\n\n\nCode\nlibrary(classifly)\nlibrary(MASS)\npenguins_lda <- lda(species ~ ., penguins_sub, prior = c(1/3, 1/3, 1/3))\np_lda_boundaries <- explore(penguins_lda, penguins_sub)\nanimate_slice(p_lda_boundaries[p_lda_boundaries$.TYPE == \"simulated\",1:4], col=p_lda_boundaries[p_lda_boundaries$.TYPE == \"simulated\",6], v_rel=0.02, axes=\"bottomleft\")\nrender_gif(p_lda_boundaries[p_lda_boundaries$.TYPE == \"simulated\",1:4],\n           planned_tour(pt1),\n           display_slice(v_rel=0.02, \n             col=p_lda_boundaries[ p_lda_boundaries$.TYPE == \"simulated\",6], \n             axes=\"bottomleft\"),                     gif_file=\"gifs/penguins_lda_boundaries.gif\",\n           frames=500,\n           loop=FALSE\n           )\n\n\n\n\n\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7: 179–88.\n\n\nJohnson, R. A., and D. W. Wichern. 2002. Applied Multivariate Statistical Analysis (5th Ed). Englewood Cliffs, NJ: Prentice-Hall.\n\n\nRao, C. R. 1948. “The Utilization of Multiple Measurements in Problems of Biological Classification (with Discussion).” Journal of the Royal Statistical Society, Series B 10: 159–203.\n\n\nRipley, B. 1996. Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press.\n\n\nVenables, W. N., and B. Ripley. 2002. Modern Applied Statistics with S. New York: Springer-Verlag."
  },
  {
    "objectID": "brush-and-spin.html",
    "href": "brush-and-spin.html",
    "title": "8  Brush-and-spin approach",
    "section": "",
    "text": "A purely graphical spin and brush approach to cluster analysis works well when there are good separations between groups, even when there are marked differences in variance structures between groups or when groups have non-linear boundaries. It does not work very well when there are clusters that overlap, or when there are no distinct clusters but rather we simply wish to partition the data. In these situations it may be better to begin with a numerical solution and to use visual tools to evaluate it, perhaps making refinements subsequently. Several examples of the spin and brush approach are documented in the literature, such as Cook et al. (1995) and Wilhelm, Wegman, and Symanzik (1999).\n\n\nCode\n# Following https://github.com/plotly/plotly.R/blob/master/demo/animation-tour-basic.R\nlibrary(tourr)\nlibrary(plotly)\nload(\"data/penguins_sub.rda\")\np_mat <- as.matrix(penguins_sub[,1:4])\ntour <- new_tour(p_mat, \n                 grand_tour(), NULL)\n\ntour_dat <- function(step_size) {\n  step <- tour(step_size)\n  proj <- center(p_mat %*% step$proj)\n  data.frame(x = proj[,1], y = proj[,2], \n             species = penguins_sub$species,\n             id = 1:nrow(penguins_sub))\n}\n\nproj_dat <- function(step_size) {\n  step <- tour(step_size)\n  data.frame(\n    x = step$proj[,1], y = step$proj[,2], measure = colnames(p_mat)\n )\n}\n\nsteps <- c(0, rep(1/15, 150))\nstepz <- cumsum(steps)\n\n# tidy version of tour data\ntour_dats <- lapply(steps, tour_dat)\ntour_datz <- Map(function(x, y) cbind(x, step = y),\n                 tour_dats, stepz)\ntour_data <- dplyr::bind_rows(tour_datz)\n\ntour_data <- highlight_key(tour_data, ~id)\n\n# tidy version of tour projection data\nproj_dats <- lapply(steps, proj_dat)\nproj_datz <- Map(function(x, y) cbind(x, step = y), proj_dats, stepz)\nproj_data <- dplyr::bind_rows(proj_datz)\nproj_data$x <- proj_data$x*3\nproj_data$y <- proj_data$y*3\n\nax <- list(\n  title = \"\",\n  range = c(-3, 3),\n  zeroline = FALSE\n)\n\n# Set colors\nclrs <- grDevices::hcl.colors(6, palette=\"Zissou 1\")\n  \n# for nicely formatted slider labels\noptions(digits = 2)\n\np_b_s <- proj_data %>%\n  plot_ly(x = ~x, y = ~y, frame = ~step, color = I(\"gray80\")) %>%\n  config(displaylogo = FALSE, \n         modeBarButtonsToRemove = c(\"toImage\", \"sendDataToCloud\", \"editInChartStudio\", \"zoom2d\", \"zoomIn2d\", \"zoomOut2d\", \"pan2d\", \"drawclosedpath\", \"drawopenpath\", \"autoScale2d\", \"hoverClosestCartesian\", \"hoverCompareCartesian\", \"resetScale2d\")) %>%\n  add_segments(xend = 0, yend = 0) %>%\n  add_text(text = ~measure) %>%\n  add_markers(color = I(\"black\"), data = tour_data, text = ~id, ids = ~id, hoverinfo = \"text\") %>%\n  hide_legend() %>%\n  layout(xaxis = ax, yaxis = ax, width=600, height=600) %>%\n  animation_opts(50, transition = 0, redraw = FALSE) %>%\n  highlight(on = \"plotly_selected\", \n            off = \"plotly_doubleclick\",\n            color = clrs, \n            persistent = TRUE, \n            dynamic = TRUE, \n            opacityDim = 0.5)\nhtmlwidgets::saveWidget(p_b_s,\n          file=\"html/penguins_brush_and_spin.html\",\n          selfcontained = TRUE)\n\n\n\n\n\n\nCode\nlibrary(detourr)\ndetour(penguins_sub, \n       tour_aes(projection = -species)) |>\n       tour_path(grand_tour(2), fps = 60) |>\n       show_scatter(alpha = 0.7, axes = FALSE)\n\n# remotes::install_github(\"pfh/langevitour\")\n# remotes::install_github(\"plotly/plotly.R\")\nlibrary(langevitour)\nlibrary(crosstalk)\nshared <- SharedData$new(penguins_sub)\n\nlangevitourWidget <- langevitour(\n    penguins_sub[,1:4], \n    link=shared,  \n    pointSize=2,\n    width=700, height=700)\n\nlibrary(liminal)\nlimn_tour(fake_trees, dim1:dim10)\n\n\n\nThis description of the spin and brush approach on , a particle physics dataset, follows that in . The data contains seven variables. We have no labels for the data, so when we begin, all the points have the same color and glyph. Watch the data in a tour for a few minutes and you will see that there are no natural clusters, but there is clearly structure.\n \nWe will use the projection pursuit guided tour to help us find that structure. We will tour on the principal components, rather than the raw variables, because that improves the performance of the projection pursuit indexes. Two indexes are useful for detecting clusters: holes and central mass. The holes index is sensitive to projections where there are few points (i.e., a hole) in the center. The central mass index is the opposite: It is sensitive to projections that have too many points in the center. These indexes are explained in (chap-toolbox?).\nThe holes index is usually the most useful for clustering, but not for the particle physics data, because it does not have a ``hole’’ at the center. The central mass index is the most appropriate here. Alternate between optimization (a guided tour) and the unguided grand tour to find local maxima, each of which is a projection that is potentially useful for revealing clusters. The process is illustrated in ?fig-prim7-tour.\nThe top left plot shows the initial default projection, the second principal component plotted against the first. The plot next to it shows the projected data corresponding to the first local maximum found by the guided tour. It has three strands of points stretching out from the central clump and several outliers. We brush the points along each strand, in red, blue, and orange, and we paint the outliers with open circles. (See the next two plots.) We continue by choosing a new random start for the guided tour, and then waiting until new territory in the data is discovered. \nThe optimization settles on a projection where there are three strands visible, as observed in the leftmost plot in the second row. Two strands have been previously brushed, but a new one has appeared; this is painted yellow.\nWe also notice that there is another new strand hidden below the red strand. It is barely distinguishable from the red strand in this projection, but the two strands separate widely in other projections. It is tricky to brush it, because it is not well separated in this projection. We use a trick: Hide the red points, brush the new strand green, and ``unhide’’ the red points again (middle plot in the second row).\nFive clusters have been easily identified, and now finding new clusters in this data is increasingly difficult. After several more alternations between the grand tour and the guided tour, we find something new (shown in the rightmost plot in the second row): One more strand has emerged, and we paint it pink.\n% Figure 3\nThe results at this stage are summarized by the bottom row of plots. There is a very visible triangular component (in gray) revealed when all of the colored points are hidden. We check the shape of this cluster by drawing lines between outer points to contain the inner ones. Touring after the lines are drawn helps to check how well they match the shape of the clusters. The colored groups pair up at each vertex, and we draw in the shape of these too — a single line matches the structures reasonably well.\nThe final step of the spin and brush clustering is to clean up this solution, touching up the color groups by continuing to tour, and repainting a point here and there. When we finish, we have found seven clusters in this data that form a very strong geometric object in the data space: a two-dimensional (2D) triangle, with two one-dimensional (1D) strands extending in different directions from each vertex. The lines confirm our understanding of this object’s shape, because the points stay close to the lines in all of the projections observed in a tour.\n% Figure 4\nThe next stage of cluster analysis is to characterize the nature of the clusters. To do that, we would calculate summary statistics for each cluster, and plot the clusters (?fig-prim7-model). When we plot the clusters of the particle physics data, we find that the 2D triangle exists primarily in the plane defined by X3 and X5. If you do the same, notice that the variance in measurements for the gray group is large in variables X3 and X5, but negligible in the other variables. The linear pieces can also be characterized by their distributions on each of the variables. With this example, we have shown that it is possible to uncover very unusual clusters in data without any domain knowledge.\nHere are several tips about the spin and brush approach.\n\nSave the dataset frequently during the exploration of a complex dataset, being sure to save your colors and glyphs, because it may take several sessions to arrive at a final clustering.\n\nManual controls are useful for refining the optimal projection because another projection in the neighborhood may be more revealing.\n\nThe holes index is usually the most successful projection pursuit index for finding clusters.\nPrincipal component coordinates may provide a better starting point than the raw variables.\n\nFinally, the spin and brush method will not work well if there are no clear separations in the data, and the clusters are high-dimensional, unlike the low-dimensional clusters found in this example.\n\n\n\n\nCook, D., A. Buja, J. Cabrera, and C. Hurley. 1995. “Grand Tour and Projection Pursuit.” Journal of Computational and Graphical Statistics 4 (3): 155–72.\n\n\nWilhelm, A. F. X., E. J. Wegman, and J. Symanzik. 1999. “Visual Clustering and Classification: The Oronsay Particle Size Data Set Revisited.” Computational Statistics: Special Issue on Interactive Graphical Data Analysis 14 (1): 109–46."
  }
]