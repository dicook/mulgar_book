[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interactive and dynamic graphics for high-dimensional data using R",
    "section": "",
    "text": "Although there are many resources available for data visualization, there are few comprehensive resources on high-dimensional data visualisation. This book fills this gap by providing a comprehensive and up-to-date guide to visualising high-dimensional data and models, with R.\nHigh-dimensional data spaces are fascinating places. You may think that there’s a lot of ways to plot one or two variables, and a lot of types of patterns that can be found. You might use a density plot and see skewness or a dot plot to find outliers. A scatterplot of two variables might reveal a non-linear relationship or a barrier beyond which no observations exist. We don’t as yet have so many different choices of plot types for high-dimensions, but these types of patterns are also what we seek in scatterplots of high-dimensional data. The additional dimensions can clarify these patterns, that clusters are likely to be more distinct. Observations that did not appear to be very different can be seen to be lonely anomalies in high-dimensions, that no other observations have quite the same combination of values.\nIf you encounter an error, you can report it as an issue at the Github repo for this book.\nPlease make a small reproducible example and report the error encountered. Reproducible examples have these components:"
  },
  {
    "objectID": "index.html#audience",
    "href": "index.html#audience",
    "title": "Interactive and dynamic graphics for high-dimensional data using R",
    "section": "Audience",
    "text": "Audience\nHigh-dimensional data arises in many fields such as biology, social sciences, finance, and more. Anyone who is doing exploratory data analysis and model fitting for more than two variables will benefit from learning how to effectively visualise high-dimensions. This book will be useful for students and teachers of mulitvariate data analysis and machine learning, and researchers, data analysts, and industry professionals who work in these areas."
  },
  {
    "objectID": "index.html#how-to-use-the-book",
    "href": "index.html#how-to-use-the-book",
    "title": "Interactive and dynamic graphics for high-dimensional data using R",
    "section": "How to use the book?",
    "text": "How to use the book?\nThe book is written with explanations followed by examples with R code. The toolbox chapter provides an overview of the primary high-dimensional visualisation methods. The remaining chapters focus on different application areas and how to use the high-dimensional visualisation to complement commonly used analytical methods.\n\nWhat should I know before reading this book?\nThe examples assume that you already use R, and have a working knowledge of base R and tidyverse way of thinking about data analysis. It also assumes that you have some knowledge of statistical methods, and some experience with machine learning methods.\nIf you feel like you need build up your skills in these areas in preparation for working through this book, these are our recommended resources:\n\nR for Data Science by Wickham and Grolemund for learning about tidyverse.\nIntroduction to Modern Statistics by Çetinkaya-Rundel and Hardin to learn about introductory statistics.\nHands-On Machine Learning with R by Boehmke and Greenwell to learn about machine learning."
  },
  {
    "objectID": "index.html#setting-up-your-workflow",
    "href": "index.html#setting-up-your-workflow",
    "title": "Interactive and dynamic graphics for high-dimensional data using R",
    "section": "Setting up your workflow",
    "text": "Setting up your workflow\nTo get started set up your computer with the current versions of R and Rstudio Desktop.\nIn addition, we have made an R package to share the data and functions used in this book, called mulgar.12\n\ninstall.packages(\"mulgar\")\n#| or the development version\ndevtools::install_github(\"dicook/mulgar\")"
  },
  {
    "objectID": "intro.html#notation-conventions-and-r-objects",
    "href": "intro.html#notation-conventions-and-r-objects",
    "title": "1  Introduction",
    "section": "1.1 Notation conventions and R objects",
    "text": "1.1 Notation conventions and R objects\nThe data can be considered to be a matrix of numbers with the columns corresponding to variables, and the rows correspond to observations. It can be helpful to write this in mathematical notation, like:\n\\[\\begin{eqnarray*}\nX_{n\\times p} =\n[X_1~X_2~\\dots~X_p]_{n\\times p} = \\left[ \\begin{array}{cccc}\nX_{11} & X_{12} & \\dots & X_{1p} \\\\\nX_{21} & X_{22} & \\dots & X_{2p}\\\\\n\\vdots & \\vdots &  & \\vdots \\\\\nX_{n1} & X_{n2} & \\dots & X_{np} \\end{array} \\right]_{n\\times p}\n\\end{eqnarray*}\\]\nwhere \\(X\\) indicates the the \\(n\\times p\\) data matrix, \\(X_j\\) indicates variable \\(j, j=1, \\dots, p\\) and \\(X_{ij}\\) indicates the value \\(j^{th}\\) variable of the \\(i^{th}\\) observation. (It can be confusing to distinguish whether one is referring to the observation or a variable, because \\(X_i\\) is used to indicate observation also. When this is done it is usally accompanied by qualifying words such as observation \\(X_3\\), or variable \\(X_3\\).)\nWhen there is a response variable(s), it is common to consider \\(X\\) to be the predictors, and use \\(Y\\) to indicate the response variable(s). \\(Y\\) could be a matrix, also, and would be \\(n\\times q\\), where commonly \\(q=1\\). \\(Y\\) could be numeric or categorical, and this would change how it is handled with visualisation.\nTo make a low-dimensional projection (shadow) of the data, we need a projection matrix:\n\\[\\begin{eqnarray*}\nA_{p\\times d} = \\left[ \\begin{array}{cccc}\nA_{11} & A_{12} & \\dots & A_{1d} \\\\\nA_{21} & A_{22} & \\dots & A_{2d}\\\\\n\\vdots & \\vdots &  & \\vdots \\\\\nA_{p1} & A_{p2} & \\dots & A_{pd} \\end{array} \\right]_{p\\times d}\n\\end{eqnarray*}\\]\n\\(A\\) should be an orthonormal matrix, which means that the \\(\\sum_{j=1}^p A_{jk}^2=1, k=1, \\dots, d\\) (columns represent vectors of length 1) and \\(\\sum_{j=1}^p A_{jk}A_{jl}=0, k,l=1, \\dots, d\\) (columns represent vectors that are orthogonal to each other).\nThen the projected data is written as:\n\\[\\begin{eqnarray*}\nY_{n\\times d} = XA = \\left[ \\begin{array}{cccc}\ny_{11} & y_{12} & \\dots & y_{1d} \\\\\ny_{21} & y_{22} & \\dots & y_{2d}\\\\\n\\vdots & \\vdots &  & \\vdots \\\\\ny_{n1} & y_{n2} & \\dots & y_{nd} \\end{array} \\right]_{n\\times d}\n\\end{eqnarray*}\\]\nwhere \\(y_{ij} = \\sum_{k=1}^p X_{ik}A_{kj}\\). Note that we are using \\(Y\\) as the projected data here, as well as it possibly being used for a response variable. Where necessary, this will be clarified with words in the text, when notation is used in explanations later.\nWhen using R, if we only have the data corresponding to \\(X\\) it makes sense to use a matrix object. However, if the response variable is included and it is categorical, then we might use a data.frame or a tibble which can accommodate non-numerical values. Then to work with the data, we can use the base R methods:\n\nX <- matrix(c(1.1, 1.3, 1.4, 1.2, 2.7, 2.6, 2.4, 2.5, 3.5, 3.4, 3.2, 3.6), ncol=4, byrow=TRUE)\nX\n\n     [,1] [,2] [,3] [,4]\n[1,]  1.1  1.3  1.4  1.2\n[2,]  2.7  2.6  2.4  2.5\n[3,]  3.5  3.4  3.2  3.6\n\n\nwhich is a data matrix with \\(n=3, p=4\\) and to extract a column (variable):\n\nX[,2]\n\n[1] 1.3 2.6 3.4\n\n\nor a row (observation):\n\nX[2,]\n\n[1] 2.7 2.6 2.4 2.5\n\n\nor an individual cell (value):\n\nX[3,2]\n\n[1] 3.4\n\n\nTo make a projection we need an orthonormal matrix:\n\nA <- matrix(c(0.707,0.707,0,0,0,0,0.707,0.707), ncol=2, byrow=FALSE)\nA\n\n      [,1]  [,2]\n[1,] 0.707 0.000\n[2,] 0.707 0.000\n[3,] 0.000 0.707\n[4,] 0.000 0.707\n\n\nYou can check that it is orthonormal by\n\nsum(A[,1]^2)\n\n[1] 0.999698\n\nsum(A[,1]*A[,2])\n\n[1] 0\n\n\nand make a projection using matrix multiplication:\n\nX %*% A\n\n       [,1]   [,2]\n[1,] 1.6968 1.8382\n[2,] 3.7471 3.4643\n[3,] 4.8783 4.8076"
  },
  {
    "objectID": "intro.html#whats-different-about-space-beyond-2d",
    "href": "intro.html#whats-different-about-space-beyond-2d",
    "title": "1  Introduction",
    "section": "1.2 What’s different about space beyond 2D",
    "text": "1.2 What’s different about space beyond 2D\n\nCubes\nA lot of Ursula’s work like the figures in the burning sage paper could be useful here\nA good spot to introduce a metaphor for the tour"
  },
  {
    "objectID": "intro.html#interactive-and-dynamic-graphics-literature",
    "href": "intro.html#interactive-and-dynamic-graphics-literature",
    "title": "1  Introduction",
    "section": "1.3 Interactive and dynamic graphics literature",
    "text": "1.3 Interactive and dynamic graphics literature\nA short history of the literature, how does that stat graphics literature differ from info vis?"
  },
  {
    "objectID": "intro.html#an-opening-case-study",
    "href": "intro.html#an-opening-case-study",
    "title": "1  Introduction",
    "section": "1.4 An opening case study",
    "text": "1.4 An opening case study\nTo give a taste of the approach used in the book"
  },
  {
    "objectID": "intro.html#the-big-picture-of-the-book",
    "href": "intro.html#the-big-picture-of-the-book",
    "title": "1  Introduction",
    "section": "1.5 The big picture of the book",
    "text": "1.5 The big picture of the book\n\nOutline of the different chapters"
  },
  {
    "objectID": "dimension.html",
    "href": "dimension.html",
    "title": "Dimension reduction",
    "section": "",
    "text": "Shadows of Mulga in the Great Victoria Desert at Sunset"
  },
  {
    "objectID": "pca.html#exercises",
    "href": "pca.html#exercises",
    "title": "3  Principal component analysis (PCA)",
    "section": "3.1 Exercises",
    "text": "3.1 Exercises\n\nMake a scatterplot matrix of the first four PCs. Is the branch pattern visible in any pair?\nConstruct five new variables to measure these skills offense, defense, playing time, ball movement, errors. Using the tour, examine the relationship between these variables. Map out how a few players could be characterised based on these directions of skills."
  },
  {
    "objectID": "nldr.html",
    "href": "nldr.html",
    "title": "4  Non-linear dimension reduction",
    "section": "",
    "text": "Code\nlibrary(liminal)\nlibrary(Rtsne)\ndata(fake_trees)\nset.seed(2020)\ntsne <- Rtsne::Rtsne(dplyr::select(fake_trees, dplyr::starts_with(\"dim\")))\ntsne_df <- data.frame(tsneX = tsne$Y[, 1],\n                      tsneY = tsne$Y[, 2])\nlimn_tour_link(\n  tsne_df,\n  fake_trees,\n  cols = dim1:dim10,\n  color = branches\n)\n\n\n\n\n\nFigure 4.1: Linked views of t-SNE dimension reduction with a tour of the fake trees data. The t-SNE view clearly shows ten 1D non-linear clusters, while the tour of the full 100 variables suggests a lot more variation in the data, and less difference between clusters.\n\n\n \n\n\n\nFigure 4.2: The t-SNE mapping of the penguins data inaccurately splits one of the clusters. The three clusters are clearly distinct when viewed with the tour.\n\n\n\n\nCode\nload(\"data/penguins_sub.rda\")\n\nset.seed(2022)\np_tsne <- Rtsne::Rtsne(penguins_sub[,2:5])\np_tsne_df <- data.frame(tsneX = p_tsne$Y[, 1], tsneY = p_tsne$Y[, 2])\nlimn_tour_link(\n  p_tsne_df,\n  penguins_sub,\n  cols = bl:bm,\n  color = species\n)"
  },
  {
    "objectID": "supervised.html",
    "href": "supervised.html",
    "title": "Supervised learning",
    "section": "",
    "text": "This chapter will methods for visualising data and models when there is a response variable."
  },
  {
    "objectID": "regression.html#support-vector-machine",
    "href": "regression.html#support-vector-machine",
    "title": "5  Regression methods",
    "section": "5.1 Support vector machine",
    "text": "5.1 Support vector machine\n% http://www.support-vector-machines.org/SVM_osh.html % wikipedia\nA support vector machine (SVM) (Vapnik 1999) is a binary classification method. An SVM looks for gaps between clusters in the data, based on the extreme observations in each class. In this sense it mirrors the graphical approach described at the start of this chapter, in which we searched for gaps between groups. We describe this method more fully than we did the other algorithms for two reasons: first, because of its apparent similarity to the graphical approach, and second, because it is difficult to find a simple explanation of the method in the literature.\nThe algorithm takes an \\(n \\times p\\) data matrix, where each column is scaled to \\([-1,1]\\) and each row is labeled as one of two classes (\\(y_i=+1\\) or \\(-1\\)), and finds a hyperplane that separates the two groups, if they are separable. Each row of the data matrix is a vector in \\(p\\)-dimensional space, denoted as\n% Should this be represented as a row instead of a column? dfs\n\\[\nX=\\left[ \\begin{array}{c}\n  x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p \\end{array} \\right]\n\\]\nand the separating hyperplane can be written as\n\\[\nW'X + b = 0\n\\]\nwhere \\(W = [ w_1~~ w_2 ~~ \\dots ~~ w_p]'\\) is the normal vector to the separating hyperplane and \\(b\\) is a constant. The best separating hyperplane is found by maximizing the margin of separation between the two classes as defined by two parallel hyperplanes:\n\\[\nW'X + b = 1, ~~~~~ W'X + b = -1.\n\\]\nThese hyperplanes should maximize the distance from the separating hyperplane and have no points between them, capitalizing on any gap between the two classes. The distance from the origin to the separating hyperplane is \\(|b|/||W||\\), so the distance between the two parallel margin hyperplanes is \\(2/||W||=2/\\sqrt{w_1^2+\\dots +w_p^2}\\). Maximizing this is the same as minimizing \\(||W||/2\\). To ensure that the two classes are separated, and that no points lie between the margin hyperplanes we need:\n\\[\nW'X_i + b \\geq 1, ~~~\\mbox{  or  } ~~~W'X_i + b \\leq -1 ~~~\\forall i=1, ..., n\n\\]\nwhich corresponds to\n\\[\\begin{eqnarray}\ny_i(W'X_i+b)\\geq 1 ~~~\\forall i=1, ..., n\n\\label{svm-crit}\n\\end{eqnarray}\\]\nThus the problem corresponds to\n\nInterestingly, only the points closest to the margin hyperplanes are needed to define the separating hyperplane. We might think of these points as lying on or close to the convex hull of each cluster in the area where the clusters are nearest to each other. These points are called support vectors, and the coefficients of the separating hyperplane are computed from a linear combination of the support vectors \\(W = \\sum_{i=1}^{s} y_i\\alpha_iX_i\\), where \\(s\\) is the number of support vectors. We could also use \\(W = \\sum_{i=1}^n y_i\\alpha_iX_i\\), where \\(\\alpha_i=0\\) if \\(X_i\\) is not a support vector. For a good fit the number of support vectors \\(s\\) should be small relative to \\(n\\). Fitting algorithms can achieve gains in efficiency by using only samples of the cases to find suitable support vector candidates; this approach is used in the SVMLight (Joachims 1999) software.\nIn practice, the assumption that the classes are completely separable is unrealistic. Classification problems rarely present a gap between the classes, such that there are no misclassifications. Cortes and Vapnik (1995) relaxed the separability condition to allow some misclassified training points by adding a tolerance value \\(\\epsilon_i\\) to Equation \\(\\ref{svm-crit}\\), which results in the modified criterion \\(y_i(W'X_i+b)>1-\\epsilon_i, \\epsilon_i\\geq 0\\). Points that meet this criterion but not the stricter one are called slack vectors.\nNonlinear classifiers can be obtained by using nonlinear transformations of \\(X_i\\), \\(\\phi(X_i)\\) (Boser, Guyon, and Vapnik 1992), which is implicitly computed during the optimization using a kernel function \\(K\\). Common choices of kernels are linear \\(K(x_i,x_j)=x_i'x_j\\), polynomial \\(K(x_i,x_j)=(\\gamma x_i'x_j+r)^d\\), radial basis \\(K(x_i,x_j)=\\exp(-\\gamma ||x_i-x_j||^2)\\), or sigmoid functions \\(K(x_i,x_j)=\\mbox{tanh}(\\gamma x_i'x_j+r)\\), where \\(\\gamma>0, r,\\) and \\(d\\) are kernel parameters.\n% She didn’t say to delete the terminating colon here, but by % analogy with these rest, I will. dfs\nThe ensuing minimization problem is formulated as\n\\[\n\\mbox{ minimizing } \\frac{1}{2}||W|| + C\\sum_{i=1}^n \\epsilon_i ~~ \\mbox{ subject to }\ny_i(W'\\phi(X)+b)>1-\\epsilon_i\n\\]\nwhere \\(\\epsilon_i\\geq 0\\), \\(C>0\\) is a penalty parameter guarding against over-fitting the training data and \\(\\epsilon\\) controls the tolerance for misclassification. The normal to the separating hyperplane \\(W\\) can be written as \\(\\sum_{i=1}^{n} y_i\\alpha_i{\\phi(X_i)}\\), where points other than the support and slack vectors will have \\(\\alpha_i=0\\). Thus the optimization problem becomes\n\\[\\begin{eqnarray*}\n\\mbox{ minimizing } \\frac{1}{2} \\sum_{i=1}^n\\sum_{j=1}^n y_iy_j\\alpha_i\\alpha_jK(X_i,X_j)+C\\sum_{i=1}^n \\epsilon_i \\\\ ~~~~~~~~~~~\\mbox{ subject to }\ny_i(W'\\phi(X)+b)>1-\\epsilon_i\n\\end{eqnarray*}\\]\n \nWe use the svm function in the e1071 package (Dimitriadou et al. 2006) of R, which uses libsvm (Chang and Lin 2006), to classify the oils of the four areas in the Southern region. SVM is a binary classifier, but this algorithm overcomes that limitation by comparing classes in pairs, fitting six separate classifiers, and then using a voting scheme to make predictions. To fit the SVM we also need to specify a kernel, or rely on the internal tuning tools of the algorithm to choose this for us. Automatic tuning in the algorithm chooses a radial basis, but we found that a linear kernel performed better, so that is what we used. (This accords with our earlier visual inspection of the data in ?sec-class-plots.) Here is the R code used to fit the model:\n> library(e1071)\n> olive.svm <- best.svm(factor(area) ~ ., data=d.olive.train)\n> olive.svm <- svm(factor(area) ~ ., data=d.olive.sth.train, \n  type=\"C-classification\", kernel=\"linear\")\n> table(d.olive.sth.train[,1], predict(olive.svm, \n  d.olive.sth.train))\n   \n      1   2   3   4\n  1  19   0   0   0\n  2   0  42   0   0\n  3   0   0 155   3\n  4   1   2   3  21\n> table(d.olive.sth.test[,1], predict(olive.svm, \n  d.olive.sth.test))\n   \n     1  2  3  4\n  1  6  0  0  0\n  2  1 12  1  0\n  3  0  0 46  2\n  4  1  1  0  7\n> support.vectors <- olive.svm$index[\n    abs(olive.svm$coefs[,1])<1 &\n    abs(olive.svm$coefs[,2])<1 & abs(olive.svm$coefs[,3])<1]\n> pointtype <- rep(0,323) # training\n> pointtype[247:323] <- 1 # test\n> pointtype[olive.svm$index] <- 2 # slack vectors\n> pointtype[support.vectors] <- 3 # support vectors\n> parea <- c(predict(olive.svm, d.olive.sth.train),\n    predict(olive.svm, d.olive.sth.test))\n> d.olive.svm <- cbind(rbind(d.olive.sth.train, \n    d.olive.sth.test), parea, pointtype)\n> gd <- ggobi(d.olive.svm)[1]\n> glyph_color(gd) <- c(6,3,2,9)[d.olive.svm$area]\nThese are our misclassification tables:\n\n\nThe training error is \\(9/246=0.037\\), and the test error is \\(6/77=0.078\\). (The training error is the same as that of the neural network classifier, but the test error is lower.) Most error is associated with Sicily, which we have seen repeatedly to be an especially difficult class to separate. In the training data there are no other errors, and in the test data there are just two samples from Calabria mistakenly classified. ?fig-olive-svm illustrates our examination of the misclassified cases, one in each row of the figure. (Points corresponding to Sicily were removed from all four plots.) Each of the two cases is brushed (using a filled red circle) in the plot of misclassification table and viewed in a linked 2D tour. Both of these cases are on the edge of their clusters so the confusion of classes is reasonable.\n% Figure 14\n% Figure 15\nThe linear SVM classifier uses 20 support vectors and 29 slack vectors to define the separating planes between the four areas. It is interesting to examine which points are selected as support vectors, and where they are located in the data space. For each pair of classes, we expect to find some projection in which the support vectors line up on either side of the margin of separation, whereas the slack vectors lie closer to the boundary, perhaps mixed in with the points of other classes.\n The plots in ?fig-olive-svm2 represent our use of the 2D tour, augmented by manual manipulation,~to look for these projections. (The Sicilian points are again removed.) The support vectors are represented by open circles and the slack vectors by open rectangles, and we have been able to find a number of projections in which the support vectors are on the opposing outer edge of the point clouds for each cluster.\nThe linear SVM does a very nice job with this difficult classification. The accuracy is almost perfect on three classes, and the misclassifications are quite reasonable mistakes, being points that are on the extreme edges of their clusters. However, this method joins the list of those defeated by the difficult problem of distinguishing the Sicilian oils from the rest.\n\n5.1.1 Examining boundaries\n\nFor some classification problems, it is possible to get a good picture of the boundary between two classes. With LDA and SVM classifiers the boundary is described by the equation of a hyperplane. For others the boundary can be determined by evaluating the classifier on points sampled in the data space, using either a regular grid or some more efficient sampling scheme.\n% Figure 16\n We use the R package classifly (Wickham 2006) to generate points illustrating boundaries, add those points to the original data, and display them in GGobi. ?fig-olive-classifly shows projections of boundaries between pairs of classes in the . In each example, we used the 2D tour with manual control~to focus the view on a projection that revealed the boundary between two groups.\n% Needs to be checked\nThe top two plots show tour projections of the North (purple) and Sardinia (green) oils where the two classes are separated and the boundary appears in gray. The LDA boundary (shown at left) slices too close to the Northern oils. This might be due to the violation of the LDA assumption that the two groups have equal variance; since that is not true here, it places the boundary too close to the group with the larger variance. The SVM boundary (at right) is a bit closer to the Sardinian oils than the LDA boundary is, yet it is still a tad too close to the oils from the North.\nThe bottom row of plots examines the more difficult classification of the areas of the South, focusing on separating the South Apulian oils (in pink), which is the largest sample, from the oils of the other areas (all in orange). Perfect separation between the classes does not occur. Both plots are tour projections showing SVM boundaries, the left plot generated by a linear kernel and the right one by a radial kernel. Recall that the radial kernel was selected automatically by the SVM software we used, whereas we actually chose to use a linear kernel. These pictures illustrate that the linear basis yields a more reasonable boundary between the two groups. The shape of the clusters of the two groups is approximately the same, and there is only a small overlap of the two. The linear boundary fits this structure neatly. The radial kernel wraps around the South Apulian oils.\n\n\n\n\nBoser, Bernhard E., Isabelle M. Guyon, and Vladimir N. Vapnik. 1992. “A Training Algorithm for Optimal Margin Classifiers.” In COLT ’92: Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144–52. New York: ACM Press. https://doi.org/http://doi.acm.org/10.1145/130385.130401.\n\n\nChang, C.-C., and C.-J. Lin. 2006. “LIBSVM: A Library for Support Vector Machines.” http://www.csie.ntu.edu.tw/\\(\\sim\\)cjlin/libsvm.\n\n\nCortes, C., and V. N. Vapnik. 1995. “Support-Vector Networks.” Machine Learning 20 (3): 273–97.\n\n\nDimitriadou, E., K. Hornik, F. Leisch, D. Meyer, and A. Weingessel. 2006. “e1071: Misc Functions of the Department of Statistics, TU Wien.” http://www.R-project.org.\n\n\nJoachims, T. 1999. “Making Large-Scale SVM Learning Practical.” In Advances in Kernel Methods-Support Vector Learning, edited by B. Schölkopf, C. Burges, and A. Smola. Cambridge, MA: MIT Press.\n\n\nVapnik, V. N. 1999. The Nature of Statistical Learning Theory. New York: Springer.\n\n\nWickham, H. 2006. “classifly: Classify and Explore a Data Set.” http://www.R-project.org."
  },
  {
    "objectID": "LDA.html#definition",
    "href": "LDA.html#definition",
    "title": "5  Linear discriminant analysis",
    "section": "5.1 Definition",
    "text": "5.1 Definition\nFisher’s linear discriminant (Fisher 1936) computes a linear combination of the variables that separates two classes by comparing the differences between class means with the variance of values within each class. It makes no assumptions about the distribution of the data.\nLinear discriminant analysis (LDA), as proposed by Rao (1948), formalizes Fisher’s approach, by recognising that it arises from making the assumption that the data values for each class arise from a \\(p\\)-dimensional multivariate normal distribution, sharing a common variance-covariance matrix with data from other classes. When this assumption holds, Fisher’s linear discriminant gives the optimal separation between the two groups.\nFor two equally weighted groups, where \\(Y\\) is coded as \\(\\{0, 1\\}\\), the LDA rule is:\nAllocate a new observation \\(X_0\\) to group 1 if\n\\[(\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}X_0 \\geq\n  \\frac{1}{2}(\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}\n  (\\bar{X}_1+\\bar{X}_2)\\]\nelse allocate it to group 2,\nwhere ${X}_k $ are the class mean vectors of an \\(n\\times p\\) data matrix \\(X_k ~~(k=1,2)\\),\n\\[S_{\\rm pooled} = \\frac{(n_1-1) S_1}{(n_1-1)+(n_2-1)} + \\frac{(n_2-1) S_2}{(n_1-1)+(n_2-1)}\\]\nis the pooled variance-covariance matrix, and\n\\[S_k = \\frac{1}{n-1}\\sum_{i=1}^{n}\n(X_{ki}-\\bar{X}_k)(X_{ki}-\\bar{X}_k)', ~~k=1,2\\]\nis the class variance–covariance matrix. The linear discriminant part of this rule is \\((\\bar{X}_1-\\bar{X}_2)'S^{-1}_{\\rm pooled}\\), which defines the linear combination of variables that best separates the two groups. To define a classification rule, we compute the value of the new observation \\(X_0\\) on this line and compare it with the value of the average of the two class means \\((\\bar{X}_1+\\bar{X}_2)/2\\) on the same line.\nFor multiple \\((g)\\) classes, the rule and the discriminant space are constructed using the between-group sum-of-squares matrix,\n\\[B=\\sum_{k=1}^g n_k(\\bar{X}_k-\\bar{X})(\\bar{X}_k-\\bar{X})'\\]\nwhich measures the differences between the class means, compared with the overall data mean \\(\\bar{X}\\) and the within-group sum-of-squares matrix,\n\\[W =\n\\sum_{k=1}^g\\sum_{i=1}^{n_k}\n(X_{ki}-\\bar{X}_k)(X_{ki}-\\bar{X}_k)'\\]\nwhich measures the variation of values around each class mean. The linear discriminant space is generated by computing the eigenvectors (canonical coordinates) of \\(W^{-1}B\\), and this is the space where the group means are most separated with respect to the pooled variance–covariance. The resulting classification rule is to allocate a new observation to the class with the highest value of\n\\[\\bar{X}_k'S^{-1}_{\\rm pooled}X_0 -\n\\frac{1}{2}\\bar{X}_k'S^{-1}_{\\rm pooled}\\bar{X}_k ~~~k=1,...,g\\]\nwhich results in allocating the new observation into the class with the closest mean."
  },
  {
    "objectID": "LDA.html#checking-assumptions",
    "href": "LDA.html#checking-assumptions",
    "title": "5  Linear discriminant analysis",
    "section": "5.2 Checking assumptions",
    "text": "5.2 Checking assumptions\nThis LDA approach is widely applicable, but it is useful to check the underlying assumptions on which it depends: (1) that the cluster structure corresponding to each class forms an ellipse, showing that the class is consistent with a sample from a multivariate normal distribution, and (2) that the variance of values around each mean is nearly the same. Figure 5.1 and Figure 5.2 illustrates two datasets, of which only one is consistent with these assumptions. Other parametric models, such as quadratic discriminant analysis or logistic regression, also depend on assumptions about the data which should be validated. \n\n\nCode\nlibrary(dplyr)\nlibrary(colorspace)\nlibrary(ggplot2)\nlibrary(mulgar)\nlibrary(MASS)\nlibrary(ggpubr)\nload(\"data/penguins_sub.rda\")\nlda1 <- ggplot(penguins_sub, aes(x=bl, \n                         y=bd, \n                         colour=species)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(-2.5, 3) + ylim(-2.5, 2.5) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio = 1) \np_ell <- NULL\nfor (i in unique(penguins_sub$species)) {\n  x <- penguins_sub %>% dplyr::filter(species == i)\n  e <- gen_xvar_ellipse(x[,1:2], n=150, nstd=1.5)\n  e$species <- i\n  p_ell <- bind_rows(p_ell, e)\n}\nlda2 <- ggplot(p_ell, aes(x=bl, \n                         y=bd, \n                         colour=species)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(-2.5, 3) + ylim(-2.5, 2.5) +\n  ggtitle(\"(b)\") +\n  theme(aspect.ratio = 1)\nggarrange(lda1, lda2, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n\n\n\n\n\nFigure 5.1: Scatterplot of flipper length by bill length of the penguins data, and corresponding variance-covariance ellipses. There is a small amount of difference between the ellipses, but they are similar enough to be confident in assuming the population variance-covariances are equal.\n\n\n\n\n\n\nCode\n# Now repeat for a data set that violates assumptions\ndata(bushfires)\nlda3 <- ggplot(bushfires, aes(x=log_dist_cfa, \n                         y=log_dist_road, \n                         colour=cause)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(6, 11) + ylim(-1, 10.5) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio = 1)\nb_ell <- NULL\nfor (i in unique(bushfires$cause)) {\n  x <- bushfires %>% dplyr::filter(cause == i)\n  e <- gen_xvar_ellipse(x[,c(57, 59)], n=150, nstd=2)\n  e$cause <- i\n  b_ell <- bind_rows(b_ell, e)\n}\nlda4 <- ggplot(b_ell, aes(x=log_dist_cfa, \n                         y=log_dist_road, \n                         colour=cause)) +\n  geom_point() +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  xlim(6, 11) + ylim(-1, 10.5) +\n  ggtitle(\"(b)\") +\n  theme(aspect.ratio = 1)\nggarrange(lda3, lda4, ncol=2, \n          common.legend = TRUE, legend = \"bottom\")\n\n\n\n\n\nFigure 5.2: Scatterplot of distance to cfa and road for the bushfires data, and corresponding variance-covariance ellipses. There is a lot of difference between the ellipses, so it cannot be assumed that the population variance-covariances are equal.\n\n\n\n\n\n\nCode\nlibrary(tourr)\np_ell <- NULL\nfor (i in unique(penguins_sub$species)) {\n  x <- penguins_sub %>% dplyr::filter(species == i)\n  e <- gen_xvar_ellipse(x[,1:4], n=150, nstd=1.5)\n  e$species <- i\n  p_ell <- bind_rows(p_ell, e)\n}\np_ell$species <- factor(p_ell$species)\nload(\"data/penguins_tour_path.rda\")\nanimate_xy(p_ell[,1:4], col=factor(p_ell$species))\nrender_gif(penguins_sub[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\", col=penguins_sub$species),\n           gif_file=\"gifs/penguins_lda1.gif\",\n           frames=500,\n           loop=FALSE)\nrender_gif(p_ell[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\", col=p_ell$species),\n           gif_file=\"gifs/penguins_lda2.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\n\n\n\n\n(a) Data\n\n\n\n\n\n\n\n(b) Variance-covariance ellipses\n\n\n\n\nFigure 5.3: Checking the assumption of equal variance-covariance matrices for the 4D penguins data."
  },
  {
    "objectID": "LDA.html#examining-results",
    "href": "LDA.html#examining-results",
    "title": "5  Linear discriminant analysis",
    "section": "5.3 Examining results",
    "text": "5.3 Examining results\n\n\nCode\nlibrary(classifly)\nlibrary(MASS)\npenguins_lda <- lda(species ~ ., penguins_sub, prior = c(1/3, 1/3, 1/3))\np_lda_boundaries <- explore(penguins_lda, penguins_sub)\nanimate_slice(p_lda_boundaries[p_lda_boundaries$.TYPE == \"simulated\",1:4], col=p_lda_boundaries[p_lda_boundaries$.TYPE == \"simulated\",6], v_rel=0.02, axes=\"bottomleft\")\nrender_gif(p_lda_boundaries[p_lda_boundaries$.TYPE == \"simulated\",1:4],\n           planned_tour(pt1),\n           display_slice(v_rel=0.02, \n             col=p_lda_boundaries[ p_lda_boundaries$.TYPE == \"simulated\",6], \n             axes=\"bottomleft\"),                     gif_file=\"gifs/penguins_lda_boundaries.gif\",\n           frames=500,\n           loop=FALSE\n           )\n\n\n\n\n\n\n\nFisher, R. A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” Annals of Eugenics 7: 179–88.\n\n\nJohnson, R. A., and D. W. Wichern. 2002. Applied Multivariate Statistical Analysis (5th Ed). Englewood Cliffs, NJ: Prentice-Hall.\n\n\nRao, C. R. 1948. “The Utilization of Multiple Measurements in Problems of Biological Classification (with Discussion).” Journal of the Royal Statistical Society, Series B 10: 159–203.\n\n\nRipley, B. 1996. Pattern Recognition and Neural Networks. Cambridge: Cambridge University Press.\n\n\nVenables, W. N., and B. Ripley. 2002. Modern Applied Statistics with S. New York: Springer-Verlag."
  },
  {
    "objectID": "forests.html#trees",
    "href": "forests.html#trees",
    "title": "13  Trees and forests",
    "section": "13.1 Trees",
    "text": "13.1 Trees\nThe tree algorithm Breiman et al. (1984) is a simple and versatile algorithmic method for supervised classification. The basic tree algorithm generates a classification rule by sequentially splitting the data into two buckets. Splits are made between sorted data values of individual variables, with the goal of obtaining pure classes on each side of the split. The inputs for a simple tree classifier commonly include (1) an impurity measure, an indication of the relative diversity among the cases in the terminal nodes; (2) a parameter that sets the minimum number of cases in a node, or the minimum number of observations in a terminal node of the tree; and (3) a complexity measure that controls the growth of a tree, balancing the use of a simple generalizable tree against a more accurate tree tailored to the sample. When applying tree methods, exploring the effects of the input parameters on the tree is instructive; for example, it helps us to assess the stability of the tree model.\nAlthough algorithmic models do not depend on distributional assumptions, that does not mean that every algorithm is suitable for all data. For example, the tree model works best when all variables are independent within each class, because it does not take such dependencies into account. Visualization can help us to determine whether a particular model should be applied. In classification problems, it is useful to explore the cluster structure, comparing the clusters with the classes and looking for evidence of correlation within each class. The plots in Figure 5.1 and Figure 5.3 shows a strong correlation between the variables within each species, which indicates that the tree model may not give good results for the penguins data. We’ll show how this is the case with two variables initially, and then extend to the four variables.\n\n\n\n\n\nCode\nlibrary(mulgar)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(colorspace)\nlibrary(classifly)\nlibrary(ggplot2)\n\nload(\"data/penguins_sub.rda\")\np_bl_bd_tree <- rpart(species~bl+bd, data=penguins_sub)\nrpart.plot(p_bl_bd_tree, box.palette=\"Grays\")\n\n\n\n\n\n(a) Default tree fit\n\n\n\n\n\n\nCode\np_bl_bd_tree_boundaries <- explore(p_bl_bd_tree, penguins_sub)\nggplot(p_bl_bd_tree_boundaries) +\n  geom_point(aes(x=bl, y=bd, colour=species, shape=.TYPE)) + \n  scale_color_discrete_qualitative(\"Dark 3\") +\n  scale_shape_manual(values=c(46, 16)) +\n  theme_minimal() +\n  theme(aspect.ratio = 1, legend.position = \"none\")\n\n\n\n\n\n(b) Boundaries of tree fit\n\n\n\n\n\nFigure 13.1: The correlation between variables causes problems for using a tree model on the penguins data.\n\n\nThe plots in Figure 13.1 show the inadequacies of the tree fit. The background color indicates the class predictions, and thus boundaries produced by the tree fit. They can be seen to be boxy, and missing the elliptical nature of the penguin clusters. This produces errors in the classification of observations which are indefensible. One could always force the tree to fit the data more closely by adjusting the parameters, but the main problem persists: that one is trying to fit elliptical data using boxes.\nThe boundaries for the tree model on all four variables of the penguins data can be viewed similarly using the tour. The default fitted tree is delightfully simple, with just six splits of the data.\n\n\nCode\np_tree <- rpart(species~., data=penguins_sub)\nrpart.plot(p_tree, box.palette=\"Grays\")\n\np_tree_boundaries <- explore(p_tree, penguins_sub)\nanimate_slice(p_tree_boundaries[p_tree_boundaries$.TYPE == \"simulated\",1:4], col=p_tree_boundaries[p_tree_boundaries$.TYPE == \"simulated\",6], v_rel=0.02, axes=\"bottomleft\")\nload(\"data/penguins_tour_path.rda\")\nrender_gif(p_tree_boundaries[p_tree_boundaries$.TYPE == \"simulated\",1:4],\n           planned_tour(pt1),\n           display_slice(v_rel=0.02, \n             col=p_tree_boundaries[p_tree_boundaries$.TYPE == \"simulated\",6], \n             axes=\"bottomleft\"),                     gif_file=\"gifs/penguins_tree_boundaries.gif\",\n           frames=500,\n           loop=FALSE\n           )\n\n\n\n\n\n\n\n\n\n(a) Boundaries produced by the LDA model.\n\n\n\n\n\n\n\n(b) Boundaries produced by the tree model.\n\n\n\n\nFigure 13.2: Comparison of the boundaries produced by the LDA model and the tree models."
  },
  {
    "objectID": "forests.html#random-forests",
    "href": "forests.html#random-forests",
    "title": "13  Trees and forests",
    "section": "13.2 Random forests",
    "text": "13.2 Random forests\nA random forest Breiman and Cutler (2004) is a classifier that is built from multiple trees generated by randomly sampling the cases and the variables. The random sampling (with replacement) of cases has the fortunate effect of creating a training (“in-bag”) and a test (“out-of-bag”) sample for each tree computed. The class of each case in the out-of-bag sample for each tree is predicted, and the predictions for all trees are combined into a vote for the class identity.\nA random forest is a computationally intensive method, a “black box” classifier, but it produces several diagnostics that make the outcome less mysterious. Some diagnostics that help us to assess the model are the votes, the measure of variable importance, and the proximity matrix.\nHere we show how to use the randomForest (Liaw et al. 2006) votes matrix for the penguins datato investigate confusion between classes, and observations which are problematic to classify. With only three classes the votes matrix is only a 2D object, and thus easy to examine. With four or more classes the votes matrix needs to be examined in a tour.\n\n\nCode\nlibrary(randomForest)\nlibrary(dplyr)\npenguins_rf <- randomForest(species~.,\n                             data=penguins_sub,\n                             importance=TRUE)\n# animate_xy(penguins_rf$votes, col=penguins_sub$species)\n\n# Project 4D into 3D\nlibrary(geozoo)\nproj <- t(geozoo::f_helmert(3)[-1,])\np_rf_v_p <- as.matrix(penguins_rf$votes) %*% proj\ncolnames(p_rf_v_p) <- c(\"x1\", \"x2\")\np_rf_v_p <- p_rf_v_p %>%\n  as.data.frame() %>%\n  mutate(species = penguins_sub$species)\n  \n# Add simplex\nsimp <- simplex(p=2)\nsp <- data.frame(cbind(simp$points), simp$points[c(2,3,1),])\ncolnames(sp) <- c(\"x1\", \"x2\", \"x3\", \"x4\")\nsp$species = sort(unique(penguins_sub$species))\nlibrary(ggthemes)\np_ternary <- ggplot() +\n  geom_segment(data=sp, aes(x=x1, y=x2, xend=x3, yend=x4)) +\n  geom_text(data=sp, aes(x=x1, y=x2, label=species),\n            nudge_x=c(-0.06, 0.07, 0),\n            nudge_y=c(0.05, 0.05, -0.05)) +\n  geom_point(data=p_rf_v_p, aes(x=x1, y=x2, colour=species), size=2, alpha=0.5) +\n  scale_color_discrete_qualitative(\"Dark 3\") +\n  theme_map() +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n\n\n\nCode\nrender_gif(penguins_rf$votes,\n           grand_tour(),\n           display_xy(v_rel=0.02, \n             col=penguins_sub$species, \n             axes=\"bottomleft\"),                     gif_file=\"gifs/penguins_rf_votes.gif\",\n           frames=500,\n           loop=FALSE\n           )\n\n\n\n\n\n\n\n\n\n(a) Votes matrix in a tour.\n\n\n\n\n\n\n\n\n(b) Votes marix in its 2D space, a ternary diagram.\n\n\n\n\n\nFigure 13.3: Examining the votes matrix from a random forest fit to the penguins.\n\n\nThe votes matrix, reports the proportion of trees each observation is classified as each class. From the tour of the votes matrix, it can be seen to be 2D in 3D space. This is due to the constraint that the three proportions for each observation sum to 1. Using a Helmert matrix, this data can be projected into the 2D space, or more generally the \\((g-1)\\)-dimensional space where it resides. In 2D this is called a ternary diagram, and in higher dimensions the bounding shapes might be considered to be a simplex. The vertices of this shape correspond to \\((1,0,0), (0,1,0), (0,0,1)\\) (and analogously for higher dimensions), which represent perfect confidence, that an observation is classified into that group all the time.\nWhat we can see here is a concentration of points in the corners of the triangle indicates that most of the penguins are confidently classified into their correct class. Then there is more separation between the Gentoo and the others, than between Chinstrap and Adelie. That means that as a group Gentoo are more distinguishable. Only one of the Gentoo penguins has substantial confusion, mostly confused as a Chinstrap, but occasionally confused as an Adelie – if it was only ever confused as a Chinstrap it would fall on the edge between Gentoo and Chinstrap. There are quite a few Chinstrap and Adelie penguins confused as each other, with a couple of each more confidently predicted to be the other class. This can be seen because there are points of the wrong colour close to those vertices.\nThe votes matrix is useful for investigating the fit, but one should remember that there are some structural elements of this data that don’t lend themselves to tree models. Although a forest has the capacity to generate non-linear boundaries by combining predictions from multiple trees, it is still based on the boxy boundaries of trees. This makes it less suitable for the penguins data with elliptical classes. You could use the techniques from the previsou section to explore the boundaries produced by the forest, and you will find that the are more boxy than the LDA models.\nTo examine a vote matrix for a problem with more classes, we’ll look at the bushfires data. There are four classes: accident, arson, burning_off, lightning. It is highly imbalanced, with most observations belonging to the lightning class, fires ignited by lightning. We can see that most of the observations lie on the face of lightning, arson and accident. The handful of the burning_off observations lie off this plane, in the direction of burning-off, so are less confused with the other three classes. This could be expected because burning off is highly regulated, and tends to occur before the bushfire season is at risk of starting. The arson cases are hard to classify, frequently confused with lightning or accident, and occasionally burning off. Lightning and accident have many more observations that are confidently classified correctly.\n\n\nCode\nlibrary(mulgar)\nlibrary(tourr)\ndata(bushfires)\n\nbushfires_sub <- bushfires[,c(5, 8:45, 48:55, 57:60)] %>%\n  mutate(cause = factor(cause))\n\nbushfires_pca <- prcomp(bushfires_sub[,-51],\n                        scale=TRUE, retx=TRUE)\nggscree(bushfires_pca)\n\nbushfires_pcs <- bushfires_pca$x[,1:7] %>%\n  as_tibble() %>%\n  mutate(cause = factor(bushfires$cause))\n\nlibrary(tourr)\nanimate_xy(bushfires_pcs[,1:7],\n           guided_tour(lda_pp(bushfires_pcs$cause)),\n           col=bushfires_pcs$cause)\n\nbushfires_pca$rotation[,2]\nggplot(bushfires, aes(x=FOR_CODE)) + geom_density()\nggplot(bushfires, aes(x=COVER)) + geom_density()\nggplot(bushfires, aes(x=HEIGHT)) + geom_density()\nggplot(bushfires, aes(x=FOREST)) + geom_density()\nggplot(bushfires, aes(x=arf28)) + geom_density()\n\nlibrary(randomForest)\nbushfires_rf <- randomForest(cause~.,\n                             data=bushfires_sub,\n                             importance=TRUE)\nbushfires_rf_votes <- bushfires_rf$votes %>%\n  as_tibble() %>%\n  mutate(cause = bushfires_sub$cause)\n\nanimate_xy(bushfires_rf_votes[,1:4],\n           col=bushfires_rf_votes$cause)\n\n# Project 4D into 3D\nlibrary(geozoo)\nproj <- t(geozoo::f_helmert(4)[-1,])\nb_rf_v_p <- as.matrix(bushfires_rf_votes[,1:4]) %*% proj\ncolnames(b_rf_v_p) <- c(\"x1\", \"x2\", \"x3\")\nb_rf_v_p <- b_rf_v_p %>%\n  as.data.frame() %>%\n  mutate(cause = bushfires_sub$cause)\n  \n# Add simplex\nsimp <- simplex(p=3)\nsp <- data.frame(simp$points)\ncolnames(sp) <- c(\"x1\", \"x2\", \"x3\")\nsp$cause = \"\"\nb_rf_v_p_s <- bind_rows(sp, b_rf_v_p) %>%\n  mutate(cause = factor(cause))\nlabels <- c(\"accident\" , \"arson\", \n                \"burning_off\", \"lightning\", \n                rep(\"\", 1021))\nanimate_xy(b_rf_v_p_s[,1:3], col = b_rf_v_p_s$cause, \n           axes = \"off\", half_range = 1.3,\n           edges = as.matrix(simp$edges),\n           obs_labels = labels)\nrender_gif(b_rf_v_p_s[,1:3],\n           grand_tour(),\n           display_xy(col = b_rf_v_p_s$cause, \n           axes = \"off\", half_range = 1.3,\n           edges = as.matrix(simp$edges),\n           obs_labels = labels),\n           gif_file=\"gifs/bushfires_votes.gif\",\n           frames=500)  \n\n\n\n\n\nFigure 13.4: The 3D votes matrix for the four class bushfires data in a tour."
  },
  {
    "objectID": "forests.html#exercises",
    "href": "forests.html#exercises",
    "title": "13  Trees and forests",
    "section": "Exercises",
    "text": "Exercises\n\nCompare the boundaries from the random forest model on the penguins data to that of the (a) LDA model, (b) a default tree model.\nTinker with the parameters of the tree model to force it to fit a tree more closely to the data. Compare the boundaries from this with the default tree, and with the forest model. Is it less boxy than the default tree, but more boxy than the forest model? \n\n\n\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.\n\n\nBreiman, L., and A. Cutler. 2004. “Random Forests.” http://www.math.usu.edu/\\(\\sim\\)adele/forests/cc_home.htm.\n\n\nBreiman, L., J. Friedman, C. Olshen, and C. Stone. 1984. Classification and Regression Trees. Monterey, CA: Wadsworth; Brooks/Cole.\n\n\nLiaw, A., M. Wiener, L. Breiman, and A. Cutler. 2006. “randomForest: Breiman and Cutler’s Random Forests for Classification and Regression.” http://www.R-project.org."
  },
  {
    "objectID": "nn.html",
    "href": "nn.html",
    "title": "8  Neural networks and deep learning",
    "section": "",
    "text": "Neural networks for classification can be thought of as additive models where explanatory variables are transformed, usually through a logistic function, added to other explanatory variables, transformed again, and added again to yield class predictions. Aside from the data mining literature, mentioned earlier, a good comprehensive and accessible description for statisticians can be found in . The model can be formulated as:\n[ = f(x) = (+{h=1}^{s} w{h}(h+{i=1}^{p} w_{ih}x_i)) ]\n\nwhere \\(x\\) is the vector of explanatory variable values, \\(y\\) is the target value, \\(p\\) is the number of variables, \\(s\\) is the number of nodes in the single hidden layer, and \\(\\phi\\) is a fixed function, usually a linear or logistic function. This model has a single hidden layer and univariate output values. The model is fit by minimizing the sum of squared differences between observed values and fitted values, and the minimization does not always converge. A neural network is a black box that accepts inputs, computes, and spits out predictions. With graphics, some insight into the black box can be gained. We use the feed-forward neural network provided in the {} package of R to illustrate.\nWe continue to work with , and we look at the performance of the neural network in classifying the oils in the four areas of the South, a difficult challenge. Because the software does not include a method for computing the predictive error, we break the data into training and test samples so we can better estimate the predictive error. (We could tweak the neural network to perfectly fit all the data, but then we could not estimate how well it would perform with new data.)\n% this may need a few more words\nAfter trying several values for \\(s\\), the number of nodes in the hidden layer, we chose \\(s=4\\); we also chose a linear \\(\\phi\\), \\(decay=0.005\\), and \\(range=0.06\\). We fit the model using many different random starting values, rejecting the results until it eventually converged to a solution with a reasonably low error:\n% Insert page break to avoid breaking the R output.\nBelow are the misclassification tables for the training and test samples.\n\n\nThe training error is \\(9/246=0.037\\), and the test error is \\(12/77=0.156\\). The overall errors, as in the random forest model, are not uniform across classes. This is particularly obvious in the test error table: The error in classifying North Apulian oils is close to a third, and it is even worse for Sicilian oils, which have an almost even chance of being misclassified.\nOur exploration of the misclassifications is shown in Fig.~\\(\\ref{olive-nn}\\). (The troublesome Sicilian oils have been excluded from all plots in this figure.) Consider first the plots in the top row. The left-hand plot shows the misclassification table. Two samples of oils from North Apulia (orange \\(+\\)) have been incorrectly classified as South Apulian (pink \\(\\times\\)), and these two points have been brushed as filled orange circles. Note where these points fall in the next two plots, which are linked 2D tour projections. One of the two misclassified points is on the edge of the cluster of North Apulian points, close to the Calabrian cluster. It is understandable that there might be some confusion about this case. The other sample is on the outer edge of the North Apulian cluster, but it is far from the Calabrian cluster — this should not have been confused.\n% Figure 13\nIn the bottom row of plots, we follow the same procedure to examine the single North Apulian sample misclassified as South Apulian. It is painted as a filled orange circle in the misclassification plot and viewed in a tour. This point is on the outer edge of the North Apulian cluster, but it is closer to the Calabrian cluster than the South Apulian cluster. It would be understandable for it to be misclassified as Calabrian, so it is puzzling that it is misclassified as South Apulian.\nIn summary, a neural network is a black box method for tackling tough classification problems. It will generate different solutions each time the net is fit, some much better than others. When numerical measures suggest that a reasonable model has been found, graphics can be used to inspect the model in more detail."
  },
  {
    "objectID": "unsupervised.html#sec-clust-bg",
    "href": "unsupervised.html#sec-clust-bg",
    "title": "Unsupervised learning",
    "section": "Background",
    "text": "Background\nBefore we can begin finding groups of cases that are similar, we need to decide on a definition of similarity. How is similarity defined? Consider a dataset with three cases \\((a_1, a_2, a_3)\\) and four variables \\((V_1, V_2, V_3, V_4)\\), described in matrix format as\n\n\\[\n\\require{mathtools}\n\\definecolor{grey}{RGB}{192, 192, 192}\n\\]\n\n\\[\\begin{align*}\nX = \\begin{bmatrix}\n& {\\color{grey} V_1} & {\\color{grey} V_2} & {\\color{grey} V_3} & {\\color{grey} V_4} \\\\\\hline\n{\\color{grey} a_1} | & x_{11} & x_{12} & x_{13} & x_{14} \\\\\n{\\color{grey} a_2} | & x_{21} & x_{22} & x_{23} & x_{24} \\\\\n{\\color{grey} a_3} | & x_{31} & x_{32} & x_{33} & x_{34}    \n\\end{bmatrix}\n=  \\begin{bmatrix}\n& {\\color{grey} V_1} & {\\color{grey} V_2} & {\\color{grey} V_3} & {\\color{grey} V_4} \\\\\\hline\n{\\color{grey} a_1} | & 7.3 & 7.6 & 7.7 & 8.0 \\\\\n{\\color{grey} a_2} | & 7.4 & 7.2 & 7.3 & 7.2 \\\\\n{\\color{grey} a_3} | & 4.1 & 4.6 & 4.6 & 4.8\n\\end{bmatrix}\n\n\\end{align*}\\]\nwhich is plotted in Figure 2. The Euclidean distance between two cases (rows of the matrix) with \\(p\\) elements is defined as\n\\[\\begin{align*}\nd_{\\rm Euc}(a_i,a_j) &=& ||a_i-a_j|| %\\\\\n% &=& \\sqrt{(x_{i1}-x_{j1})^2+\\dots + (x_{ip}-x_{jp})^2},\n~~~~~~i,j=1,\\dots, n,\n\\end{align*}\\]\nwhere \\(||x_i||=\\sqrt{x_{i1}^2+x_{i2}^2+\\dots +x_{ip}^2}\\). For example, the Euclidean distance between cases 1 and 2 in the above data, is\n\\[\\begin{align*}\nd_{\\rm Euc}(a_1,a_2) &= \\sqrt{(7.3-7.4)^2+(7.6-7.2)^2+ (7.7-7.3)^2+(8.0-7.2)^2} \\\\\n&= 1.0\n\\end{align*}\\]\n\nFor the three cases, the interpoint Euclidean distance matrix is\n\\[\\begin{align*}\nd_{\\rm Euc} =\n\\left[ \\begin{array}{ccc}\n0.0  ~&     &   \\\\\n1.0 ~&  0.0 ~  &  \\\\\n6.3 ~& 5.5 ~&  0.0 ~ \\\\\n\\end{array} \\right]\n\\begin{array}{r}\na_1 \\\\ a_2 \\\\ a_3 \\\\\n\\end{array}\n\\end{align*}\\]\n\n\n\n\n\nCode\nx <- tibble::tibble(V1 = c(7.3, 7.4, 4.1),\n                    V2 = c(7.6, 7.2, 4.6),\n                    V3 = c(7.7, 7.3, 4.6),\n                    V4 = c(8.0, 7.2, 4.8),\n                    point = factor(c(\"a1\", \"a2\", \"a3\")))\nlibrary(GGally)\nlibrary(colorspace)\nlibrary(gridExtra)\npscat <- ggpairs(x, columns=1:4,\n                 upper=list(continuous=\"points\"),\n                 diag=list(continuous=\"blankDiag\"),\n                 axisLabels=\"internal\",\n                 ggplot2::aes(colour=point)) +\n    scale_colour_discrete_qualitative(\n      palette = \"Dark 3\") +\n    theme(aspect.ratio=1)\npscat\n\n\n\n\n\n\n\nCode\nppar <- ggparcoord(x, columns=1:4, \n                   groupColumn = 5, \n                   scale = \"globalminmax\") +\n          scale_colour_discrete_qualitative(\n            palette = \"Dark 3\") +\n  xlab(\"\") + ylab(\"\") + \n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank(),\n        legend.title = element_blank())\nppar\n\n\n\n\n\n\nFigure 2: The scatterplot matrix (left) shows that cases \\(a_1\\) and \\(a_2\\) have similar values. The parallel coordinate plot (right) allows a comparison of other structure, which shows the similarity in the trend of the profiles on cases \\(a_1\\) and \\(a_3\\).\n\n\nCases \\(a_1\\) and \\(a_2\\) are more similar to each other than they are to case \\(a_3\\), because the Euclidean distance between cases \\(a_1\\) and \\(a_2\\) is much smaller than the distance between cases \\(a_1\\) and \\(a_3\\) and between cases \\(a_2\\) and \\(a_3\\).\nThere are many different ways to calculate similarity. Similarity measures based on correlation distance have become common. Correlation distance is typically used where similarity of structure is more important than similarity in magnitude.\n\nAs an example, see the parallel coordinate plot of the sample data at the right of Figure 2. Cases \\(a_1\\) and \\(a_3\\) are widely separated, but their shapes are similar (low, medium, medium, high). Case \\(a_2\\), although overlapping with Case \\(a_1\\), has a very different shape (high, medium, medium, low). The correlation between two cases is defined as\n\\[\\begin{align*}\n\\rho(a_i,a_j) = \\frac{(a_i-c_i)'(a_j-c_j)}\n{\\sqrt{(a_i-c_i)'(a_i-c_i)} \\sqrt{(a_j-c_j)'(a_j-c_j)}}\n\\label{corc}\n\\end{align*}\\]\nWhen \\(c_i, c_j\\) are the sample means \\(\\bar{a}_i,\\bar{a}_j\\), then \\(\\rho\\) is the Pearson correlation coefficient. If, indeed, they are set at 0, as is commonly done, \\(\\rho\\) is a generalized correlation that describes the angle between the two data vectors. The correlation is then converted to a distance metric; one equation for doing so is as follows:\n\\[\\begin{align*}\nd_{\\rm Cor}(a_i,a_j) = \\sqrt{2(1-\\rho(a_i,a_j))}\n\\end{align*}\\]\nThe above distance metric will treat cases that are strongly negatively correlated as the most distant.\nThe interpoint distance matrix for the sample data using \\(d_{\\rm Cor}\\) and the Pearson correlation coefficient is\n\\[\\begin{align*}\nd_{\\rm Cor} =\n\\left[ \\begin{array}{rrrrrrrrr}\n0.0  ~&     &  \\\\\n3.6 ~ & 0.0 ~ &  \\\\\n0.1 ~ & 3.8 ~ &  0.0 ~\\\\\n\\end{array} \\right]\n\\begin{array}{r}\na_1 \\\\ a_2 \\\\ a_3 \\\\\n\\end{array}\n\\end{align*}\\]\nBy this metric, cases \\(a_1\\) and \\(a_3\\) are the most similar, because the correlation distance is smaller between these two cases than the other pairs of cases. \nNote that these interpoint distances differ dramatically from those for Euclidean distance. As a consequence, the way the cases would be clustered is also be very different. Choosing the appropriate distance measure is an important part of a cluster analysis.\nAfter a distance metric has been chosen and a cluster analysis has been performed, the analyst must evaluate the results, and this is actually a difficult task. A cluster analysis does not generate \\(p\\)-values or other numerical criteria, and the process tends to produce hypotheses rather than testing them. Even the most determined attempts to produce the “best” results using modeling and validation techniques may result in clusters that, although seemingly significant, are useless for practical purposes. As a result, cluster analysis is best thought of as an exploratory technique, and it can be quite useful despite the lack of formal validation because of its power in data simplification.\nThe context in which the data arises is the key to assessing the results. If the clusters can be characterized in a sensible manner, and they increase our knowledge of the data, then we are on the right track. To use an even more pragmatic criterion, if a company can gain an economic advantage by using a particular clustering method to carve up their customer database, then that is the method they should use.\n\n\n\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning with r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377.\n\n\nGiordani, Paolo, Maria Brigida Ferraro, and Francesca Martella. 2020. An Introduction to Clustering with r. Springer Singapore. https://doi.org/10.1007/978-981-13-0553-5.\n\n\nHennig, Christian, Marina Meila, Fionn Murtagh, and Roberto Rocci, eds. 2015. Handbook of Cluster Analysis. Chapman; Hall/CRC. https://doi.org/10.1201/b19706.\n\n\nKassambara, Alboukadel. 2017. Practical Guide to Cluster Analysis in r: Unsupervised Machine Learning. STHDA.\n\n\nLeisch, Friedrich, and Bettina Gruen. 2023. “CRAN Task View: Cluster Analysis & Finite Mixture Models.” https://cran.r-project.org/web/views/Cluster.html.\n\n\nVenables, W. N., and B. Ripley. 2002. Modern Applied Statistics with S. New York: Springer-Verlag."
  },
  {
    "objectID": "brush-and-spin.html#exercises",
    "href": "brush-and-spin.html#exercises",
    "title": "10  Spin and brush approach",
    "section": "10.1 Exercises",
    "text": "10.1 Exercises\n\nUse the spin and brush approach to identify the three clusters in the mulgar::clusters data set.\nUse the spin and brush approach to identify the six clusters in the mulgar::multicluster data set. (The code below using detourr could be useful.)\n\n\nlibrary(detourr)\n\n# Use a random starting basis because the first two variables make it too easy\nstrt <- tourr::basis_random(10, 2)\ndetour(multicluster, \n       tour_aes(projection = -group)) |>\n       tour_path(grand_tour(2), start=strt, fps = 60) |>\n       show_scatter(alpha = 0.7, axes = FALSE)\n\n\nUse the spin and brush technique to identify the branches of the fake_trees data available in the liminal package (originally from PHATE). The result should look something like this:\n\n\n\n\nFigure 10.3: Example solution after spin and brush on fake_trees data.\n\n\nYou can use the download button to save the data with the colours. Tabulate the branches id variable in the original data with the colour groups created from brushing, to see how closely you have recovered the original classes.\n\n\nCode\nlibrary(detourr)\nlibrary(liminal)\nlibrary(mulgar)\ndata(\"fake_trees\")\n\n# Original data is 100D, so need to reduce dimension using PCA first\nft_pca <- prcomp(fake_trees[,1:100], \n                 scale=TRUE, retx=TRUE)\nggscree(ft_pca)\ndetour(as.data.frame(ft_pca$x[,1:10]), \n       tour_aes(projection = PC1:PC10)) |>\n       tour_path(grand_tour(2), fps = 60, max_bases=50) |>\n       show_scatter(alpha = 0.7, axes = FALSE)\n\nft_sb <- read_csv(\"data/fake_trees_sb.csv\")\ntable(fake_trees$branches, ft_sb$colour)\n\n\n\n\n\n\n\nCook, D., A. Buja, J. Cabrera, and C. Hurley. 1995. “Grand Tour and Projection Pursuit.” Journal of Computational and Graphical Statistics 4 (3): 155–72.\n\n\nWilhelm, A. F. X., E. J. Wegman, and J. Symanzik. 1999. “Visual Clustering and Classification: The Oronsay Particle Size Data Set Revisited.” Computational Statistics: Special Issue on Interactive Graphical Data Analysis 14 (1): 109–46."
  },
  {
    "objectID": "hierarchical-clustering.html#explanation-of-the-algorithm",
    "href": "hierarchical-clustering.html#explanation-of-the-algorithm",
    "title": "11  Hierarchical clustering",
    "section": "11.1 Explanation of the algorithm",
    "text": "11.1 Explanation of the algorithm\nHierarchical cluster algorithms sequentially fuse neighboring points to form ever-larger clusters, starting from a full interpoint distance matrix. Figure 11.1 illustrates the hierarchical clustering approach for a simple simulated data set (a) with two well-separated clusters in 2D. The dendrogram (b) is a representation of the order that points are joined into clusters. The dendrogram strongly indicates two clusters because there are two branches branches representing the last join are much longer than all of the other branches. Although, the dendrogram is usually a good summary of the steps taken by the algorithm, it can be misleading. The dendrogram might strongly suggest a clustering but it might be a terrible solution. To check this we need to show the model with the data, as shown in plot (c). The segments show how the points and clusters are joined. Note that once points are joined into a cluster, the centroid of that cluster is used as the join location with other points or other clusters, and this is represented by a “+”. We can see that the longest edge is the one stretching across the gap between the two clusters, which is the location where the dendrogram would be cut to produce the two-cluster solution. This two-cluster solution is shown in plot (d).\n\n\nCode\nlibrary(ggplot2)\nlibrary(mulgar)\nlibrary(ggdendro)\nlibrary(dplyr)\nlibrary(patchwork)\nlibrary(tourr)\nlibrary(plotly)\nlibrary(htmlwidgets)\n#library(RColorBrewer)\nlibrary(colorspace)\nlibrary(GGally)\n\n\n\n\nCode\ndata(simple_clusters)\n# Data has two well-separated clusters\npd <- ggplot(simple_clusters, aes(x=x1, y=x2)) +\n  geom_point(colour=\"orange\", size=2, alpha=0.8) +\n  ggtitle(\"(a)\") +\n  theme(aspect.ratio=1) \n\n# Compute hierarchical clustering with Ward's linkage\ncl_hw <- hclust(dist(simple_clusters[,1:2]),\n                method=\"ward.D2\")\ncl_ggd <- dendro_data(cl_hw, type = \"triangle\")\nph <- ggplot() +\n  geom_segment(data=cl_ggd$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=cl_ggd$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(b)\") +\n  theme_dendro()\n\n# Compute dendrogram in data\ncl_hfly <- hierfly(simple_clusters, cl_hw, scale=FALSE)\n\npdh <- ggplot() +\n  geom_segment(data=cl_hfly$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=cl_hfly$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(c)\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nsimple_clusters <- simple_clusters %>%\n  mutate(clw = factor(cutree(cl_hw, 2)))\npc <- ggplot(simple_clusters) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(d)\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd + ph + pdh + pc + plot_layout(ncol=2)\n\n\n\n\n\nFigure 11.1: Hierarchical clustering on simulated data: (a) data, (b) dendrogram, (c) dendrogram on the data, and (d) two cluster solution. Nodes of the dendrogram indicated by + when it is drawn on the data.\n\n\n\n\nClustering algorithms are all prone to being confused by different problems occurring in data. For hierarchical clustering, plotting the dendrogram on the data provides another way to assess the solution. For hierarchical clustering additional the complications arise from a range of choices for defining distance once points have been joined into clusters.\nDistance between clusters is described by a “linkage method”, of which there are many. For example, single linkage measures the distance between clusters by the smallest interpoint distance between the members of the two clusters clusters, complete linkage uses the maximum interpoint distance, and average linkage uses the average of the interpoint distances. Wards linkage, which usually produces the best clustering solutions, defines the distance as the reduction in the within-group variance. A good discussion on cluster analysis and linkage can be found in Boehmke and Greenwell (2019), on Wikipedia or any multivariate textbook."
  },
  {
    "objectID": "hierarchical-clustering.html#why-you-should-look-at-the-dendrogram-on-the-data",
    "href": "hierarchical-clustering.html#why-you-should-look-at-the-dendrogram-on-the-data",
    "title": "11  Hierarchical clustering",
    "section": "11.2 Why you should look at the dendrogram on the data",
    "text": "11.2 Why you should look at the dendrogram on the data\n\n\nCode\n# Nuisance observations\nset.seed(20190514)\nx <- (runif(20)-0.5)*4\ny <- x\nd1 <- data.frame(x1 = c(rnorm(50, -3), \n                            rnorm(50, 3), x),\n                 x2 = c(rnorm(50, -3), \n                            rnorm(50, 3), y),\n                 cl = factor(c(rep(\"A\", 50), \n                             rep(\"B\", 70))))\nd1 <- d1 %>% \n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\npd1 <- ggplot(data=d1, aes(x=x1, y=x2)) + \n  geom_point() +\n    ggtitle(\"Nuisance observations\") +\n    theme(aspect.ratio=1) \n\n# Nuisance variables\nset.seed(20190512)\nd2 <- data.frame(x1=c(rnorm(50, -4), \n                            rnorm(50, 4)),\n                 x2=c(rnorm(100)),\n                 cl = factor(c(rep(\"A\", 50), \n                             rep(\"B\", 50))))\nd2 <- d2 %>% \n  mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))\npd2 <- ggplot(data=d2, aes(x=x1, y=x2)) + \n  geom_point() +\n    ggtitle(\"Nuisance variables\") +\n    theme(aspect.ratio=1)\n\npd1 + pd2 + plot_layout(ncol=2)\n\n\n\n\n\nFigure 11.2: Two examples of data structure that causes problems for hierarchical clustering. Nuisance observations can cause problems because the close observations between the two clusters can cause some chaining in the hierarchical joining of observations. Nuisance variables can cause problems because observations across the gap can seem closer than observations at the end of each cluster.\n\n\n\n\n\n\nCode\n# Compute single linkage\nd1_hs <- hclust(dist(d1[,1:2]),\n                method=\"single\")\nd1_ggds <- dendro_data(d1_hs, type = \"triangle\")\npd1s <- ggplot() +\n  geom_segment(data=d1_ggds$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d1_ggds$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(a) Single linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd1_hflys <- hierfly(d1, d1_hs, scale=FALSE)\n\npd1hs <- ggplot() +\n  geom_segment(data=d1_hflys$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d1_hflys$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(b) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd1 <- d1 %>%\n  mutate(cls = factor(cutree(d1_hs, 2)))\npc_d1s <- ggplot(d1) +\n  geom_point(aes(x=x1, y=x2, colour=cls), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(c) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Compute Wards linkage\nd1_hw <- hclust(dist(d1[,1:2]),\n                method=\"ward.D2\")\nd1_ggdw <- dendro_data(d1_hw, type = \"triangle\")\npd1w <- ggplot() +\n  geom_segment(data=d1_ggdw$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d1_ggdw$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(d) Ward's linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd1_hflyw <- hierfly(d1, d1_hw, scale=FALSE)\n\npd1hw <- ggplot() +\n  geom_segment(data=d1_hflyw$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d1_hflyw$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(e) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd1 <- d1 %>%\n  mutate(clw = factor(cutree(d1_hw, 2)))\npc_d1w <- ggplot(d1) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(f) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd1s + pd1hs + pc_d1s + \n  pd1w + pd1hw + pc_d1w +\n  plot_layout(ncol=3)\n\n\n\n\n\nFigure 11.3: Single linkage clustering on nuisance cases in comparison to Ward’s linkage.\n\n\n\n\n\n\nCode\n# Compute complete linkage\nd2_hc <- hclust(dist(d2[,1:2]),\n                method=\"complete\")\nd2_ggdc <- dendro_data(d2_hc, type = \"triangle\")\npd2c <- ggplot() +\n  geom_segment(data=d2_ggdc$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d2_ggdc$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(a) Complete linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd2_hflyc <- hierfly(d2, d2_hc, scale=FALSE)\n\npd2hc <- ggplot() +\n  geom_segment(data=d2_hflyc$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d2_hflyc$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(b) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd2 <- d2 %>%\n  mutate(clc = factor(cutree(d2_hc, 2)))\npc_d2c <- ggplot(d2) +\n  geom_point(aes(x=x1, y=x2, colour=clc), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(c) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Compute Wards linkage\nd2_hw <- hclust(dist(d2[,1:2]),\n                method=\"ward.D2\")\nd2_ggdw <- dendro_data(d2_hw, type = \"triangle\")\npd2w <- ggplot() +\n  geom_segment(data=d2_ggdw$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=d2_ggdw$labels, aes(x=x, y=y),\n             colour=\"orange\", alpha=0.8) +\n  ggtitle(\"(d) Ward's linkage dendrogram\") +\n  theme_dendro()\n\n# Compute dendrogram in data\nd2_hflyw <- hierfly(d2, d2_hw, scale=FALSE)\n\npd2hw <- ggplot() +\n  geom_segment(data=d2_hflyw$segments, \n                aes(x=x, xend=xend,\n                    y=y, yend=yend)) +\n  geom_point(data=d2_hflyw$data, \n             aes(x=x1, y=x2,\n                 shape=factor(node),\n                 colour=factor(node),\n                 size=1-node), alpha=0.8) +\n  scale_shape_manual(values = c(16, 3)) +\n  scale_colour_manual(values = c(\"orange\", \"black\")) +\n  scale_size(limits=c(0,17)) +\n  ggtitle(\"(e) Dendrogram in data\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\n# Show result\nd2 <- d2 %>%\n  mutate(clw = factor(cutree(d2_hw, 2)))\npc_d2w <- ggplot(d2) +\n  geom_point(aes(x=x1, y=x2, colour=clw), \n             size=2, alpha=0.8) +\n  scale_colour_discrete_qualitative(palette = \"Dark 3\") +\n  ggtitle(\"(f) Two-cluster solution\") +\n  theme(aspect.ratio=1, legend.position=\"none\")\n\npd2c + pd2hc + pc_d2c + \n  pd2w + pd2hw + pc_d2w +\n  plot_layout(ncol=3)\n\n\n\n\n\nFigure 11.4: Complete linkage clustering on nuisance variables in comparison to Ward’s linkage. The complete linkage dendrogram looks quite reasonably suggesting a two-cluster solution, but when it is plotted amongst the data that it is clearly not a good two-cluster solution."
  },
  {
    "objectID": "hierarchical-clustering.html#dendrograms-in-high-dimensions",
    "href": "hierarchical-clustering.html#dendrograms-in-high-dimensions",
    "title": "11  Hierarchical clustering",
    "section": "11.3 Dendrograms in high-dimensions",
    "text": "11.3 Dendrograms in high-dimensions\nCheck the data: pretend we don’t know the clusters. Think you can see three elliptical clusters. One is further from the others.\n\n\nCode\nload(\"data/penguins_sub.rda\")\n\n\n\n\nCode\nggscatmat(penguins_sub[,1:4]) \n\n\nWarning: The dot-dot notation (`..scaled..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(scaled)` instead.\nℹ The deprecated feature was likely used in the GGally package.\n  Please report the issue at <https://github.com/ggobi/ggally/issues>.\n\n\n\n\n\nScatterplot matrix of the penguins data, assuming we don’t know the species\n\n\n\n\n\nset.seed(20230329)\nb <- basis_random(4,2)\npt1 <- save_history(penguins_sub[,1:4], \n                    max_bases = 500, \n                    start = b)\nsave(pt1, file=\"data/penguins_tour_path.rda\")\nanimate_xy(penguins_sub[,1:4], \n           tour_path = planned_tour(pt1), \n           axes=\"off\", rescale=FALSE, \n           half_range = 3.5)\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\nrender_gif(penguins_sub[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9, axes=\"off\"),\n           gif_file=\"gifs/penguins_gt.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\nFigure 11.5: Grand tour of the penguins data\n\n\n\n\nCode\np_dist <- dist(penguins_sub[,1:4])\np_hcw <- hclust(p_dist, method=\"ward.D2\")\np_hcs <- hclust(p_dist, method=\"single\")\n\np_clw <- penguins_sub %>% \n  mutate(cl = factor(cutree(p_hcw, 3)))\np_cls <- penguins_sub %>% \n  mutate(cl = factor(cutree(p_hcs, 3)))\n\np_w_hfly <- hierfly(p_clw, p_hcw, scale=FALSE)\np_s_hfly <- hierfly(p_cls, p_hcs, scale=FALSE)\n\n\n\n\nCode\n# Generate the dendrograms in 2D\np_hcw_dd <- dendro_data(p_hcw)\npw_dd <- ggplot() +\n  geom_segment(data=p_hcw_dd$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=p_hcw_dd$labels, aes(x=x, y=y),\n             alpha=0.8) +\n  theme_dendro()\n\np_hcs_dd <- dendro_data(p_hcs)\nps_dd <- ggplot() +\n  geom_segment(data=p_hcs_dd$segments, \n               aes(x = x, y = y, \n                   xend = xend, yend = yend)) + \n  geom_point(data=p_hcs_dd$labels, aes(x=x, y=y),\n             alpha=0.8) +\n  theme_dendro()\n\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\nglyphs <- c(16, 46)\npchw <- glyphs[p_w_hfly$data$node+1]\npchs <- glyphs[p_s_hfly$data$node+1]\n\nanimate_xy(p_w_hfly$data[,1:4], \n           #col=colw, \n           tour_path = planned_tour(pt1),\n           pch = pchw,\n           edges=p_w_hfly$edges, \n           axes=\"bottomleft\")\n\nanimate_xy(p_s_hfly$data[,1:4], \n           #col=colw, \n           tour_path = planned_tour(pt1),\n           pch = pchs,\n           edges=p_s_hfly$edges, \n           axes=\"bottomleft\")\n\nrender_gif(p_w_hfly$data[,1:4], \n           planned_tour(pt1),\n           display_xy(half_range=0.9,            \n                      pch = pchw,\n                      edges = p_w_hfly$edges,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_hflyw.gif\",\n           frames=500,\n           loop=FALSE)\n\nrender_gif(p_s_hfly$data[,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9,            \n                      pch = pchs,\n                      edges = p_s_hfly$edges,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_hflys.gif\",\n           frames=500,\n           loop=FALSE)\n\n# Show three cluster solutions\nclrs <- qualitative_hcl(3, \"Dark 3\")\nw3_col <- clrs[p_w_hfly$data$cl[p_w_hfly$data$node == 0]]\nrender_gif(p_w_hfly$data[p_w_hfly$data$node == 0, 1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9,   \n                      col=w3_col,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_w3.gif\",\n           frames=500,\n           loop=FALSE)\n\nclrs <- qualitative_hcl(3, \"Dark 3\")\ns3_col <- clrs[p_s_hfly$data$cl[p_w_hfly$data$node == 0]]\nrender_gif(p_s_hfly$data[p_w_hfly$data$node == 0,1:4], \n           planned_tour(pt1), \n           display_xy(half_range=0.9,   \n                      col=s3_col,\n                      axes = \"off\"),\n           gif_file=\"gifs/penguins_s3.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\n\n\n\n\n\n(a) Wards linkage\n\n\n\n\n\n\n\n\n\n(b) single linkage\n\n\n\n\n\n\n\n\n\n\n(c) Wards linkage\n\n\n\n\n\n\n\n(d) Single linkage\n\n\n\n\n\n\n\n\n\n(e) Wards linkage\n\n\n\n\n\n\n\n(f) Single linkage\n\n\n\n\nFigure 11.6: Dendrograms for Wards and single linkage of the penguins data, shown in 2D (top) and in 4D (middle), and the three-cluster solution of each.\n\n\n\n\nCode\nload(\"data/penguins_tour_path.rda\")\n# Create a smaller one, for space concerns\npt1i <- interpolate(pt1[,,1:5], 0.1)\npw_anim <- render_anim(p_w_hfly$data,\n                       vars=1:4,\n                       frames=pt1i, \n                       edges = p_w_hfly$edges,\n             obs_labels=paste0(1:nrow(p_w_hfly$data),\n                               p_w_hfly$data$cl))\n\npw_gp <- ggplot() +\n     #geom_path(data=pw_anim$circle, \n     #          aes(x=c1, y=c2,\n     #              frame=frame), linewidth=0.1) +\n     #geom_segment(data=pw_anim$axes, \n     #             aes(x=x1, y=y1, \n     #                 xend=x2, yend=y2, \n     #                 frame=frame), \n     #             linewidth=0.1) +\n     #geom_text(data=pw_anim$axes, \n     #          aes(x=x2, y=y2, \n     #              frame=frame, \n     #              label=axis_labels), \n     #          size=5) +\n     geom_segment(data=pw_anim$edges, \n                    aes(x=x, xend=xend,\n                        y=y, yend=yend,\n                        frame=frame)) +\n     geom_point(data=pw_anim$frames, \n                aes(x=P1, y=P2, \n                    frame=frame, \n                    shape=factor(node),\n                    label=obs_labels), \n                alpha=0.8, size=1) +\n     xlim(-1,1) + ylim(-1,1) +\n     scale_shape_manual(values=c(16, 46)) +\n     coord_equal() +\n     theme_bw() +\n     theme(legend.position=\"none\", \n           axis.text=element_blank(),\n           axis.title=element_blank(),\n           axis.ticks=element_blank(),\n           panel.grid=element_blank())\n\npwg <- ggplotly(pw_gp, width=450, height=500,\n                tooltip=\"label\") %>%\n       animation_button(label=\"Go\") %>%\n       animation_slider(len=0.8, x=0.5,\n                        xanchor=\"center\") %>%\n       animation_opts(easing=\"linear\", transition = 0)\nhtmlwidgets::saveWidget(pwg,\n          file=\"html/penguins_cl_ward.html\",\n          selfcontained = TRUE)\n\n# Single\nps_anim <- render_anim(p_s_hfly$data, vars=1:4,\n                         frames=pt1i, \n                       edges = p_s_hfly$edges,\n             obs_labels=paste0(1:nrow(p_s_hfly$data),\n                               p_s_hfly$data$cl))\n\nps_gp <- ggplot() +\n     #geom_path(data=pw_anim$circle, \n     #          aes(x=c1, y=c2,\n     #              frame=frame), linewidth=0.1) +\n     #geom_segment(data=pw_anim$axes, \n     #             aes(x=x1, y=y1, \n     #                 xend=x2, yend=y2, \n     #                 frame=frame), \n     #             linewidth=0.1) +\n     #geom_text(data=pw_anim$axes, \n     #          aes(x=x2, y=y2, \n     #              frame=frame, \n     #              label=axis_labels), \n     #          size=5) +\n     geom_segment(data=ps_anim$edges, \n                    aes(x=x, xend=xend,\n                        y=y, yend=yend,\n                        frame=frame)) +\n     geom_point(data=ps_anim$frames, \n                aes(x=P1, y=P2, \n                    frame=frame, \n                    shape=factor(node),\n                    label=obs_labels), \n                alpha=0.8, size=1) +\n     xlim(-1,1) + ylim(-1,1) +\n     scale_shape_manual(values=c(16, 46)) +\n     coord_equal() +\n     theme_bw() +\n     theme(legend.position=\"none\", \n           axis.text=element_blank(),\n           axis.title=element_blank(),\n           axis.ticks=element_blank(),\n           panel.grid=element_blank())\n\npsg <- ggplotly(ps_gp, width=450, height=500,\n                tooltip=\"label\") %>%\n       animation_button(label=\"Go\") %>%\n       animation_slider(len=0.8, x=0.5,\n                        xanchor=\"center\") %>%\n       animation_opts(easing=\"linear\", transition = 0)\nhtmlwidgets::saveWidget(psg,\n          file=\"html/penguins_cl_single.html\",\n          selfcontained = TRUE)\n\n\n\n\n\n\n\n\nFigure 11.7: Animation of dendrogram from Wards (top) and single (bottom) linkage clustering of the penguins data.\n\n\n\n\n\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning with r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377."
  },
  {
    "objectID": "model-based-clustering.html#overview",
    "href": "model-based-clustering.html#overview",
    "title": "13  Model-based clustering",
    "section": "13.1 Overview",
    "text": "13.1 Overview\nModel-based clustering Fraley and Raftery (2002) fits a multivariate normal mixture model to the data. It uses the EM algorithm to fit the parameters for the mean, variance–covariance of each population, and the mixing proportion. The variance-covariance matrix is re-parametrized using an eigen-decomposition\n\\[\n\\Sigma_k = \\lambda_kD_kA_kD_k', ~~~k=1, \\dots, g ~~\\mbox{(number of clusters)}\n\\]\nresulting in several model choices, ranging from simple to complex, as shown in Table 13.1.\n\n\nCode\nlibrary(dplyr)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(mclust)\nlibrary(mulgar)\nlibrary(patchwork)\nlibrary(colorspace)\nlibrary(tourr)\n\n\n\n\n\n\nTable 13.1:  Parameterizations of the covariance matrix. \n \n  \n    Model \n    Sigma \n    Family \n    Volume \n    Shape \n    Orientation \n  \n \n\n  \n    EII \n    $$\\lambda I$$ \n    Spherical \n    Equal \n    Equal \n    NA \n  \n  \n    VII \n    $$\\lambda_k I$$ \n    Spherical \n    Variable \n    Equal \n    NA \n  \n  \n    EEI \n    $$\\lambda A$$ \n    Diagonal \n    Equal \n    Equal \n    Coordinate axes \n  \n  \n    VEI \n    $$\\lambda_kA$$ \n    Diagonal \n    Variable \n    Equal \n    Coordinate axes \n  \n  \n    EVI \n    $$\\lambda A_k$$ \n    Diagonal \n    Equal \n    Variable \n    Coordinate axes \n  \n  \n    VVI \n    $$\\lambda_k A_k$$ \n    Diagonal \n    Variable \n    Variable \n    Coordinate axes \n  \n  \n    EEE \n    $$\\lambda DAD^T$$ \n    Diagonal \n    Equal \n    Equal \n    Equal \n  \n  \n    EVE \n    $$\\lambda DA_kD^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Equal \n  \n  \n    VEE \n    $$\\lambda_k DAD^T$$ \n    Ellipsoidal \n    Variable \n    Equal \n    Equal \n  \n  \n    VVE \n    $$\\lambda_k DA_kD^T$$ \n    Ellipsoidal \n    Variable \n    Equal \n    Equal \n  \n  \n    EEV \n    $$\\lambda D_kAD_k^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Variable \n  \n  \n    VEV \n    $$\\lambda_k D_kAD_k^T$$ \n    Ellipsoidal \n    Variable \n    Variable \n    Variable \n  \n  \n    EVV \n    $$\\lambda D_kA_kD_k^T$$ \n    Ellipsoidal \n    Equal \n    Variable \n    Variable \n  \n  \n    VVV \n    $$\\lambda_k D_kA_kD_k^T$$ \n    Ellipsoidal \n    Variable \n    Variable \n    Variable \n  \n\n\n\n\n\n\nNote the distribution descriptions “spherical” and “ellipsoidal”. These are descriptions of the shape of the variance-covariance for a multivariate normal distribution. A standard multivariate normal distribution has a variance-covariance matrix with zeros in the off-diagonal elements, which corresponds to spherically shaped data. When the variances (diagonals) are different or the variables are correlated, then the shape of data from a multivariate normal is ellipsoidal.\n\nThe models are typically scored using the Bayes Information Criterion (BIC), which is based on the log likelihood, number of variables, and number of mixture components. They should also be assessed using graphical methods, as we demonstrate using the data."
  },
  {
    "objectID": "model-based-clustering.html#two-variables",
    "href": "model-based-clustering.html#two-variables",
    "title": "13  Model-based clustering",
    "section": "13.2 Two variables",
    "text": "13.2 Two variables\nWe start with two of the four real-valued variables (bl, fl) and the three species. The goal is to determine whether model-based methods can discover clusters that closely correspond to the three species. Based on the scatterplot in Figure 13.1 we would expect it to do well, and suggest an elliptical variance-covariance of roughly equal sizes as the model.\n\n\nCode\nload(\"data/penguins_sub.rda\")\nggplot(penguins_sub, aes(x=bl, \n                         y=fl)) + #, \n                         #colour=species)) +\n  geom_point() +\n  geom_density2d() +\n  #scale_color_discrete_qualitative(\"Dark 3\") +\n  theme(aspect.ratio = 1)\n\n\n\n\n\nFigure 13.1: Scatterplot of flipper length by bill length of the penguins data.\n\n\n\n\n\n\nCode\npenguins_BIC <- mclustBIC(penguins_sub[,c(1,3)])\nggmc <- ggmcbic(penguins_BIC, cl=2:9, top=4) + ggtitle(\"(a)\")\npenguins_mc <- Mclust(penguins_sub[,c(1,3)], \n                      G=3, \n                      modelNames = \"EVE\")\npenguins_mce <- mc_ellipse(penguins_mc)\npenguins_cl <- penguins_sub[,c(1,3)]\npenguins_cl$cl <- factor(penguins_mc$classification)\nggell <- ggplot() +\n   geom_point(data=penguins_cl, aes(x=bl, y=fl,\n                                    colour=cl),\n              alpha=0.3) +\n   geom_point(data=penguins_mce$ell, aes(x=bl, y=fl,\n                                         colour=cl),\n              shape=16) +\n   geom_point(data=penguins_mce$mn, aes(x=bl, y=fl,\n                                        colour=cl),\n              shape=3, size=2) +\n  scale_color_discrete_qualitative(palette = \"Dark 3\") +\n   theme(aspect.ratio=1, legend.position=\"none\") +\n  ggtitle(\"(b)\")\nggmc + ggell + plot_layout(ncol=2)\n\n\n\n\n\nFigure 13.2: Summary plots from model-based clustering: (a) BIC values for clusters 2-9 of top four models, (b) variance-covariance ellipses and cluster means (+) corresponding to the best model. The best model is three-cluster EVE, which has differently shaped variance-covariances albeit the same volume and orientation.\n\n\n\n\nFigure 13.2 summarises the results. All models agree that three clusters is the best. The different variance-covariance models for three clusters have similar BIC values with EVE (different shape, same volume and orientation) being slightly higher. These plots are made from the mclust package output using the ggmcbic and mc_ellipse functions fro the mulgar package."
  },
  {
    "objectID": "model-based-clustering.html#high-dimensions",
    "href": "model-based-clustering.html#high-dimensions",
    "title": "13  Model-based clustering",
    "section": "13.3 High-dimensions",
    "text": "13.3 High-dimensions\nNow we will examine how model-based clustering will group the penguins data using all four variables.\n\n\nCode\npenguins_BIC <- mclustBIC(penguins_sub[,1:4])\nggmc <- ggmcbic(penguins_BIC, cl=2:9, top=7) \nggmc\n\n\n\n\n\nFigure 13.3: BIC values for the top models for 2-9 clusters on the penguins data. The interpretation is mixed: if one were to choose three clusters any of the variance-covariance models would be equally as good, but the very best model is the four-cluster VEE.\n\n\n\n\n\n\nCode\npenguins_mc <- Mclust(penguins_sub[,1:4], \n                      G=4, \n                      modelNames = \"VEE\")\npenguins_mce <- mc_ellipse(penguins_mc)\npenguins_cl <- penguins_sub\npenguins_cl$cl <- factor(penguins_mc$classification)\n\npenguins_mc_data <- penguins_cl %>%\n  select(bl:bm, cl) %>%\n  mutate(type = \"data\") %>%\n  bind_rows(bind_cols(penguins_mce$ell,\n                      type=rep(\"ellipse\",\n                               nrow(penguins_mce$ell)))) %>%\n  mutate(type = factor(type))\n\nanimate_xy(penguins_mc_data[,1:4],\n           col=penguins_mc_data$cl,\n           pch=c(4, 20 )[as.numeric(penguins_mc_data$type)], \n           axes=\"off\")\n\n# \nload(\"data/penguins_tour_path.rda\")\nrender_gif(penguins_mc_data[,1:4], \n           planned_tour(pt1), \n           display_xy(col=penguins_mc_data$cl,\n               pch=c(4, 20)[\n                 as.numeric(penguins_mc_data$type)], \n                      axes=\"off\",\n               half_range = 0.7),\n           gif_file=\"gifs/penguins_best_mc.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\nCode\npenguins_mc <- Mclust(penguins_sub[,1:4], \n                      G=3, \n                      modelNames = \"EEE\")\npenguins_mce <- mc_ellipse(penguins_mc)\npenguins_cl <- penguins_sub\npenguins_cl$cl <- factor(penguins_mc$classification)\n\npenguins_mc_data <- penguins_cl %>%\n  select(bl:bm, cl) %>%\n  mutate(type = \"data\") %>%\n  bind_rows(bind_cols(penguins_mce$ell,\n                      type=rep(\"ellipse\",\n                               nrow(penguins_mce$ell)))) %>%\n  mutate(type = factor(type))\n\nanimate_xy(penguins_mc_data[,1:4],\n           col=penguins_mc_data$cl,\n           pch=c(4, 20)[as.numeric(penguins_mc_data$type)], \n           axes=\"off\")\n\n# Save the animated gif\nload(\"data/penguins_tour_path.rda\")\nrender_gif(penguins_mc_data[,1:4], \n           planned_tour(pt1), \n           display_xy(col=penguins_mc_data$cl,\n               pch=c(4, 20)[\n                 as.numeric(penguins_mc_data$type)], \n                      axes=\"off\",\n               half_range = 0.7),\n           gif_file=\"gifs/penguins_simpler_mc.gif\",\n           frames=500,\n           loop=FALSE)\n\n\n\n\n\n\n\n\n\n(a) Best model: four-cluster VEE\n\n\n\n\n\n\n\n(b) Three-cluster EEE\n\n\n\n\nFigure 13.4: Examining the model-based clustering results for the penguins data: (a) best model according to BIC value, (b) simpler three-cluster model. Dots are ellipse points, and “x” are data points. It is important to note that the three cluster solution fits the data better, even though it has a lower BIC.\n\n\n\n\n\n\n\n\n\n\n\nFraley, C., and A. E. Raftery. 2002. “Model-Based Clustering, Discriminant Analysis, Density Estimation.” Journal of the American Statistical Association 97: 611–31."
  },
  {
    "objectID": "som.html",
    "href": "som.html",
    "title": "14  Self-organizing maps",
    "section": "",
    "text": "A self-organizing map (SOM) is constructed using a constrained \\(k\\)-means algorithm. A 1D or 2D net is stretched through the data. The knots, in the net, form the cluster means, and the points closest to the knot are considered to belong to that cluster. The similarity of nodes (and their corresponding clusters) is defined as proportional to their distance from one another on the net.\nWe will demonstrate SOM using the music data. The data has 62 cases, each one corresponding to a piece of music. For each piece there are seven variables: the artist, the type of music, and five characteristics, based on amplitude and frequency, that were computed using the first 40 seconds of the piece on CD. The music used included popular rock songs by Abba, the Beatles, and the Eels; classical compositions by Vivaldi, Mozart and Beethoven; and several new wave pieces by Enya. Figure~\\(\\ref{music-som}\\) displays a typical view of the results of clustering using SOM on the music data. Each data point corresponds to a piece of music and is labeled by the band or the composer. The map was generated by this R code:\n> library(som)\n> d.music <- read.csv(\"music-sub.csv\", row.names=1)\n> d.music.std <- cbind(subset(d.music.std,\n   select=c(artist,type)),\n   apply(subset(d.music.std,select=lvar:lfreq), \n   2, f.std.data))\n> music.som <- som(subset(d.music.std,select=lvar:lfreq), \n   6, 6, neigh=\"bubble\", rlen=1000)\nThe left plot in ?fig-music-som is called the 2D map view. Here we have used a \\(6\\times 6\\) net pulled through the 5D data. The net that was wrapped through the high-dimensional space is straightened and laid out flat, and the points, like fish in a fishing net, are laid out where they have been trapped. In the plot shown here, the points have been jittered slightly, away from the knots of the net, so that the labels do not overlap too much. If the fit is good, the points that are close together in this 2D map view are close together in the high-dimensional data space and close to the net as it was placed in the high-dimensional space.\nMuch of the structure in the map is no surprise: The rock (purple) and classical tracks (green) are on opposing corners, with rock in the upper right and classical in the lower left. The Abba tracks are all grouped at the top and left of the map. The Beatles and Eels tracks are mixed. There are some unexpected associations: For example, one Beatles song, which turns out to be “Hey Jude”, is mixed among the classical compositions!\n\nConstruction of a self-organizing map is a dimension reduction method, akin to multidimensional scaling Borg and Groenen (2005) or principal component analysis Johnson and Wichern (2002). Using principal component analysis to find a low-dimensional approximation of the similarity between music pieces, yields the second plot in ?fig-music-som. There are many differences between the two representations. The SOM has a more even spread of music pieces across the grid, in contrast to the stronger clumping of points in the PCA view. Indeed, the PCA view shows several outliers, notably one of the Vivaldi compositions, which could lead us to learn things about the data that we might miss by relying exclusively on the SOM. \nThese two methods, SOM and PCA, have provided two contradictory clustering models. How can we determine which is the more accurate description of the data structure? An important part of model assessment is plotting the model in relation to the data. Although plotting the low-dimensional map is the common way to graphically assess the SOM results, it is woefully limited. If a model has flaws, they may not show up in this view and will only appear in plots of the model in the data space. We will use the grand tour to create these plots, and this will help us assess the two models.\nWe will use a grand tour to view the net wrapped in among the data, hoping to learn how the net converged to this solution, and how it wrapped through the data space. Actually, it is rather tricky to fit a SOM: Like many algorithms, it has a number of parameters and initialization conditions that affect the outcome.\nTo set up the data, we will need to add variables containing the map coordinates to the data:\n> d.music.som <- f.ggobi.som(subset(d.music.std,\n   select=lvar:lfreq), music.som)\nBecause this data has several useful categorical labels for each row, we will want to keep this information in the data when it is loaded into the tour:\n> d.music.som <- data.frame(\n  Songs=factor(c(as.character(row.names(d.music)),\n    rep(\"0\",36))),\n  artist=factor(c(as.character(d.music[,1]),rep(\"0\",36))),\n  type=factor(c(as.character(d.music[,2]),rep(\"0\",36))),\n  lvar=d.music.som[,1], \n  lave=d.music.som[,2], \n  lmax=d.music.som[,3],\n  lfener=d.music.som[,4], lfreq=d.music.som[,5],\n  Map.1=d.music.som[,6], Map.2=d.music.som[,7])\n> gd <- ggobi(d.music.som)[1]\nAdd the edges that form the SOM net:\n> d.music.som.net <- f.ggobi.som.net(music.som)\n> edges(gd) <- d.music.som.net + 62\nAnd finally color the points according to the type of music:\n> gcolor <- rep(8,98)\n> gcolor[d.music.som$Type==\"Rock\"] <- 6\n> gcolor[d.music.som$Type==\"Classical\"] <- 4\n> gcolor[d.music.som$Type==\"New wave\"] <- 1\n> glyph_color(g) <- gcolor\nThe results can be seen in ?fig-clust-SOMa and ?fig-clust-SOMb. The plots show two different states of the fitting process and of the SOM net cast through the data. In both fits, a \\(6\\times 6\\) grid is used and the net is initialized in the direction of the first two principal components. In both fits the variables were standardized to have mean equal to zero and standard deviation equal to 1. The first SOM fit, shown in ?fig-clust-SOMa, was obtained using the default settings; it gave terrible results. At the left is the map view, in which the fit looks deceptively reasonable. The points are spread evenly through the grid, with rock tracks (purple) at the upper right, classical tracks (green) at the lower left, and new wave tracks (the three black rectangles) in between. The tour view in the same figure, however, shows the fit to be inadequate. The net is a flat rectangle in the 5D space and has not sufficiently wrapped through the data. This is the result of stopping the algorithm too soon, thus failing to let it converge fully.\n\n?fig-clust-SOMb shows our favorite fit to the data. The data was standardized, we used a 6$$6 net, and we ran the SOM algorithm for 1,000 iterations. The map is at the top left, and it matches the map already shown in ?fig-music-som, except for the small jittering of points in the earlier figure. The other three plots show different projections from the grand tour. The upper right plot shows how the net curves with the nonlinear dependency in the data: The net is warped in some directions to fit the variance pattern. At the bottom right we see that one side of the net collects a long separated cluster of the Abba tracks. We can also see that the net has not been stretched out to the full extent of the range of the data. It is tempting to manually manipulate the net to stretch it in different directions and update the fit.\nIt turns out that the PCA view of the data more accurately reflects the structure in the data than the map view. The music pieces really are clumped together in the 5D space, and there are a few outliers. \n\n\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern Multidimensional Scaling. New York: Springer.\n\n\nJohnson, R. A., and D. W. Wichern. 2002. Applied Multivariate Statistical Analysis (5th Ed). Englewood Cliffs, NJ: Prentice-Hall."
  },
  {
    "objectID": "unsupervised-summary.html#chap-clust-recap",
    "href": "unsupervised-summary.html#chap-clust-recap",
    "title": "15  Comparing methods",
    "section": "15.1 Recap",
    "text": "15.1 Recap\nGraphics are invaluable for cluster analysis, whether they are used to find clusters or to interpret and evaluate the results of a cluster analysis arrived at by other means.\nThe spin and brush approach can be used to get an initial look at the data and to find clusters, and occasionally, it is sufficient. When the clustering is the result of an algorithm, a very useful first step is to paint the points by cluster membership and to look at the data to see whether the clustering seems sensible. How many clusters are there, and how big are they? What shape are they, and do they overlap one another? Which variables have contributed most to the clustering? Can the clusters be qualitatively described? All the plots we have described can be useful: scatterplots, parallel coordinate plots, and area plots, as well as static plots like dendrograms.\nWhen the clusters have been generated by a model, we should also use graphics to help us assess the model. If the model makes distributional assumptions, we can generate ellipses and compare them with the clusters to see whether the shapes are consistent. For self-organizing maps the tour can assist in uncovering problems with the fit, such as when the map wraps in on itself through the data making it appear that some cases are far apart when they are truly close together. A confusion table can come alive with linked brushing, so that mismatches and agreements between methods can be explored. \n\n\n\n\nDasu, T., D. F. Swayne, and D. Poole. 2005. “Grouping Multivariate Time Series: A Case Study.” In Proceedings of the IEEE Workshop on Temporal Data Mining: Algorithms, Theory and Applications, in Conjunction with the Conference on Data Mining, Houston, November 27, 2005, 25–32. IEEE Computer Society."
  },
  {
    "objectID": "unsupervised-ex.html",
    "href": "unsupervised-ex.html",
    "title": "Exercises",
    "section": "",
    "text": "Using the spin and brush method, uncover three clusters in the data and confirm that these correspond to the three species. (Hint: Transform the data to principal components and enter these variables into the projection pursuit guided tour running the holes index. Also the species should not be identified by color or symbol, until the clusters have been uncovered.)\nRun hierarchical clustering with average linkage on the data (excluding ).\n\nCut the tree at three clusters and append a cluster id to the dataset. How well do the clusters correspond to the species? (Plot vs , and use jittering if necessary.) Using brushing in a plot of linked to a tour plot of the six variables, examine the beetles that are misclassified.\nNow cut the tree at four clusters, and repeat the last part.\nWhich is the better solution, three or four clusters? Why?\n\nFor the ,\n\nConsider the oils from the four areas of Southern Italy. What would you expect to be the result of model-based clustering on the eight fatty acid variables?\nRun model-based clustering on the Southern oils, with the goal of extracting clusters corresponding to the four areas. What is the best model? Create ellipsoids corresponding to the model and examine these in a tour. Do they match your expectations?\nCreate ellipsoids corresponding to alternative models and use these to decide on a best solution.\n\nThis question uses the data.\n\nExplore the patterns in expression level for the functional classes. Can you characterize the expression patterns for each class?\nHow well do the cluster analysis results match the functional classes? Where do they differ?\nCould you use the cluster analysis results to refine the classification of genes into functional classes? How would you do this?\n\nIn the data, make further comparisons of the five-cluster solutions of \\(k\\)-means and Ward’s hierarchical clustering.\n\nOn what tracks do the methods disagree?\nWhich track does \\(k\\)-means consider to be a singleton cluster, while Ward’s hierarchical clustering groups it with 12 other tracks?\nIdentify and characterize the tracks in the four clusters where both methods agree.\n\nIn the data, fit a \\(5\\times 5\\) grid SOM, and observe the results for 100, 200, 500, and 1,000 updates. How does the net change with the increasing number of updates?\nThere is a mystery dataset in the collection, called . How many clusters are in this dataset?"
  },
  {
    "objectID": "temporal.html",
    "href": "temporal.html",
    "title": "Time series",
    "section": "",
    "text": "computing features, and exploring feature space linked to individual time series plots\nprojection pursuit of multiple time series, 1D indexes against time"
  },
  {
    "objectID": "multivariate-time-series.html",
    "href": "multivariate-time-series.html",
    "title": "16  Multiple time series",
    "section": "",
    "text": "Potential topics:\n\ncomputing features, and exploring feature space linked to individual time series plots\nprojection pursuit of multiple time series, 1D indexes against time\nlongitudinal data"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Australian Bureau of Agricultural and Resource Economics and Sciences.\n2018. “Forests of Australia.”\nhttps://www.agriculture.gov.au/abares/forestsaustralia/forest-data-maps-and-tools/spatial-data/forest-cover.\n\n\nBishop, C. M. 2006. Pattern Recognition and\nMachine Learning. New York: Springer.\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning\nwith r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377.\n\n\nBorg, I., and P. J. F. Groenen. 2005. Modern\nMultidimensional Scaling. New York:\nSpringer.\n\n\nBreiman, L. 2001. “Random Forests.”\nMachine Learning 45 (1): 5–32.\n\n\nBreiman, L., J. Friedman, C. Olshen, and C. Stone. 1984.\nClassification and Regression Trees.\nMonterey, CA: Wadsworth; Brooks/Cole.\n\n\nCook, D., A. Buja, J. Cabrera, and C. Hurley. 1995. “Grand\nTour and Projection\nPursuit.” Journal of Computational and Graphical\nStatistics 4 (3): 155–72.\n\n\nDasu, T., D. F. Swayne, and D. Poole. 2005. “Grouping\nMultivariate Time Series: A\nCase Study.” In Proceedings of the\nIEEE Workshop on Temporal Data\nMining: Algorithms, Theory and\nApplications, in Conjunction with the Conference on Data\nMining, Houston, November 27, 2005, 25–32. IEEE Computer Society.\n\n\nDepartment of Environment, Land, Water & Planning. 2019.\n“Fire Origins - Current and\nHistorical.” https://discover.data.vic.gov.au/dataset/fire-origins-current-and-historical.\n\n\n———. 2020a. “CFA - Fire Station.” https://discover.data.vic.gov.au/dataset/cfa-fire-station-vmfeat-geomark_point.\n\n\n———. 2020b. “Recreation Sites.” https://discover.data.vic.gov.au/dataset/recreation-sites.\n\n\nFisher, R. A. 1936. “The Use of Multiple\nMeasurements in Taxonomic\nProblems.” Annals of Eugenics 7: 179–88.\n\n\nFraley, C., and A. E. Raftery. 2002. “Model-Based\nClustering, Discriminant\nAnalysis, Density\nEstimation.” Journal of the American Statistical\nAssociation 97: 611–31.\n\n\nGiordani, Paolo, Maria Brigida Ferraro, and Francesca Martella. 2020.\nAn Introduction to Clustering with r. Springer Singapore. https://doi.org/10.1007/978-981-13-0553-5.\n\n\nHastie, T., R. Tibshirani, and J. Friedman. 2001. The\nElements of Statistical\nLearning. New York: Springer.\n\n\nHennig, Christian, Marina Meila, Fionn Murtagh, and Roberto Rocci, eds.\n2015. Handbook of Cluster Analysis. Chapman;\nHall/CRC. https://doi.org/10.1201/b19706.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://doi.org/10.5281/zenodo.3960218.\n\n\nIowa State University. 2020. “ASOS-AWOS-METAR Data\nDownload.” https://mesonet.agron.iastate.edu/request/download.phtml?network=AU__ASOS.\n\n\nJohnson, R. A., and D. W. Wichern. 2002. Applied Multivariate\nStatistical Analysis (5th Ed). Englewood Cliffs, NJ: Prentice-Hall.\n\n\nKassambara, Alboukadel. 2017. Practical Guide to Cluster Analysis in\nr: Unsupervised Machine Learning. STHDA.\n\n\nLeisch, Friedrich, and Bettina Gruen. 2023. “CRAN Task View:\nCluster Analysis & Finite Mixture Models.”\nhttps://cran.r-project.org/web/views/Cluster.html.\n\n\nLiaw, A., M. Wiener, L. Breiman, and A. Cutler. 2006.\n“randomForest: Breiman and\nCutler’s Random Forests for\nClassification and Regression.”\nhttp://www.R-project.org.\n\n\nMcVicar, Tim. 2011. “Near-Surface Wind Speed. V10. CSIRO. Data\nCollection.” https://doi.org/10.25919/5c5106acbcb02.\n\n\nOpenStreetMap contributors. 2020. “Planet\ndump retrieved from https://planet.osm.org .”\nhttps://www.openstreetmap.org.\n\n\nP-Tree System. 2020. “JAXA Himawari Monitor -\nUser’s Guide.” https://www.eorc.jaxa.jp/ptree/userguide.html.\n\n\nRao, C. R. 1948. “The Utilization of\nMultiple Measurements in Problems\nof Biological Classification (with\nDiscussion).” Journal of the Royal Statistical Society,\nSeries B 10: 159–203.\n\n\nRipley, B. 1996. Pattern Recognition and\nNeural Networks. Cambridge: Cambridge\nUniversity Press.\n\n\nSparks, Adam H., Jonathan Carroll, James Goldie, Dean Marchiori, Paul\nMelloy, Mark Padgham, Hugh Parsonage, and Keith Pembleton. 2020.\nbomrang: Australian Government Bureau of\nMeteorology (BOM) Data Client. https://CRAN.R-project.org/package=bomrang.\n\n\nVenables, W. N., and B. Ripley. 2002. Modern Applied\nStatistics with S. New York:\nSpringer-Verlag.\n\n\nWilhelm, A. F. X., E. J. Wegman, and J. Symanzik. 1999. “Visual\nClustering and Classification:\nThe Oronsay Particle\nSize Data Set\nRevisited.” Computational Statistics: Special\nIssue on Interactive Graphical Data Analysis 14 (1): 109–46."
  },
  {
    "objectID": "toolbox.html",
    "href": "toolbox.html",
    "title": "Appendix A — Toolbox",
    "section": "",
    "text": "Description and explanation of primary methods used throughout the book. Mostly focusing on tour methods.\nWe need a really nice friendly introduction to tour methods:\n\n2D to 1D projections as basic explanation\nmaybe 3D to 2D, or even 4D to 2D illustration\n\nShow examples of what can be gained from looking at combinations as opposed to pairs plots, for example. Clusters is a good situation, but also collinearity and outliers\nExplain data needed for input, what happens if discrete data is included, or time series\nWorking with many dimensions, how to adapt\nGrand tour, and show paths on the space, starting with 1D. How more time gives more coverage of the sphere. Then the torus and paths on torus.\nDifferent types of tours, and when to use them.\nAnd finally how to save tours, and make plot of single projection\nNeed to include - half_range - standardizing variables"
  },
  {
    "objectID": "data.html#australian-football-league-women",
    "href": "data.html#australian-football-league-women",
    "title": "Appendix B — Data",
    "section": "B.1 Australian Football League Women",
    "text": "B.1 Australian Football League Women\n\nDescription\nThis is data from the 2021 Women’s Australian Football League. These are average player statistics across the season, with game statistics provided by the fitzRoy package. If you are new to the game of AFL, there is a nice explanation on Wikipedia.\n\n\nVariables\n\n\n\n\n\nRows: 381\nColumns: 35\n$ id              <chr> \"CD_I1001678\", \"CD_I1001679\", \"CD_I1001681\", \"CD_I1001…\n$ given_name      <chr> \"Jordan\", \"Brianna\", \"Jodie\", \"Ebony\", \"Emma\", \"Pepa\",…\n$ surname         <chr> \"Zanchetta\", \"Green\", \"Hicks\", \"Antonio\", \"King\", \"Ran…\n$ number          <int> 2, 3, 5, 12, 60, 21, 22, 23, 35, 14, 3, 8, 16, 12, 19,…\n$ team            <chr> \"Brisbane Lions\", \"West Coast Eagles\", \"GWS Giants\", \"…\n$ position        <chr> \"INT\", \"INT\", \"HFFR\", \"WL\", \"RK\", \"BPL\", \"INT\", \"INT\",…\n$ time_pct        <dbl> 63.00000, 61.25000, 76.50000, 74.90000, 85.10000, 77.4…\n$ goals           <dbl> 0.0000000, 0.0000000, 0.0000000, 0.1000000, 0.6000000,…\n$ behinds         <dbl> 0.0000000, 0.0000000, 0.5000000, 0.4000000, 0.4000000,…\n$ kicks           <dbl> 5.000000, 2.500000, 3.750000, 8.800000, 4.100000, 3.22…\n$ handballs       <dbl> 2.500000, 3.750000, 3.000000, 3.600000, 2.700000, 2.22…\n$ disposals       <dbl> 7.500000, 6.250000, 6.750000, 12.400000, 6.800000, 5.4…\n$ marks           <dbl> 1.5000000, 0.2500000, 1.0000000, 3.7000000, 2.2000000,…\n$ bounces         <dbl> 0.0000000, 0.0000000, 0.0000000, 0.6000000, 0.1000000,…\n$ tackles         <dbl> 3.000000, 2.250000, 2.250000, 3.900000, 2.000000, 1.77…\n$ contested       <dbl> 3.500000, 2.250000, 3.500000, 5.700000, 4.400000, 2.66…\n$ uncontested     <dbl> 3.500000, 4.500000, 3.000000, 7.000000, 2.800000, 1.77…\n$ possessions     <dbl> 7.000000, 6.750000, 6.500000, 12.700000, 7.200000, 4.4…\n$ marks_in50      <dbl> 1.0000000, 0.0000000, 0.2500000, 0.5000000, 0.9000000,…\n$ contested_marks <dbl> 1.0000000, 0.0000000, 0.0000000, 0.4000000, 1.2000000,…\n$ hitouts         <dbl> 0.0000000, 0.0000000, 0.0000000, 0.0000000, 19.4000000…\n$ one_pct         <dbl> 0.0000000, 1.5000000, 0.5000000, 1.2000000, 2.6000000,…\n$ disposal        <dbl> 60.25000, 67.15000, 37.20000, 65.96000, 61.72000, 66.8…\n$ clangers        <dbl> 2.000000, 0.500000, 2.500000, 3.100000, 2.400000, 1.33…\n$ frees_for       <dbl> 1.0000000, 0.5000000, 0.2500000, 2.5000000, 0.5000000,…\n$ frees_against   <dbl> 1.0000000, 0.5000000, 1.2500000, 1.3000000, 1.1000000,…\n$ rebounds_in50   <dbl> 0.0000000, 0.5000000, 0.2500000, 1.1000000, 0.0000000,…\n$ assists         <dbl> 0.00000000, 0.00000000, 0.00000000, 0.20000000, 0.2000…\n$ accuracy        <dbl> 0.00000, 0.00000, 0.00000, 5.00000, 30.00000, 0.00000,…\n$ turnovers       <dbl> 1.500000, 1.000000, 2.500000, 4.000000, 1.700000, 1.22…\n$ intercepts      <dbl> 2.0000000, 2.0000000, 0.5000000, 5.3000000, 1.3000000,…\n$ tackles_in50    <dbl> 0.5000000, 0.0000000, 0.7500000, 0.5000000, 0.5000000,…\n$ shots           <dbl> 0.5000000, 0.0000000, 0.7500000, 1.0000000, 1.2000000,…\n$ metres          <dbl> 72.50000, 58.50000, 76.00000, 225.90000, 89.80000, 76.…\n$ clearances      <dbl> 0.5000000, 0.2500000, 1.2500000, 0.4000000, 0.9000000,…\n\n\n\n\nPurpose\nThe primary analysis is to summarise the variation using principal component analysis, which gives information about relationships between the statistics or skills sets common in players. One also might be tempted to cluster the players, but there are no obvious clusters so it could be frustrating. At best one could partition the players into groups, while recognising there are no absolutely distinct and separated groups.\n\n\nSource\nSee the information provided with the fitzRoy package.\n\n\nPre-processing\nThe code for downloading and pre-processing the data is available at the mulgar website in the data-raw folder. The data provided by the fitzRoy package was pre-processed to reduce the variables to only those that relate to player skills and performance. It is possible that using some transformations on the variables would be useful to make them less skewed."
  },
  {
    "objectID": "data.html#palmer-penguins",
    "href": "data.html#palmer-penguins",
    "title": "Appendix B — Data",
    "section": "B.2 Palmer penguins",
    "text": "B.2 Palmer penguins\n\n\nCode\nlibrary(palmerpenguins)\npenguins <- penguins %>%\n  na.omit() # 11 observations out of 344 removed\n# use only vars of interest, and standardise\n# them for easier interpretation\npenguins_sub <- penguins[,c(1, 3:6)] %>% \n  mutate(across(where(is.numeric),  ~ scale(.)[,1])) %>%\n  rename(bl = bill_length_mm,\n         bd = bill_depth_mm,\n         fl = flipper_length_mm,\n         bm = body_mass_g)\nsave(penguins_sub, file=\"data/penguins_sub.rda\")\n\n\n\nDescription\nThis data measure four physical characteristics of three species of penguins.\n\n\nVariables\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nbl\na number denoting bill length (millimeters)\n\n\nbd\na number denoting bill depth (millimeters)\n\n\nfl\nan integer denoting flipper length (millimeters)\n\n\nbm\nan integer denoting body mass (grams)\n\n\nspecies\na factor denoting penguin species (Adélie, Chinstrap and Gentoo)\n\n\n\n\n\nPurpose\nThe primary goal is to find a combination of the four variables where the three species are distinct. This is also a useful data set to illustrate cluster analysis.\n\n\nSource\nDetails of the penguins data can be found at https://allisonhorst.github.io/palmerpenguins/, and Horst, Hill, and Gorman (2020) is the package source.\n\n\nPre-processing\nThe data is loaded from the palmerpenguins package. The four physical measurement variables and the species are selected, and the penguins with missing values are removed. Variables are standardised, and their names are shortened.\n\nlibrary(palmerpenguins)\npenguins <- penguins %>%\n  na.omit() # 11 observations out of 344 removed\n# use only vars of interest, and standardise\n# them for easier interpretation\npenguins_sub <- penguins[,c(3:6, 1)] %>% \n  mutate(across(where(is.numeric),  ~ scale(.)[,1])) %>%\n  rename(bl = bill_length_mm,\n         bd = bill_depth_mm,\n         fl = flipper_length_mm,\n         bm = body_mass_g) %>%\n  as.data.frame()\nsave(penguins_sub, file=\"data/penguins_sub.rda\")"
  },
  {
    "objectID": "data.html#bushfires",
    "href": "data.html#bushfires",
    "title": "Appendix B — Data",
    "section": "B.3 Bushfires",
    "text": "B.3 Bushfires\n\nDescription\nThis data was collated by Weihao (Patrick) Li as part of his Honours research at Monash University. It contains fire ignitions as detected from satellite hotspots, and processed using the spotoroo package, augmented with measurements on weather, vegetation, proximity to human activity. The cause variable is predicted based on historical fire ignition data collected by County Fire Authority personnel.\n\n\nVariables\n\n\nRows: 1,021\nColumns: 60\n$ id            <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ lon           <dbl> 141.1300, 141.3000, 141.4800, 147.1600, 148.1050, 144.18…\n$ lat           <dbl> -37.13000, -37.65000, -37.35000, -37.85000, -37.57999, -…\n$ time          <date> 2019-10-01, 2019-10-01, 2019-10-02, 2019-10-02, 2019-10…\n$ FOR_CODE      <dbl> 41, 41, 91, 44, 0, 44, 0, 102, 0, 91, 45, 41, 45, 45, 45…\n$ FOR_TYPE      <chr> \"Eucalypt Medium Woodland\", \"Eucalypt Medium Woodland\", …\n$ FOR_CAT       <chr> \"Native forest\", \"Native forest\", \"Commercial plantation…\n$ COVER         <dbl> 1, 1, 4, 2, 6, 2, 6, 5, 6, 4, 2, 1, 2, 2, 2, 2, 6, 6, 6,…\n$ HEIGHT        <dbl> 2, 2, 4, 2, 6, 2, 6, 5, 6, 4, 3, 2, 3, 3, 3, 2, 6, 6, 6,…\n$ FOREST        <dbl> 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,…\n$ rf            <dbl> 0.0, 0.0, 15.4, 4.8, 6.0, 11.6, 11.6, 0.6, 0.2, 0.6, 0.0…\n$ arf7          <dbl> 5.0857143, 2.4000000, 2.4000000, 0.7142857, 0.8571429, 1…\n$ arf14         <dbl> 2.8142857, 1.7428571, 1.8000000, 1.6714286, 1.5714286, 1…\n$ arf28         <dbl> 1.9785714, 1.5357143, 1.5357143, 3.7857143, 1.9000000, 1…\n$ arf60         <dbl> 2.3033333, 1.7966667, 1.7966667, 4.0000000, 2.5333333, 1…\n$ arf90         <dbl> 1.2566667, 1.0150000, 1.0150000, 2.9600000, 2.1783333, 1…\n$ arf180        <dbl> 0.9355556, 0.8444444, 0.8444444, 2.3588889, 1.7866667, 1…\n$ arf360        <dbl> 1.3644444, 1.5255556, 1.5255556, 1.7272222, 1.4716667, 1…\n$ arf720        <dbl> 1.3011111, 1.5213889, 1.5213889, 1.7111111, 1.5394444, 1…\n$ se            <dbl> 3.8, 4.6, 14.2, 23.7, 23.8, 16.8, 18.0, 12.9, 14.7, 12.9…\n$ ase7          <dbl> 18.02857, 18.50000, 21.41429, 23.08571, 23.11429, 22.014…\n$ ase14         <dbl> 17.03571, 17.44286, 18.03571, 19.17143, 18.45714, 18.628…\n$ ase28         <dbl> 19.32857, 18.47500, 19.33929, 18.23571, 16.86071, 19.375…\n$ ase60         <dbl> 20.38644, 19.99153, 20.39492, 19.90847, 19.26780, 20.449…\n$ ase90         <dbl> 22.54118, 21.93193, 22.04370, 20.59328, 20.04538, 21.809…\n$ ase180        <dbl> 20.79106, 19.93966, 19.99385, 19.11006, 18.66760, 19.810…\n$ ase360        <dbl> 15.55153, 14.83259, 14.87883, 14.69276, 14.44318, 14.755…\n$ ase720        <dbl> 15.52350, 14.75049, 14.77427, 14.53463, 14.32656, 14.540…\n$ maxt          <dbl> 21.3, 17.8, 15.4, 20.8, 19.8, 15.8, 19.5, 12.6, 18.8, 12…\n$ amaxt7        <dbl> 22.38571, 20.44286, 22.21429, 24.21429, 23.14286, 21.671…\n$ amaxt14       <dbl> 21.42857, 19.72857, 19.86429, 21.80000, 20.89286, 19.578…\n$ amaxt28       <dbl> 20.71071, 19.10000, 19.18929, 19.75000, 19.05714, 18.885…\n$ amaxt60       <dbl> 24.02667, 22.28000, 22.38667, 22.93167, 22.12000, 21.031…\n$ amaxt90       <dbl> 27.07750, 25.77667, 25.89833, 24.93667, 23.93750, 23.164…\n$ amaxt180      <dbl> 26.92000, 25.92722, 25.98500, 24.84056, 23.95389, 23.343…\n$ amaxt360      <dbl> 21.55389, 20.79778, 20.81333, 20.21972, 19.99389, 19.505…\n$ amaxt720      <dbl> 21.47750, 20.57222, 20.57694, 20.13153, 20.03875, 19.650…\n$ mint          <dbl> 9.6, 9.0, 7.3, 7.7, 8.3, 8.3, 6.1, 5.9, 7.4, 5.9, 6.9, 7…\n$ amint7        <dbl> 9.042857, 7.971429, 9.171429, 10.328571, 11.200000, 10.6…\n$ amint14       <dbl> 9.928571, 9.235714, 9.421429, 10.007143, 10.900000, 10.7…\n$ amint28       <dbl> 8.417857, 7.560714, 7.353571, 8.671429, 9.575000, 10.060…\n$ amint60       <dbl> 11.156667, 9.903333, 9.971667, 10.971667, 11.975000, 12.…\n$ amint90       <dbl> 11.96667, 10.81250, 10.87833, 12.49000, 13.46167, 13.638…\n$ amint180      <dbl> 11.96778, 11.01056, 11.02000, 12.41944, 13.42500, 13.695…\n$ amint360      <dbl> 9.130556, 8.459722, 8.448333, 9.588611, 10.456389, 11.03…\n$ amint720      <dbl> 8.854861, 8.266250, 8.254028, 9.674861, 10.517083, 10.96…\n$ dist_cfa      <dbl> 9442.206, 6322.438, 7957.374, 7790.785, 10692.055, 6054.…\n$ dist_camp     <dbl> 50966.485, 6592.893, 31767.235, 8816.272, 15339.702, 941…\n$ ws            <dbl> 1.263783, 1.263783, 1.456564, 5.424445, 4.219751, 4.1769…\n$ aws_m0        <dbl> 2.644795, 2.644795, 2.644795, 5.008369, 3.947659, 5.2316…\n$ aws_m1        <dbl> 2.559202, 2.559202, 2.559202, 5.229680, 4.027398, 4.9704…\n$ aws_m3        <dbl> 2.446211, 2.446211, 2.446211, 5.386005, 3.708622, 5.3045…\n$ aws_m6        <dbl> 2.144843, 2.144843, 2.144843, 5.132617, 3.389890, 5.0355…\n$ aws_m12       <dbl> 2.545008, 2.545008, 2.548953, 5.045297, 3.698736, 5.2341…\n$ aws_m24       <dbl> 2.580671, 2.580671, 2.584047, 5.081100, 3.745286, 5.2522…\n$ dist_road     <dbl> 498.75145, 102.22032, 1217.22446, 281.69151, 215.56176, …\n$ log_dist_cfa  <dbl> 9.152945, 8.751860, 8.981854, 8.960697, 9.277256, 8.7084…\n$ log_dist_camp <dbl> 10.838924, 8.793748, 10.366191, 9.084354, 9.638200, 9.15…\n$ log_dist_road <dbl> 6.212108, 4.627130, 7.104329, 5.640813, 5.373247, 5.0047…\n$ cause         <chr> \"lightning\", \"lightning\", \"lightning\", \"lightning\", \"lig…\n\n\n\n\nPurpose\nThe primary goal is to predict the cause of the bushfire using the weather and distance from human activity variables provided.\n\n\nSource\nCollated data was part of Weihao Li’s Honours thesis, which is not publicly available. The hotspots data was collected from P-Tree System (2020), climate data was taken from the Australian Bureau of Meteorology using the bomrang package (Sparks et al. 2020), wind data from McVicar (2011) and Iowa State University (2020), vegetation data from Australian Bureau of Agricultural and Resource Economics and Sciences (2018), distance from roads calculated using OpenStreetMap contributors (2020), CFA stations from Department of Environment, Land, Water & Planning (2020a), and campsites from Department of Environment, Land, Water & Planning (2020b). The cause was predicted from training data provided by Department of Environment, Land, Water & Planning (2019).\n\n\nPre-processing\nThe 60 variables are too many to view with a tour, so it should be pre-processed using principal component analysis. The categorical variables of FOR_TYPE and FOR_CAT are removed. It would be possible to keep these if they are converted to dummy (binary variables)."
  },
  {
    "objectID": "data.html#notes-to-self",
    "href": "data.html#notes-to-self",
    "title": "Appendix B — Data",
    "section": "B.4 Notes to self",
    "text": "B.4 Notes to self\nThere were fifteen datasets listed in chapter 7 of the first edition. Several of these were related to networks which we are not including this time. I have tried to give a mix of things on a variety of topics. It would be nice to see if there’s updated versions of the “tips” (there seems to be a lot of noise in the literature here and no open datasets) and Di’s music data (perhaps we could scrape our own spotify accounts to get an equivalent), there are also a few audio challenge datasets like FSD50K . I think it would be useful to have more unstructured data sets like natural text that we have used for 1010.\n\nOther possible sources for data\nThere are now many search engines available for datasets that originate from research contexts that list licensing information and DOIs:\n\nhttps://zenodo.org (mostly ecology/biology)\nhttps://datadryad.org/stash (mostly biology)\nhttps://dataverse.harvard.edu/dataverse/harvard/ (mostly social sciences, but has a mixture of things)\nThere’s also a big list of datasets here: https://docs.google.com/spreadsheets/d/1ejOJTNTL5ApCuGTUciV0REEEAqvhI2Rd2FCoj7afops/edit#gid=0 (all psychology related)\nTidyTuesday\n\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2022/2022-03-29/readme.md Would need to rearrange data to look at count, participation, revenue, expenditure\n\ngapminder\naccounting records\nLyn’s ecology data\nlearningtower, yowie\n\n\n\n\n\nAustralian Bureau of Agricultural and Resource Economics and Sciences. 2018. “Forests of Australia.” https://www.agriculture.gov.au/abares/forestsaustralia/forest-data-maps-and-tools/spatial-data/forest-cover.\n\n\nDepartment of Environment, Land, Water & Planning. 2019. “Fire Origins - Current and Historical.” https://discover.data.vic.gov.au/dataset/fire-origins-current-and-historical.\n\n\n———. 2020a. “CFA - Fire Station.” https://discover.data.vic.gov.au/dataset/cfa-fire-station-vmfeat-geomark_point.\n\n\n———. 2020b. “Recreation Sites.” https://discover.data.vic.gov.au/dataset/recreation-sites.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nIowa State University. 2020. “ASOS-AWOS-METAR Data Download.” https://mesonet.agron.iastate.edu/request/download.phtml?network=AU__ASOS.\n\n\nMcVicar, Tim. 2011. “Near-Surface Wind Speed. V10. CSIRO. Data Collection.” https://doi.org/10.25919/5c5106acbcb02.\n\n\nOpenStreetMap contributors. 2020. “Planet dump retrieved from https://planet.osm.org .” https://www.openstreetmap.org.\n\n\nP-Tree System. 2020. “JAXA Himawari Monitor - User’s Guide.” https://www.eorc.jaxa.jp/ptree/userguide.html.\n\n\nSparks, Adam H., Jonathan Carroll, James Goldie, Dean Marchiori, Paul Melloy, Mark Padgham, Hugh Parsonage, and Keith Pembleton. 2020. bomrang: Australian Government Bureau of Meteorology (BOM) Data Client. https://CRAN.R-project.org/package=bomrang."
  },
  {
    "objectID": "dimension-overview.html",
    "href": "dimension-overview.html",
    "title": "2  Overview",
    "section": "",
    "text": "This chapter will focus on methods for reducing dimension, and how the tour can be used to assist with the common methods such as principal component analysis (PCA), multidimensional scaling (MDS), t-stochastic neighbour embedding (t-SNE), and factor analysis.\nDimension is perceived in a tour using the spread of points. When the points are spread far apart, then the data is filling the space. Conversely when the points “collapse” into a sub-region then the data is only partially filling the space, and some dimension reduction to reduce to this smaller dimensional space may be worthwhile.\nLet’s start with some 2D examples.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\nset.seed(6045)\nx1 <- runif(123)\nx2 <- x1 + rnorm(123, sd=0.1)\nx3 <- rnorm(123, sd=0.2)\ndf <- tibble(x1 = (x1-mean(x1))/sd(x1), \n             x2 = (x2-mean(x2))/sd(x2),\n             x3, \n             x3scaled = (x3-mean(x3))/sd(x3))\ndp1 <- ggplot(df) + \n  geom_point(aes(x=x1, y=x2)) +\n  xlim(-2.5, 2.5) + ylim(-2.5, 2.5) +\n  annotate(\"segment\", x=0, xend=2, y=0, yend=0) +\n  annotate(\"segment\", x=0, xend=0, y=0, yend=2) +\n  annotate(\"text\", x=2.1, y=0, label=\"x1\") +\n  annotate(\"text\", x=0, y=2.1, label=\"x2\") +\n  theme(aspect.ratio=1)\ndp2 <- ggplot(df) + \n  geom_point(aes(x=x1, y=x3)) +\n  xlim(-2.5, 2.5) + ylim(-2.5, 2.5) +\n  annotate(\"segment\", x=0, xend=2, y=0, yend=0) +\n  annotate(\"segment\", x=0, xend=0, y=0, yend=2) +\n  annotate(\"text\", x=2.1, y=0, label=\"x1\") +\n  annotate(\"text\", x=0, y=2.1, label=\"x3\") +\n  theme(aspect.ratio=1)\ndp3 <- ggplot(df) + \n  geom_point(aes(x=x1, y=x3scaled)) +\n  xlim(-2.5, 2.5) + ylim(-2.5, 2.5) +\n  annotate(\"segment\", x=0, xend=2, y=0, yend=0) +\n  annotate(\"segment\", x=0, xend=0, y=0, yend=2) +\n  annotate(\"text\", x=2.1, y=0, label=\"x1\") +\n  annotate(\"text\", x=0, y=2.1, label=\"x3\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n\n(a) Two variables with strong linear association. Both variables contribute to the association, as indicated by their axes extending out from the “collapsed” direction of the points.\n\n\n\n\n\n\n\n\n\n(b) Two variables with no linear association. But x3 has less variation, so points collapse in this direction.\n\n\n\n\n\n\n\n\n\n(c) The situation in plot (b) does not arise in a tour because all variables are (usually) scaled.\n\n\n\n\n\nFigure 2.1: Explanation of how dimension reduction is perceived in 2D, relative to variables. When an axes extends out of a direction where the points are collapsed, it means that this variable is partially responsible for the reduced dimension.\n\n\n\n\nCode\nlibrary(mulgar)\ndata(plane)\ndata(box)\nrender_gif(plane,\n           grand_tour(), \n           display_xy(),\n           gif_file=\"gifs/plane.gif\",\n           frames=500,\n           width=200,\n           height=200)\nrender_gif(box,\n           grand_tour(), \n           display_xy(),\n           gif_file=\"gifs/box.gif\",\n           frames=500,\n           width=200,\n           height=200)\n# Simulate full cube\nlibrary(geozoo)\ncube5d <- data.frame(cube.solid.random(p=5, n=300)$points)\ncolnames(cube5d) <- paste0(\"x\", 1:5)\nrender_gif(cube5d,\n           grand_tour(), \n           display_xy(),\n           gif_file=\"gifs/cube5d.gif\",\n           frames=500,\n           width=200,\n           height=200)\n\n\n\n\n\n\n\n\n\n2D plane in 5D\n\n\n\n\n\n\n\n3D plane in 5D\n\n\n\n\n\n\n\n5D plane in 5D\n\n\n\n\nFigure 2.2: Different dimensional planes - 2D, 3D, 5D - displayed in a grand tour projecting into 2D. Notice that the 5D in 5D always fills out the box (although it does concentrate some in the middle which is typical when projecting from high to low dimensions). Also you can see that the 2D in 5D, concentrates into a line more than the 3D in 5D. This suggests that it is lower dimensional.\n\n\nThe next step is to determine which variables contribute. In the examples just provided, all variables are linearly associated in the 2D and 2D data. You can check this by making a scatterplot matrix.\n\n\nCode\nlibrary(GGally)\nlibrary(mulgar)\ndata(plane)\nggscatmat(plane)\n\n\n\n\n\nFigure 2.3: Scatterplot matrix of plane data. You can see that x1-x3 are strongly linearly associated, and also x4 and x5. When you watch the tour of this data, any time the data collapses into a line you should see only (x1, x2, x3) or (x4, x5). When combinations of x1 and x4 or x5 show, the data should be spread out.\n\n\n\n\nTo make an example where not all variables contribute, we have added two additional variables to the plane data set, which are purely noise.\n\n# Add two pure noise dimensions to the plane\nplane_noise <- plane\nplane_noise$x6 <- rnorm(100)\nplane_noise$x7 <- rnorm(100)\nggduo(plane_noise, columnsX = 1:5, columnsY = 6:7, \n      types = list(continuous = \"points\")) +\n  theme(aspect.ratio=1, axis.text = element_blank())\n\n\n\n\nFigure 2.4: Additional noise variables are not associated with any of the first five variables.\n\n\n\n\nNow we have 2D structure in 7D, but only five of the variables contribute to the 2D structure, that is, five of the variables are linearly related with each other. The other two variables (x6, x7) are not linearly related to any of the others.\nWe can still see the concentration of points along a line in some dimensions, which tells us that the data is not fully 7D. Then if you look closely at the variable axes you will see that the collapsing to a line only occurs when any of x1-x5 contribute strongly in the direction orthogonal to this. This does not happen when x6 or x7 contribute strongly to a projection - the data is always expanded to fill much of the space. That tells us that x6 and x7 don’t substantially contribute to the dimension reduction, that is, they are not linearly related to the other variables.\n\n\nCode\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(htmlwidgets)\n\nset.seed(78)\nb <- basis_random(7, 2)\npn_t <- tourr::save_history(plane_noise, \n                    tour_path = grand_tour(),\n                    start = b,\n                    max_bases = 8)\npn_t <- interpolate(pn_t, 0.1)\npn_anim <- render_anim(plane_noise,\n                         frames=pn_t)\n\npn_gp <- ggplot() +\n     geom_path(data=pn_anim$circle, \n               aes(x=c1, y=c2,\n                   frame=frame), linewidth=0.1) +\n     geom_segment(data=pn_anim$axes, \n                  aes(x=x1, y=y1, \n                      xend=x2, yend=y2, \n                      frame=frame), \n                  linewidth=0.1) +\n     geom_text(data=pn_anim$axes, \n               aes(x=x2, y=y2, \n                   frame=frame, \n                   label=axis_labels), \n               size=5) +\n     geom_point(data=pn_anim$frames, \n                aes(x=P1, y=P2, \n                    frame=frame), \n                alpha=0.8) +\n     xlim(-1,1) + ylim(-1,1) +\n     coord_equal() +\n     theme_bw() +\n     theme(axis.text=element_blank(),\n         axis.title=element_blank(),\n         axis.ticks=element_blank(),\n         panel.grid=element_blank())\npn_tour <- ggplotly(pn_gp,\n                        width=500,\n                        height=550) %>%\n       animation_button(label=\"Go\") %>%\n       animation_slider(len=0.8, x=0.5,\n                        xanchor=\"center\") %>%\n       animation_opts(easing=\"linear\", \n                      transition = 0)\n\nhtmlwidgets::saveWidget(pn_tour,\n          file=\"html/plane_noise.html\",\n          selfcontained = TRUE)\n\n\n\n\n\n\nFigure 2.5: Grand tour of the plane with two additional dimensions of pure noise. The collapsing of the points indicates that this is not fully 7D. This only happens when any of x1-x5 are contributing strongly (frame 49 x4, x5; frame 79 x1; frame 115 x2, x3). If x6 or x7 are contributing strongly the data is spread out fully. This tells us that x6 and x7 are not linearly associated, but other variables are.\n\n\nThe simulated data here is very simple, and what we have learned from the tour could also be learned from principal component analysis. However, if there are small complications, such as outliers or nonlinear relationships, that might not be visible from principal component analysis, the tour can help you to see them.\nHere’s an example with an outlier.\n\n\nCode\n# Add several outliers to the plane_noise data\nplane_noise_outliers <- plane_noise\nplane_noise_outliers[101,] <- c(2, 2, -2, 0, 0, 0, 0)\nplane_noise_outliers[102,] <- c(0, 0, 0,-2, -2, 0, 0)\n\nggscatmat(plane_noise_outliers, columns = 1:5) +\n  theme(aspect.ratio=1, axis.text = element_blank())\n\n\n\n\n\nFigure 2.6: Outliers added to the plane with noise data.\n\n\n\n\n\n\nCode\nrender_gif(plane_noise_outliers,          \n           grand_tour(), \n           display_xy(),\n           gif_file=\"gifs/pn_outliers.gif\",\n           frames=500,\n           width=200,\n           height=200)\n\ndata(plane_nonlin)\nrender_gif(plane_nonlin,          \n           grand_tour(), \n           display_xy(),\n           gif_file=\"gifs/plane_nonlin.gif\",\n           frames=500,\n           width=200,\n           height=200)\n\n\n\n\n\n\n\n\n\nOutliers in addition to 2D plane.\n\n\n\n\n\n\n\nNon-linear relationship between several variables, primarily x3.\n\n\n\n\nFigure 2.7: Examples of different types of dimensionality issues: outliers and nonlinearity. In the left plot, you can see two points far from the others in some projections. Also the two can be seen with different movement patterns, moving faster that other points during the tour. Outliers will affect detection of reduced dimension, but it is easy to ignore with the tour. Non-linear relationships may not be captured by other techniques but are visible with the tour."
  },
  {
    "objectID": "unsupervised-overview.html#sec-clust-bg",
    "href": "unsupervised-overview.html#sec-clust-bg",
    "title": "9  Overview",
    "section": "9.1 Background",
    "text": "9.1 Background\nBefore we can begin finding groups of cases that are similar, we need to decide on a definition of similarity. How is similarity defined? Consider a dataset with three cases \\((a_1, a_2, a_3)\\) and four variables \\((V_1, V_2, V_3, V_4)\\), described in matrix format as\n\n\\[\n\\require{mathtools}\n\\definecolor{grey}{RGB}{192, 192, 192}\n\\]\n\n\\[\\begin{align*}\nX = \\begin{bmatrix}\n& {\\color{grey} V_1} & {\\color{grey} V_2} & {\\color{grey} V_3} & {\\color{grey} V_4} \\\\\\hline\n{\\color{grey} a_1} | & x_{11} & x_{12} & x_{13} & x_{14} \\\\\n{\\color{grey} a_2} | & x_{21} & x_{22} & x_{23} & x_{24} \\\\\n{\\color{grey} a_3} | & x_{31} & x_{32} & x_{33} & x_{34}    \n\\end{bmatrix}\n=  \\begin{bmatrix}\n& {\\color{grey} V_1} & {\\color{grey} V_2} & {\\color{grey} V_3} & {\\color{grey} V_4} \\\\\\hline\n{\\color{grey} a_1} | & 7.3 & 7.6 & 7.7 & 8.0 \\\\\n{\\color{grey} a_2} | & 7.4 & 7.2 & 7.3 & 7.2 \\\\\n{\\color{grey} a_3} | & 4.1 & 4.6 & 4.6 & 4.8\n\\end{bmatrix}\n\n\\end{align*}\\]\nwhich is plotted in Figure 9.2. The Euclidean distance between two cases (rows of the matrix) with \\(p\\) elements is defined as\n\\[\\begin{align*}\nd_{\\rm Euc}(a_i,a_j) &=& ||a_i-a_j|| %\\\\\n% &=& \\sqrt{(x_{i1}-x_{j1})^2+\\dots + (x_{ip}-x_{jp})^2},\n~~~~~~i,j=1,\\dots, n,\n\\end{align*}\\]\nwhere \\(||x_i||=\\sqrt{x_{i1}^2+x_{i2}^2+\\dots +x_{ip}^2}\\). For example, the Euclidean distance between cases 1 and 2 in the above data, is\n\\[\\begin{align*}\nd_{\\rm Euc}(a_1,a_2) &= \\sqrt{(7.3-7.4)^2+(7.6-7.2)^2+ (7.7-7.3)^2+(8.0-7.2)^2} \\\\\n&= 1.0\n\\end{align*}\\]\n\nFor the three cases, the interpoint Euclidean distance matrix is\n\\[\\begin{align*}\nd_{\\rm Euc} =\n\\left[ \\begin{array}{ccc}\n0.0  ~&     &   \\\\\n1.0 ~&  0.0 ~  &  \\\\\n6.3 ~& 5.5 ~&  0.0 ~ \\\\\n\\end{array} \\right]\n\\begin{array}{r}\na_1 \\\\ a_2 \\\\ a_3 \\\\\n\\end{array}\n\\end{align*}\\]\n\n\n\n\n\nCode\nx <- tibble::tibble(V1 = c(7.3, 7.4, 4.1),\n                    V2 = c(7.6, 7.2, 4.6),\n                    V3 = c(7.7, 7.3, 4.6),\n                    V4 = c(8.0, 7.2, 4.8),\n                    point = factor(c(\"a1\", \"a2\", \"a3\")))\nlibrary(GGally)\nlibrary(colorspace)\nlibrary(gridExtra)\npscat <- ggpairs(x, columns=1:4,\n                 upper=list(continuous=\"points\"),\n                 diag=list(continuous=\"blankDiag\"),\n                 axisLabels=\"internal\",\n                 ggplot2::aes(colour=point)) +\n    scale_colour_discrete_qualitative(\n      palette = \"Dark 3\") +\n    theme(aspect.ratio=1)\npscat\n\n\n\n\n\n\n\nCode\nppar <- ggparcoord(x, columns=1:4, \n                   groupColumn = 5, \n                   scale = \"globalminmax\") +\n          scale_colour_discrete_qualitative(\n            palette = \"Dark 3\") +\n  xlab(\"\") + ylab(\"\") + \n  theme(axis.ticks.y = element_blank(),\n        axis.text.y = element_blank(),\n        legend.title = element_blank())\nppar\n\n\n\n\n\n\nFigure 9.2: The scatterplot matrix (left) shows that cases \\(a_1\\) and \\(a_2\\) have similar values. The parallel coordinate plot (right) allows a comparison of other structure, which shows the similarity in the trend of the profiles on cases \\(a_1\\) and \\(a_3\\).\n\n\nCases \\(a_1\\) and \\(a_2\\) are more similar to each other than they are to case \\(a_3\\), because the Euclidean distance between cases \\(a_1\\) and \\(a_2\\) is much smaller than the distance between cases \\(a_1\\) and \\(a_3\\) and between cases \\(a_2\\) and \\(a_3\\).\nThere are many different ways to calculate similarity. Similarity measures based on correlation distance have become common. Correlation distance is typically used where similarity of structure is more important than similarity in magnitude.\n\nAs an example, see the parallel coordinate plot of the sample data at the right of Figure 9.2. Cases \\(a_1\\) and \\(a_3\\) are widely separated, but their shapes are similar (low, medium, medium, high). Case \\(a_2\\), although overlapping with Case \\(a_1\\), has a very different shape (high, medium, medium, low). The correlation between two cases is defined as\n\\[\\begin{align*}\n\\rho(a_i,a_j) = \\frac{(a_i-c_i)'(a_j-c_j)}\n{\\sqrt{(a_i-c_i)'(a_i-c_i)} \\sqrt{(a_j-c_j)'(a_j-c_j)}}\n\\label{corc}\n\\end{align*}\\]\nWhen \\(c_i, c_j\\) are the sample means \\(\\bar{a}_i,\\bar{a}_j\\), then \\(\\rho\\) is the Pearson correlation coefficient. If, indeed, they are set at 0, as is commonly done, \\(\\rho\\) is a generalized correlation that describes the angle between the two data vectors. The correlation is then converted to a distance metric; one equation for doing so is as follows:\n\\[\\begin{align*}\nd_{\\rm Cor}(a_i,a_j) = \\sqrt{2(1-\\rho(a_i,a_j))}\n\\end{align*}\\]\nThe above distance metric will treat cases that are strongly negatively correlated as the most distant.\nThe interpoint distance matrix for the sample data using \\(d_{\\rm Cor}\\) and the Pearson correlation coefficient is\n\\[\\begin{align*}\nd_{\\rm Cor} =\n\\left[ \\begin{array}{rrrrrrrrr}\n0.0  ~&     &  \\\\\n3.6 ~ & 0.0 ~ &  \\\\\n0.1 ~ & 3.8 ~ &  0.0 ~\\\\\n\\end{array} \\right]\n\\begin{array}{r}\na_1 \\\\ a_2 \\\\ a_3 \\\\\n\\end{array}\n\\end{align*}\\]\nBy this metric, cases \\(a_1\\) and \\(a_3\\) are the most similar, because the correlation distance is smaller between these two cases than the other pairs of cases. \nNote that these interpoint distances differ dramatically from those for Euclidean distance. As a consequence, the way the cases would be clustered is also be very different. Choosing the appropriate distance measure is an important part of a cluster analysis.\nAfter a distance metric has been chosen and a cluster analysis has been performed, the analyst must evaluate the results, and this is actually a difficult task. A cluster analysis does not generate \\(p\\)-values or other numerical criteria, and the process tends to produce hypotheses rather than testing them. Even the most determined attempts to produce the “best” results using modeling and validation techniques may result in clusters that, although seemingly significant, are useless for practical purposes. As a result, cluster analysis is best thought of as an exploratory technique, and it can be quite useful despite the lack of formal validation because of its power in data simplification.\nThe context in which the data arises is the key to assessing the results. If the clusters can be characterized in a sensible manner, and they increase our knowledge of the data, then we are on the right track. To use an even more pragmatic criterion, if a company can gain an economic advantage by using a particular clustering method to carve up their customer database, then that is the method they should use.\n\n\n\n\nBoehmke, B., and B. M. Greenwell. 2019. Hands-on Machine Learning with r (1st Ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377.\n\n\nGiordani, Paolo, Maria Brigida Ferraro, and Francesca Martella. 2020. An Introduction to Clustering with r. Springer Singapore. https://doi.org/10.1007/978-981-13-0553-5.\n\n\nHennig, Christian, Marina Meila, Fionn Murtagh, and Roberto Rocci, eds. 2015. Handbook of Cluster Analysis. Chapman; Hall/CRC. https://doi.org/10.1201/b19706.\n\n\nKassambara, Alboukadel. 2017. Practical Guide to Cluster Analysis in r: Unsupervised Machine Learning. STHDA.\n\n\nLeisch, Friedrich, and Bettina Gruen. 2023. “CRAN Task View: Cluster Analysis & Finite Mixture Models.” https://cran.r-project.org/web/views/Cluster.html.\n\n\nVenables, W. N., and B. Ripley. 2002. Modern Applied Statistics with S. New York: Springer-Verlag."
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Appendix C — Glossary",
    "section": "",
    "text": "Term\n      Synonyms\n      Description\n    \n  \n  \n    variable\nfeature, attribute\na characteristic, number or quantity that can be measured\n    observations\ncases, items, experimental units, observational units, records, statistical units, instances, examples\nindividuals on which the observations are made\n    data set\nNA\ncollection of observations made on one or more variables\n    response\ntarget\nvariable that one wishes to predict\n    predictor\nindependent variable, feature\nvariables used to produce a mode to predict the response"
  },
  {
    "objectID": "svm.html",
    "href": "svm.html",
    "title": "6  Support vector machine",
    "section": "",
    "text": "% http://www.support-vector-machines.org/SVM_osh.html % wikipedia\nA support vector machine (SVM) (Vapnik 1999) is a binary classification method. An SVM looks for gaps between clusters in the data, based on the extreme observations in each class. In this sense it mirrors the graphical approach described at the start of this chapter, in which we searched for gaps between groups. We describe this method more fully than we did the other algorithms for two reasons: first, because of its apparent similarity to the graphical approach, and second, because it is difficult to find a simple explanation of the method in the literature.\nThe algorithm takes an \\(n \\times p\\) data matrix, where each column is scaled to \\([-1,1]\\) and each row is labeled as one of two classes (\\(y_i=+1\\) or \\(-1\\)), and finds a hyperplane that separates the two groups, if they are separable. Each row of the data matrix is a vector in \\(p\\)-dimensional space, denoted as\n% Should this be represented as a row instead of a column? dfs\n\\[\nX=\\left[ \\begin{array}{c}\n  x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p \\end{array} \\right]\n\\]\nand the separating hyperplane can be written as\n\\[\nW'X + b = 0\n\\]\nwhere \\(W = [ w_1~~ w_2 ~~ \\dots ~~ w_p]'\\) is the normal vector to the separating hyperplane and \\(b\\) is a constant. The best separating hyperplane is found by maximizing the margin of separation between the two classes as defined by two parallel hyperplanes:\n\\[\nW'X + b = 1, ~~~~~ W'X + b = -1.\n\\]\nThese hyperplanes should maximize the distance from the separating hyperplane and have no points between them, capitalizing on any gap between the two classes. The distance from the origin to the separating hyperplane is \\(|b|/||W||\\), so the distance between the two parallel margin hyperplanes is \\(2/||W||=2/\\sqrt{w_1^2+\\dots +w_p^2}\\). Maximizing this is the same as minimizing \\(||W||/2\\). To ensure that the two classes are separated, and that no points lie between the margin hyperplanes we need:\n\\[\nW'X_i + b \\geq 1, ~~~\\mbox{  or  } ~~~W'X_i + b \\leq -1 ~~~\\forall i=1, ..., n\n\\]\nwhich corresponds to\n\\[\\begin{eqnarray}\ny_i(W'X_i+b)\\geq 1 ~~~\\forall i=1, ..., n\n\\label{svm-crit}\n\\end{eqnarray}\\]\nThus the problem corresponds to\n\nInterestingly, only the points closest to the margin hyperplanes are needed to define the separating hyperplane. We might think of these points as lying on or close to the convex hull of each cluster in the area where the clusters are nearest to each other. These points are called support vectors, and the coefficients of the separating hyperplane are computed from a linear combination of the support vectors \\(W = \\sum_{i=1}^{s} y_i\\alpha_iX_i\\), where \\(s\\) is the number of support vectors. We could also use \\(W = \\sum_{i=1}^n y_i\\alpha_iX_i\\), where \\(\\alpha_i=0\\) if \\(X_i\\) is not a support vector. For a good fit the number of support vectors \\(s\\) should be small relative to \\(n\\). Fitting algorithms can achieve gains in efficiency by using only samples of the cases to find suitable support vector candidates; this approach is used in the SVMLight (Joachims 1999) software.\nIn practice, the assumption that the classes are completely separable is unrealistic. Classification problems rarely present a gap between the classes, such that there are no misclassifications. Cortes and Vapnik (1995) relaxed the separability condition to allow some misclassified training points by adding a tolerance value \\(\\epsilon_i\\) to Equation \\(\\ref{svm-crit}\\), which results in the modified criterion \\(y_i(W'X_i+b)>1-\\epsilon_i, \\epsilon_i\\geq 0\\). Points that meet this criterion but not the stricter one are called slack vectors.\nNonlinear classifiers can be obtained by using nonlinear transformations of \\(X_i\\), \\(\\phi(X_i)\\) (Boser, Guyon, and Vapnik 1992), which is implicitly computed during the optimization using a kernel function \\(K\\). Common choices of kernels are linear \\(K(x_i,x_j)=x_i'x_j\\), polynomial \\(K(x_i,x_j)=(\\gamma x_i'x_j+r)^d\\), radial basis \\(K(x_i,x_j)=\\exp(-\\gamma ||x_i-x_j||^2)\\), or sigmoid functions \\(K(x_i,x_j)=\\mbox{tanh}(\\gamma x_i'x_j+r)\\), where \\(\\gamma>0, r,\\) and \\(d\\) are kernel parameters.\n% She didn’t say to delete the terminating colon here, but by % analogy with these rest, I will. dfs\nThe ensuing minimization problem is formulated as\n\\[\n\\mbox{ minimizing } \\frac{1}{2}||W|| + C\\sum_{i=1}^n \\epsilon_i ~~ \\mbox{ subject to }\ny_i(W'\\phi(X)+b)>1-\\epsilon_i\n\\]\nwhere \\(\\epsilon_i\\geq 0\\), \\(C>0\\) is a penalty parameter guarding against over-fitting the training data and \\(\\epsilon\\) controls the tolerance for misclassification. The normal to the separating hyperplane \\(W\\) can be written as \\(\\sum_{i=1}^{n} y_i\\alpha_i{\\phi(X_i)}\\), where points other than the support and slack vectors will have \\(\\alpha_i=0\\). Thus the optimization problem becomes\n\\[\\begin{eqnarray*}\n\\mbox{ minimizing } \\frac{1}{2} \\sum_{i=1}^n\\sum_{j=1}^n y_iy_j\\alpha_i\\alpha_jK(X_i,X_j)+C\\sum_{i=1}^n \\epsilon_i \\\\ ~~~~~~~~~~~\\mbox{ subject to }\ny_i(W'\\phi(X)+b)>1-\\epsilon_i\n\\end{eqnarray*}\\]\n \nWe use the svm function in the e1071 package (Dimitriadou et al. 2006) of R, which uses libsvm (Chang and Lin 2006), to classify the oils of the four areas in the Southern region. SVM is a binary classifier, but this algorithm overcomes that limitation by comparing classes in pairs, fitting six separate classifiers, and then using a voting scheme to make predictions. To fit the SVM we also need to specify a kernel, or rely on the internal tuning tools of the algorithm to choose this for us. Automatic tuning in the algorithm chooses a radial basis, but we found that a linear kernel performed better, so that is what we used. (This accords with our earlier visual inspection of the data in ?sec-class-plots.) Here is the R code used to fit the model:\n> library(e1071)\n> olive.svm <- best.svm(factor(area) ~ ., data=d.olive.train)\n> olive.svm <- svm(factor(area) ~ ., data=d.olive.sth.train, \n  type=\"C-classification\", kernel=\"linear\")\n> table(d.olive.sth.train[,1], predict(olive.svm, \n  d.olive.sth.train))\n   \n      1   2   3   4\n  1  19   0   0   0\n  2   0  42   0   0\n  3   0   0 155   3\n  4   1   2   3  21\n> table(d.olive.sth.test[,1], predict(olive.svm, \n  d.olive.sth.test))\n   \n     1  2  3  4\n  1  6  0  0  0\n  2  1 12  1  0\n  3  0  0 46  2\n  4  1  1  0  7\n> support.vectors <- olive.svm$index[\n    abs(olive.svm$coefs[,1])<1 &\n    abs(olive.svm$coefs[,2])<1 & abs(olive.svm$coefs[,3])<1]\n> pointtype <- rep(0,323) # training\n> pointtype[247:323] <- 1 # test\n> pointtype[olive.svm$index] <- 2 # slack vectors\n> pointtype[support.vectors] <- 3 # support vectors\n> parea <- c(predict(olive.svm, d.olive.sth.train),\n    predict(olive.svm, d.olive.sth.test))\n> d.olive.svm <- cbind(rbind(d.olive.sth.train, \n    d.olive.sth.test), parea, pointtype)\n> gd <- ggobi(d.olive.svm)[1]\n> glyph_color(gd) <- c(6,3,2,9)[d.olive.svm$area]\nThese are our misclassification tables:\n\n\nThe training error is \\(9/246=0.037\\), and the test error is \\(6/77=0.078\\). (The training error is the same as that of the neural network classifier, but the test error is lower.) Most error is associated with Sicily, which we have seen repeatedly to be an especially difficult class to separate. In the training data there are no other errors, and in the test data there are just two samples from Calabria mistakenly classified. ?fig-olive-svm illustrates our examination of the misclassified cases, one in each row of the figure. (Points corresponding to Sicily were removed from all four plots.) Each of the two cases is brushed (using a filled red circle) in the plot of misclassification table and viewed in a linked 2D tour. Both of these cases are on the edge of their clusters so the confusion of classes is reasonable.\n% Figure 14\n% Figure 15\nThe linear SVM classifier uses 20 support vectors and 29 slack vectors to define the separating planes between the four areas. It is interesting to examine which points are selected as support vectors, and where they are located in the data space. For each pair of classes, we expect to find some projection in which the support vectors line up on either side of the margin of separation, whereas the slack vectors lie closer to the boundary, perhaps mixed in with the points of other classes.\n The plots in ?fig-olive-svm2 represent our use of the 2D tour, augmented by manual manipulation,~to look for these projections. (The Sicilian points are again removed.) The support vectors are represented by open circles and the slack vectors by open rectangles, and we have been able to find a number of projections in which the support vectors are on the opposing outer edge of the point clouds for each cluster.\nThe linear SVM does a very nice job with this difficult classification. The accuracy is almost perfect on three classes, and the misclassifications are quite reasonable mistakes, being points that are on the extreme edges of their clusters. However, this method joins the list of those defeated by the difficult problem of distinguishing the Sicilian oils from the rest.\n\n6.0.1 Examining boundaries\n\nFor some classification problems, it is possible to get a good picture of the boundary between two classes. With LDA and SVM classifiers the boundary is described by the equation of a hyperplane. For others the boundary can be determined by evaluating the classifier on points sampled in the data space, using either a regular grid or some more efficient sampling scheme.\n% Figure 16\n We use the R package classifly (Wickham 2006) to generate points illustrating boundaries, add those points to the original data, and display them in GGobi. ?fig-olive-classifly shows projections of boundaries between pairs of classes in the . In each example, we used the 2D tour with manual control~to focus the view on a projection that revealed the boundary between two groups.\n% Needs to be checked\nThe top two plots show tour projections of the North (purple) and Sardinia (green) oils where the two classes are separated and the boundary appears in gray. The LDA boundary (shown at left) slices too close to the Northern oils. This might be due to the violation of the LDA assumption that the two groups have equal variance; since that is not true here, it places the boundary too close to the group with the larger variance. The SVM boundary (at right) is a bit closer to the Sardinian oils than the LDA boundary is, yet it is still a tad too close to the oils from the North.\nThe bottom row of plots examines the more difficult classification of the areas of the South, focusing on separating the South Apulian oils (in pink), which is the largest sample, from the oils of the other areas (all in orange). Perfect separation between the classes does not occur. Both plots are tour projections showing SVM boundaries, the left plot generated by a linear kernel and the right one by a radial kernel. Recall that the radial kernel was selected automatically by the SVM software we used, whereas we actually chose to use a linear kernel. These pictures illustrate that the linear basis yields a more reasonable boundary between the two groups. The shape of the clusters of the two groups is approximately the same, and there is only a small overlap of the two. The linear boundary fits this structure neatly. The radial kernel wraps around the South Apulian oils.\n\n\n\n\nBoser, Bernhard E., Isabelle M. Guyon, and Vladimir N. Vapnik. 1992. “A Training Algorithm for Optimal Margin Classifiers.” In COLT ’92: Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 144–52. New York: ACM Press. https://doi.org/http://doi.acm.org/10.1145/130385.130401.\n\n\nChang, C.-C., and C.-J. Lin. 2006. “LIBSVM: A Library for Support Vector Machines.” http://www.csie.ntu.edu.tw/\\(\\sim\\)cjlin/libsvm.\n\n\nCortes, C., and V. N. Vapnik. 1995. “Support-Vector Networks.” Machine Learning 20 (3): 273–97.\n\n\nDimitriadou, E., K. Hornik, F. Leisch, D. Meyer, and A. Weingessel. 2006. “e1071: Misc Functions of the Department of Statistics, TU Wien.” http://www.R-project.org.\n\n\nJoachims, T. 1999. “Making Large-Scale SVM Learning Practical.” In Advances in Kernel Methods-Support Vector Learning, edited by B. Schölkopf, C. Burges, and A. Smola. Cambridge, MA: MIT Press.\n\n\nVapnik, V. N. 1999. The Nature of Statistical Learning Theory. New York: Springer.\n\n\nWickham, H. 2006. “classifly: Classify and Explore a Data Set.” http://www.R-project.org."
  }
]