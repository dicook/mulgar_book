# Support vector machines
\index{classification methods!support vector machines (SVM)}

A support vector machine (SVM) [@Va99] looks for gaps between clusters in the data, based on the extreme observations in each class. In this sense it mirrors the graphical approach described @sec-clust-graphics, in which we searched for gaps between groups. It can be viewed as similar to LDA, in that the boundary between classes is a hyper-plane.  The difference between LDA and SVM is the placement of the boundary. LDA uses the means and covariance matrices of the classes to place the boundary, but SVM uses extreme observations.

::: info
The key elements of the SVM model to examine are:

- support vectors
- separating hyper-plane.

:::

## Example: simple 2D

To illustrate the approach, we use two simple simulated data examples. Both have only two variables, and two classes. Explaining SVM is easier when there are just two groups. In the first data set the two classes have different covariances matrices, which will cause trouble for LDA, but SVM should see the gap between the two clusters and place the separating hyper-plane in the middle of the gap. In the second data set the two groups are concentric circles, with the inner one solid. A non-linear SVM should be fitted to this data, which should see circular gap between the two classes. 

Note that the `svm` function in the `e1071` package will automatically scale observations into the range $[0,1]$. To make it easier to examine the fitted model, it is best to scale your data first, and then fit the model.

```{r}
# Toy examples
library(mulgar)
library(ggplot2)
set.seed(1071)
n1 <- 162
vc1 <- matrix(c(1, -0.7, -0.7, 1), ncol=2, byrow=TRUE)
c1 <- rmvn(n=n1, p=2, mn=c(-2, -2), vc=vc1)
vc2 <- matrix(c(1, -0.4, -0.4, 1)*2, ncol=2, byrow=TRUE)
n2 <- 138
c2 <- rmvn(n=n2, p=2, mn=c(2, 2), vc=vc2)
df1 <- data.frame(x1=mulgar:::rescale01(c(c1[,1], c2[,1])), 
                 x2=mulgar:::rescale01(c(c1[,2], c2[,2])), 
                 cl = factor(c(rep("A", n1), 
                               rep("B", n2))))
library(geozoo)
c1 <- sphere.hollow(p=2, n=n1)$points*3 + 
  c(rnorm(n1, sd=0.3), rnorm(n1, sd=0.3))
c2 <- sphere.solid.random(p=2, n=n2)$points
df2 <- data.frame(x1=mulgar:::rescale01(c(c1[,1], c2[,1])), 
                  x2=mulgar:::rescale01(c(c1[,2], c2[,2])), 
                  cl = factor(c(rep("A", n1), 
                               rep("B", n2))))
```

```{r}
#| label: fig-svm-toy
#| fig-cap: "SVM classifier fit overlaid on two simulated data examples: (a) groups with different variance-covariance, fitted using a linear kernel, (b) groups with non-linear separation, fitted using a radial kernel. The band of points shown as '+' mark the SVM boundary, and points marked by 'x' are the support vectors used to define the boundary. "
#| code-fold: false
#| fig-width: 8
#| fig-height: 4
library(classifly)
library(e1071)
df1_svm <- svm(cl~., data=df1, 
                     probability=TRUE, 
                     kernel="linear")
#df1_svm <- ksvm(cl~., data=df1, 
#                     probability=TRUE,
#                     kernel="vanilladot")
df1_svm_e <- explore(df1_svm, df1)

s1 <- ggplot() + 
  geom_point(data=df1, aes(x=x1, y=x2, colour=cl),
             shape=20) +
  scale_colour_viridis_d("", begin=0.3, end=0.8) +
  geom_point(data=df1_svm_e[(!df1_svm_e$.BOUNDARY)&(df1_svm_e$.TYPE=="simulated"),], 
             aes(x=x1, y=x2, colour=cl), shape=3) +
  geom_point(data=df1[df1_svm$index,], 
             aes(x=x1, y=x2, colour=cl), 
             shape=4, size=4) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position = "none") +
  ggtitle("(a)")

df2_svm <- svm(cl~., data=df2,  
                     probability=TRUE, 
                     kernel="radial")
df2_svm_e <- explore(df2_svm, df2)
s2 <- ggplot() + 
  geom_point(data=df2, aes(x=x1, y=x2, colour=cl), shape=20) +
  scale_colour_viridis_d("", begin=0.3, end=0.8) +
  geom_point(data=df2_svm_e[(!df2_svm_e$.BOUNDARY)&(df2_svm_e$.TYPE=="simulated"),], 
             aes(x=x1, y=x2, colour=cl), 
             shape=3) +
  geom_point(data=df2[df2_svm$index,], 
             aes(x=x1, y=x2, colour=cl), 
             shape=4, size=4) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position = "none") +
  ggtitle("(b)")

library(patchwork)
s1+s2
```

@fig-svm-toy shows the two data sets and the important aspects of the fitted SVM model for each. The observations are represented by dots, the separating hyper-plane (just a line for 2D) is represented by '+'. Where the two colours merge is the actual location of the boundary between classes. It can be seen that this is located right down the middle of the gap, for both data sets. Even though the boundary is circular for the second data set, in a transformed high-dimensional space it would be linear.

SVMs use a subset of the observations to define the boundary, and these are called the support vectors. For each of the data sets these are marked with 'x'. For the linear boundary, there are nine support vectors, five in one group and four in the other. There is one interesting observation in the green group, which falls far from its group and on the other side of the boundary with the blue points. It is marked as a support vector, but its contribution to the fitted hyper-plane is limited by a control parameter in the model fitting process. <!--The equation of the fitted plane can be explicitly written when the boundary is linear. -->

Observations `r df1_svm$index` are the support vectors. All but 15, 45 and 180 are actually bounded support vectors, which means their coefficients are bounded to magnitude 1. <!--The coefficients for the separating hyperplane can be computed with:

```{r}
#| code-fold: false
df1[df1_svm$index,]
df1_svm$coefs
df1_svm$rho
```


```{r}
#| code-fold: false
coefs <- apply(((df1[df1_svm$index,-3]-df1_svm$x.scale$`scaled:center`)/df1_svm$x.scale$`scaled:scale`)*df1_svm$coefs, 2, sum)
ggplot() + 
  geom_point(data=df1, aes(x=x1, y=x2, colour=cl),
             shape=20) +
  scale_colour_viridis_d("", begin=0.3, end=0.8) +
  geom_point(data=df1_svm_e[(!df1_svm_e$.BOUNDARY)&(df1_svm_e$.TYPE=="simulated"),], 
             aes(x=x1, y=x2, colour=cl), shape=3) +
  geom_abline(slope=-coefs[1]/coefs[2],
              intercept=df1_svm$rho+0.5) +
  theme_minimal() +
  theme(aspect.ratio=1, legend.position = "none") 
```
-->

## Example: penguins

For higher dimensions, the procedures are similar, with the hyper-plane and support vectors being examined using a tour.

```{r}
#| code-fold: false
library(dplyr)
load("data/penguins_sub.rda")
chinstrap <- penguins_sub %>%
  filter(species == "Chinstrap") %>%
  select(-species)
chinstrap_svm <- svm(sex~., data=chinstrap, 
                     kernel="linear",
                     probability=TRUE)
chinstrap_svm_e <- explore(chinstrap_svm, chinstrap)
```

```{r}
#| eval: false
# Tour raw data
library(tourr)
animate_xy(chinstrap[,1:4], col=chinstrap$sex)
# Add all SVs, including bounded
c_pch <- rep(20, nrow(chinstrap))
c_pch[chinstrap_svm$index] <- 4
animate_xy(chinstrap[,1:4], col=chinstrap$sex, pch=c_pch)
# Only show the SVs with |coefs| < 1
c_pch <- rep(20, nrow(chinstrap))
c_pch[chinstrap_svm$index[abs(chinstrap_svm$coefs)<1]] <- 4
animate_xy(chinstrap[,1:4], col=chinstrap$sex, pch=c_pch)
render_gif(chinstrap[,1:4],
           grand_tour(),
           display_xy(col=chinstrap$sex, pch=c_pch),
           gif_file="gifs/chinstrap_svs.gif",
           frames=500)

# Tour the separating hyper-plane also
symbols <- c(3, 20)
c_pch <- symbols[as.numeric(chinstrap_svm_e$.TYPE[!chinstrap_svm_e$.BOUNDARY])]
animate_xy(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4], 
           col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY],
           pch=c_pch)
render_gif(chinstrap_svm_e[!chinstrap_svm_e$.BOUNDARY,1:4],
           grand_tour(),
           display_xy(col=chinstrap_svm_e$sex[!chinstrap_svm_e$.BOUNDARY], pch=c_pch),
           gif_file="gifs/chinstrap_svm.gif",
           frames=500)
```

::: {#fig-p-svm layout-ncol=2}

![Exploring which points are support vectors.](gifs/chinstrap_svs.gif){#fig-chinstrap_svs fig-alt="FIX ME" width=300}

![Exploring SVM boundary.](gifs/chinstrap_svm.gif){#fig-chinstrap_svm fig-alt="FIX ME" width=300}

SVM model for distinguishing the sexes of the Chinstrap penguins. The separating hyper-plane is 3D, and separates primarily on variables `bl` and `bd`, as seen because these two axes extend out from the plane when it is seen on its side, separating the two groups.
:::

This is a good place to show manual tour also, to find the separation.

<!-- We describe this method more fully than we did the other algorithms for two reasons: first, because of its apparent similarity to the graphical approach, and second, because it is difficult to find a simple explanation of the method in the literature.

The algorithm takes an $n \times p$ data matrix, where each column is scaled to $[-1,1]$ and each row is labeled as one of two classes ($y_i=+1$ or $-1$), and finds a hyperplane that separates the two groups, if they are separable. Each row of the data matrix is a vector in $p$-dimensional space, denoted as

% Should this be represented as a row instead of a column?  dfs

$$
X=\left[ \begin{array}{c}
  x_1 \\ x_2 \\ \vdots \\ x_p \end{array} \right]
$$

\noindent and the separating hyperplane can be written as

$$
W'X + b = 0
$$

\noindent where $W = [ w_1~~ w_2 ~~ \dots ~~ w_p]'$ is the normal
vector to the separating hyperplane and $b$ is a constant.  The best separating hyperplane is found by maximizing the margin of separation between the two classes as defined by two parallel hyperplanes:

$$
W'X + b = 1, ~~~~~ W'X + b = -1.
$$

\noindent These hyperplanes should maximize the distance from the
separating hyperplane and have no points between them, capitalizing on any gap between the two classes. The distance from the origin to the
separating hyperplane is $|b|/||W||$, so the distance between the
two parallel margin hyperplanes is $2/||W||=2/\sqrt{w_1^2+\dots
+w_p^2}$. Maximizing this is the same as minimizing $||W||/2$. To
ensure that the two classes are separated, and that no points lie
between the margin hyperplanes we need:

$$
W'X_i + b \geq 1, ~~~\mbox{  or  } ~~~W'X_i + b \leq -1 ~~~\forall i=1, ..., n
$$

\noindent which corresponds to

\begin{eqnarray}
y_i(W'X_i+b)\geq 1 ~~~\forall i=1, ..., n
\label{svm-crit}
\end{eqnarray}

\noindent Thus the problem corresponds to
\begin{quote} 
{\em minimizing} $\frac{||W||}{2}$ {\em subject to }
$y_i(X_iW+b)\geq~1 ~~~\forall i=1, ..., n$.
\end{quote}

\index{SVMLight}

\noindent Interestingly, only the points closest to the margin
hyperplanes are needed to define the separating hyperplane. We might think of these points as lying on or close to the convex hull of each cluster in the area where the clusters are nearest to each other. These points are called support vectors, and the coefficients of the separating hyperplane are computed from a linear combination of the support vectors $W = \sum_{i=1}^{s} y_i\alpha_iX_i$, where $s$ is the number of support vectors.  We could also use $W = \sum_{i=1}^n y_i\alpha_iX_i$, where $\alpha_i=0$ if $X_i$ is not a support vector. For a good fit the number of support vectors $s$ should be small relative to $n$. Fitting algorithms can achieve gains in efficiency by using only samples of the cases to find suitable support vector candidates; this approach is used in the SVMLight
[@Jo99] software.

In practice, the assumption that the classes are completely separable is unrealistic. Classification problems rarely present a gap between the classes, such that there are no misclassifications.
@CV95 relaxed the separability condition to allow some
misclassified training points by adding a tolerance value $\epsilon_i$ to Equation \ref{svm-crit}, which results in the modified criterion $y_i(W'X_i+b)>1-\epsilon_i, \epsilon_i\geq 0$. Points that meet this criterion but not the stricter one are called slack vectors.

Nonlinear classifiers can be obtained by using nonlinear
transformations of $X_i$, $\phi(X_i)$ [@BGV92], which is
implicitly computed during the optimization using a kernel function
$K$. Common choices of kernels are linear
$K(x_i,x_j)=x_i'x_j$, polynomial
$K(x_i,x_j)=(\gamma x_i'x_j+r)^d$, radial basis
$K(x_i,x_j)=\exp(-\gamma ||x_i-x_j||^2)$, or sigmoid
functions $K(x_i,x_j)=\mbox{tanh}(\gamma x_i'x_j+r)$, where
$\gamma>0, r,$ and $d$ are kernel parameters.

% She didn't say to delete the terminating colon here, but by 
% analogy with these rest, I will.  dfs

The ensuing minimization problem is formulated as

$$
\mbox{ minimizing } \frac{1}{2}||W|| + C\sum_{i=1}^n \epsilon_i ~~ \mbox{ subject to } 
y_i(W'\phi(X)+b)>1-\epsilon_i
$$

\noindent where $\epsilon_i\geq 0$, $C>0$ is a penalty parameter guarding  against over-fitting the training data and $\epsilon$ controls the tolerance for misclassification. The normal to the separating hyperplane $W$ can be written as $\sum_{i=1}^{n}
y_i\alpha_i{\phi(X_i)}$, where points other than the support and
slack vectors will have $\alpha_i=0$.  Thus the optimization problem becomes

\begin{eqnarray*}
\mbox{ minimizing } \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n y_iy_j\alpha_i\alpha_jK(X_i,X_j)+C\sum_{i=1}^n \epsilon_i \\ ~~~~~~~~~~~\mbox{ subject to } 
y_i(W'\phi(X)+b)>1-\epsilon_i
\end{eqnarray*}

\index{R package!e1071}
\index{R package!libsvm}

We use the `svm` function in the `e1071` package
[@DHLMW] of R, which uses `libsvm` [@CL], to classify
the oils of the four areas in the Southern region.  SVM is a binary classifier, but this algorithm overcomes that limitation by comparing classes in pairs, fitting six separate classifiers, and then using a voting scheme to make predictions. To fit the SVM we also need to specify a kernel, or rely on the internal tuning tools of the algorithm to choose this for us.  Automatic tuning in the algorithm chooses a radial basis, but we found that a linear kernel performed better, so that is what we used. (This accords with our earlier visual inspection of the data in @sec-class-plots.)  Here is the R code used to fit the model:

```
> library(e1071)
> olive.svm <- best.svm(factor(area) ~ ., data=d.olive.train)
> olive.svm <- svm(factor(area) ~ ., data=d.olive.sth.train, 
  type="C-classification", kernel="linear")
> table(d.olive.sth.train[,1], predict(olive.svm, 
  d.olive.sth.train))
   
      1   2   3   4
  1  19   0   0   0
  2   0  42   0   0
  3   0   0 155   3
  4   1   2   3  21
> table(d.olive.sth.test[,1], predict(olive.svm, 
  d.olive.sth.test))
   
     1  2  3  4
  1  6  0  0  0
  2  1 12  1  0
  3  0  0 46  2
  4  1  1  0  7
> support.vectors <- olive.svm$index[
    abs(olive.svm$coefs[,1])<1 &
    abs(olive.svm$coefs[,2])<1 & abs(olive.svm$coefs[,3])<1]
> pointtype <- rep(0,323) # training
> pointtype[247:323] <- 1 # test
> pointtype[olive.svm$index] <- 2 # slack vectors
> pointtype[support.vectors] <- 3 # support vectors
> parea <- c(predict(olive.svm, d.olive.sth.train),
    predict(olive.svm, d.olive.sth.test))
> d.olive.svm <- cbind(rbind(d.olive.sth.train, 
    d.olive.sth.test), parea, pointtype)
> gd <- ggobi(d.olive.svm)[1]
> glyph_color(gd) <- c(6,3,2,9)[d.olive.svm$area]
```

\noindent These are our misclassification tables:

\smallskip
\noindent \emph{Training:}
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\  \cline{3-7}

\T         & North Apulia & 19 & 0 & 0 & 0 & 0.000\\
\Vbl{area} & Calabria & 0 & 42 & 0 &  0 & 0.000\\
           & South Apulia & 0 & 0 & 155 & {\bf 3} & 0.019\\
\B         & Sicily & {\bf 1} & {\bf 2} & {\bf 3} & 21 & 0.222 \\ \cline{3-7}
\T         &        &         &         &         &    & 0.037
\end{tabular}
\end{center}

\bigskip

\noindent \emph {Test:}
\begin{center}
\begin{tabular}{l@{\hspace{.3in}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1em}}r@{\hspace{1.5em}}r@{\hspace{2em}}r}
\B & & \multicolumn{4}{c}{Predicted \Vbl{area}} & Error \\

\T & & \multicolumn{1}{c}{North}  & \multicolumn{1}{c}{Calabria} & \multicolumn{1}{c}{South}  & \multicolumn{1}{c}{Sicily} &  \\
\B & & \multicolumn{1}{c}{Apulia} &  & \multicolumn{1}{c}{Apulia} & & \\  \cline{3-7}

\T         & North Apulia & 6 & 0 & 0 & 0 & 0.000\\
\Vbl{area} & Calabria & {\bf 1} & 12 & {\bf 1} &  0 & 0.143\\
           & South Apulia & 0 & 0 & 46 & {\bf 2} & 0.042\\
\B         & Sicily & {\bf 1} & {\bf 1} & 0 & 7 & 0.286 \\  \cline{3-7}
\T         &        &         &         &         &    & 0.078
\end{tabular}
\end{center}
\bigskip

\noindent The training error is $9/246=0.037$, and the test error is
$6/77=0.078$.  (The training error is the same as that of the neural network classifier, but the test error is lower.)  Most error is associated with Sicily, which we have seen repeatedly to be an especially difficult class to separate.  In the training data there are no other errors, and in the test data there are just two samples from Calabria mistakenly classified. @fig-olive-svm illustrates our examination of the misclassified cases, one in each row of the figure. (Points corresponding to Sicily were removed from all four plots.)  Each of the two cases is brushed (using a filled red
circle) in the plot of misclassification table and viewed in a linked 2D tour.  \index{brushing!linked}\index{tour!grand} Both of these cases are on the edge of their clusters so the confusion of classes is reasonable.

% Figure 14
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=2.2in]{chap-class/olive-svm6.pdf}}
 {\includegraphics[width=2.2in]{chap-class/olive-svm7.pdf}}}
\smallskip

\centerline{{\includegraphics[width=2.2in]{chap-class/olive-svm8.pdf}}
 {\includegraphics[width=2.2in]{chap-class/olive-svm9.pdf}}}
\caption[Misclassifications of a support vector machine classifying
 the oils of the South]{Misclassifications of a support vector machine
 classifying the oils of the South by \Vbl{area}.  The
 misclassification table {\bf (left)} is linked to 2D tour plots {\bf
 (right)}; different misclassifications are examined in each row of
 plots. (The oils from Sicily, the fourth area, have been removed from
 all plots.)  }
\label{olive-svm}
\end{figure*}

% Figure 15
\begin{figure*}[htbp]
\centerline{
 {\includegraphics[width=1.5in]{chap-class/olive-svm3.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-svm4.pdf}}
 {\includegraphics[width=1.5in]{chap-class/olive-svm5.pdf}}}
\caption[Using the tour to examine the choice of support
vectors]{Using the tour to examine the choice of support vectors when
classifying Southern oils by \Vbl{area}.  Support vectors are open
circles, and slack vectors are open rectangles; the data points are
represented by $+$es and $\times$es.}
\label{olive-svm2}
\end{figure*}

The linear SVM classifier uses 20 support vectors and 29 slack vectors
to define the separating planes between the four areas. It is
interesting to examine which points are selected as support vectors,
and where they are located in the data space.  For each pair of
classes, we expect to find some projection in which the support vectors
line up on either side of the margin of separation, whereas the slack
vectors lie closer to the boundary, perhaps mixed in with the points
of other classes.

\index{tour!grand} \index{tour!manual} The plots in
@fig-olive-svm2 represent our use of the 2D tour, augmented by
manual manipulation,~to look for these projections.  (The Sicilian points are again removed.) The support vectors are represented by open circles and the slack vectors by open rectangles, and we have been able to find a number of projections in which the support vectors are on the opposing outer edge of the point clouds for each cluster. 

The linear SVM does a very nice job with this difficult
classification. The accuracy is almost perfect on three classes,
and the misclassifications are quite reasonable mistakes, being points that are on the extreme edges of their clusters.  However, this method joins the list of those defeated by the difficult problem of distinguishing the Sicilian oils from the rest.

### Examining boundaries

\index{classification!examining boundaries}

For some classification problems, it is possible to get a good picture of the boundary between two classes. With LDA and SVM classifiers the boundary is described by the equation of a hyperplane. For others the boundary can be determined by evaluating the classifier on points sampled in the data space, using either a regular grid or some more efficient sampling scheme.

% Figure 16
\begin{figure*}[htbp]
\centerline{{\includegraphics[width=2in]{chap-class/olive-classifly-lda.pdf}}
 {\includegraphics[width=2in]{chap-class/olive-classifly-svm.pdf}}}
\smallskip
\centerline{{\includegraphics[width=2in]{chap-class/olive-classifly-svm3.pdf}}
 {\includegraphics[width=2in]{chap-class/olive-classifly-svm5.pdf}}}
\caption[Classification boundaries for different
models]{Classification boundaries for different models shown in the 2D
tour. Points on the boundary are gray stars.  We first compare LDA
{\bf (top left)} with linear SVM {\bf (top right)} in finding the
boundary between oils from the North and Sardinia.  Both boundaries
are too close to the cluster of Northern oils.  We also compare linear
SVM {\bf (bottom left)} and radial kernel SVM {\bf (bottom right)} in
finding the boundary between oils from South Apulia and other Southern
oils.  }
\label{olive-classifly} 
\end{figure*}

\index{R package!classifly} \index{tour!grand}
\index{tour!manual} We use the R package `classifly`
[@Wi06] to generate points illustrating boundaries, add those
points to the original data, and display them in GGobi.
@fig-olive-classifly shows projections of boundaries between
pairs of classes in the \Data{Olive Oils}.  In each example, we used the 2D tour with manual control~to focus the view on a projection that revealed the boundary between two groups.

% Needs to be checked
\begin{verbatim}
> library(classifly)
> d.olive.sub <- subset(d.olive,region!=1,
   select=c(region,palmitic:eicosenoic))
> classifly(d.olive.sub, region~linoleic+oleic+arachidic, 
   lda)
> classifly(d.olive.sub, region~linoleic+oleic+arachidic, 
   svm, probability=TRUE, kernel="linear")
\end{verbatim}

The top two plots show tour projections of the North (purple) and
Sardinia (green) oils where the two classes are separated and the
boundary appears in gray.  The LDA boundary (shown at left) slices too close to the Northern oils. This might be due to the violation of the LDA assumption that the two groups have equal variance; since that is not true here, it places the boundary too close to the group with the larger variance.  The SVM boundary (at right) is a bit closer to the Sardinian oils than the LDA boundary is, yet it is still a tad too close to the oils from the North.

The bottom row of plots examines the more difficult classification of the areas of the South, focusing on separating the South Apulian oils (in pink), which is the largest sample, from the oils of the other areas (all in orange). Perfect separation between the classes does not occur. Both plots are tour projections showing SVM boundaries, the left plot generated by a linear kernel and the right one by a radial
kernel.  Recall that the radial kernel was selected automatically by the SVM software we used, whereas we actually chose to use a linear kernel.  These pictures illustrate that the linear basis yields a more reasonable boundary between the two groups. The shape of the clusters of the two groups is approximately the same, and there is only a small overlap of the two. The linear boundary fits this structure neatly. The radial kernel wraps around the South Apulian oils.
-->

<!-- include Distance-weighted discrimination, eg kerndwd -->
